Link,Tags,Job Title,Company,Location,Job Type,Job Description
https://startup.jobs/data-engineer-hybrid-knorex-4556194,Engineer,Data Engineer (Hybrid),KNOREX ,"Ho Chi Minh City, Vietnam",Full-Time,"

About Knorex
Established in 2010, Knorex is a cutting edge advertising technology MNC with offices across USA, Australia, China, Singapore, Vietnam, India, Thailand and Malaysia. Knorex provides Precision Performance Marketing products and solutions to the worlds leading brands and media agencies. With its full-stack platform, Knorex XPO httpsxpo.knorex.com supplies the technology platform to deliver the right marketing message to the right audience at the right moment and right place, underpinned by a multi-layered data-driven approach. Knorex XPO shields its customers from dealing with the complexity and fuss while delivering immersive, dynamic and personalized marketing experiences that connects with their users. Knorex also provides managed services to complement its offering.
Why Knorex
We are constantly on the lookout to recruit the best and the brightest - from engineering to sales to account management to operations and HR.
Knorex offers you many different opportunities to scale your ambition and creativity far and beyond. We embrace a dynamic and pragmatic way of doing things, setting ourselves up for long term achievement yet relentlessly focused on delivering the short term goals. If you love the joy of building stuffs and seeing them grow, growing yourself and others in the process, and challenging yourself to do stuffs that you once thought impossible, we invite you to explore a career with us.
Key Responsibilities
 Develop clever algorithms and pragmatic solutions to our data analytics problems. Develop metrics to measure the outcomeimpact of your introduced solutions. Develop and maintain API to support other teams in retrieving the metrics Work with other members to implement and integrate into our existing systems. Document and improve the solutions over time. Evaluate and identify new technologies for implementation. Communicate with our business and technical teams to understand the analytics requirements. Respond and follow up to incorporate feedback and draw new insights. Prioritize tasks to meet multiple deadlines. 
Requirements
 At least 3 years of experience for similar positions Must be proficient in either Python, Scala, Spark or NodeJS Proficiency in multiple languages listed above is a plus Knowledge in MongoDB is nice to have Good knowledge of algorithms and data structures Experience with ad serving, ad tracking and optimization is a plus Strong in analytics and problem solving technique Willingness to learn and able to pick up new technology or new concepts fast; Able to work independently as well as in collaborative mode with minimum supervision; Work productively even under pressure; Possess good work ethic, attitude with good follow-through; Excellent communication in written and spoken English. 
Benefits
 Attractive salary, 13rd month salary Quarterly bonus scheme W3F fund for learning and development SHUI  Bao Viet Health care insurance Macbook provided Ample opportunities to grow. You get to propose your own ideas and see it through. Work with passionate, talented and driven colleagues who get things done! Opportunity to work cross-country and with variety of projects of different nature. Challenging and exciting problems that await you to solve. Comprehensive Health Insurance Coverage. Personal Development Fund for courses and materials. 

"
https://startup.jobs/data-engineer-centre-of-excellence-ovo-external-4555397,Engineer,Data Engineer (Centre of Excellence),OVO External ,,,"

Role OVO-View
Location Hub based! Bristol, London, Glasgow or Remote! But you have the flexibility to work wherever suits you bestTeam Platform Technology, Data PlatformSalary banding 55,770 - 80,170Experience Mid-level  SeniorWorking pattern Full-TimeReporting to Software Engineering ManagerSponsorship Unfortunately we are unable to offer sponsorship for this role.This role in 3 words Explore, Cross-collaboration, OwnershipTop 3 qualities for this role Communication, Collaborative, Adaptability
Everyone belongs at OVO
At OVO, we are on a mission to solve one of humanitys biggest challenges, the climate crisis. And we know it takes all of us to change the world. Thats why we need diverse people from all gender identities, ethnicities, ages, sexual orientations, life experiences and backgrounds to join us.
Teamworking for the planet
Everything we do here spins around Plan Zero. So, naturally, the team youll be joining plays a gigantic role in making that happen. Heres how
Were hiring world-changers. Every role were hiring plays their own part in our mission; our role is to find those people and bring them on our Zero Carbon journey. Across OVO we have around 350 Engineers all with varying backgrounds and levels of experience. One key thing that all of our engineers have in common is a desire to develop brilliant, industry redefining products as well as their own skills.
This role in a nutshell
Youll be part of OVOs Data Engineering Team, which will act as a Centre of Excellence for data engineering across OVO, providing strategy, advice and guidance, reusable approaches, patterns and assets, and tool recommendations. This team creates and facilitates a Data Engineering community to support data engineers working in product and business teams as they build data pipelines and transform data for new products and for analytics and reporting.
Your key outcomes will be

Part of a team responsible for setting a strategy for data engineering across OVO and developing guidelines for how we work across our streaming and data warehouse technologies.
Building a data engineering community with engineers working in product teams, providing advice and guidance, tool recommendations etc.
Building and maintaining integration pipelines that power our automated journeys.
Transforming streaming data to meet target schemas.
Being part of an agile engineering team where you will have the opportunity to influence technology selection.
Establishing good data engineering practices including using infrastructure as code; contributing to automated testing strategies; setting up monitoring and alerting tools; employing CICD best practices to deploy regularly to production.
Participatingleading efforts in the Open Source community, speaking at conferences, contributing to OVOs tech blog, etc.
Engaging and encouraging engagement in the wider community through conference participation and contributing to our Tech Blog.
Working with key stakeholders to understand their data needs and help deliver solutions that provide them with excellent quality data that allows teams to realise their objectives.


Youll be a successful Data Engineer at OVO if you


Experience of designing, building, monitoring and managing large-scale data products, pipelines, tooling and platforms.

A track record as a Data Engineer, setting strategy and defining ways of working.


Experience working on streaming ETL solutions utilising streaming data processing tools e.g. Kafka Streams, Kinesis, Spark or similar.Experience developing cloud-based solutions on GCP preferably, AWS or Azure using Infrastructure as code tools such as Terraform.


Excellent knowledge of at least one programming language e.g. Scala, Python, Typescript

Knowledge of the best engineering practices and continuous delivery.
An understanding that building quality software is essential and you value automation and continuous delivery.
A love for building scalable, resilient solutions, and you enjoy influencing the teams technology selection and architectural direction.

You will be comfortable working in an agile software development environment and have experience of CICD and deployment strategies


Lets talk about whats in it for you
Well pay you between 55,770 and 80,170, depending on your specific skills and experience. If your expectations are a little different, have a chat with us!
We keep our pay ranges broad on purpose to give us, and you, flexibility to match your experience to our zero carbon mission.
Youll be eligible for an on-target bonus of 15. We have one OVO bonus plan that focuses on the collective performance of our people to deliver our Plan Zero goal. 
We also offer plenty of green benefits and progressive policies to help you feel like you belong at OVOand theres flex pay.  Its an extra 9 of your salary on top of your core pay to use as you like. You can take it as cash, add to your pension, or choose to spend it on a huge range of flex benefits. Heres a taster of whats on offer 
For starters, youll get 34 days of holiday including bank holidays. For your healthWith benefits like a healthcare cash plan or private medical insurance depending on your career level, critical illness cover, life assurance, health assessments, and moreFor your wellbeingWith gym membership, gadget, travel and cyber insurance, workplace ISA, will writing services, DNA testing, dental insurance, and more For your lifestyle With extra holiday buying, discount dining, culture cards, tech loans, and supporting your favourite charities with give-as-you-earn donationsFor your home  Get up to 300 off any OVO Energy plan when you pay by Direct Debit, plus personal carbon offsetting and great discounts on smart thermostats and EV chargersFor your commute Nab a great deal on ultra-low emission car leasing, plus our cycle to work scheme and public transport season ticket loans Want to hear about our full range of flexible benefits and progressive people policies? Our People Team can tell you everything you need to know.
For your Belonging
To find better ways to support our people, we need to listen to each others experiences and find ways to build a truly inclusive and diverse workplace. As part of this, we have 8 Belonging Networks at OVO. Led by our people, for our people - so when you join OVO, you can play a part - big or small - with any of the Networks. Its up to you.
Oh, and one last thing...
Wed be thrilled if you tick off all our boxes yet we also believe its just as important we tick off all of yours. And if you think you have most of what were looking for but not every single thing, go ahead and hit apply. Wed still love to hear from you!
If you have any additional requirements, theres a space to let us know on the application form; we want to make the process as easy and comfortable for you as possible..

"
https://startup.jobs/data-engineer-hack-the-box-4554646,Engineer,Data Engineer,Hack The Box ,"Alimos, Greece",Full-Time,"

Welcome! Super excited you dropped by 
Join us in redefining the standards of cybersecurity expertise by bringing together community and business!
About Hack The Box
Hack The Box is a leading gamified cybersecurity upskilling, certification, and talent assessment platform enabling individuals, businesses, government institutions, and universities to sharpen their offensive and defensive security expertise.
Launched in 2017, Hack The Box brings together the largest global cybersecurity community of more than 2m platform members and is on a mission to create and connect cyber-ready humans and organizations through highly engaging hacking experiences that cultivate out-of-the-box thinking. Offering a fully guided and exploratory skills development environment, Hack The Box is the ideal solution for cybersecurity professionals and organizations to continuously enhance their cyber-attack readiness by improving their red, blue, and purple team capabilities.
Rapidly growing its international footprint and reach, Hack The Box is headquartered in the UK, with additional offices in Greece and the US.
 Exciting News In Jan 2023 we secured a 55 Million Series B funding  The new investment will accelerate Hack The Boxs growth trajectory with a focus on further building out its category-defining gamer-first solutions offering. Hack The Box will also enhance its go-to-market function, doubling down on the companys ongoing international expansion with strong commercial traction in the US, Europe, and APAC. Furthermore, the HTBs Greek entity has been certified as a Great Place to Work for 2023 
Get more insights about our HTB culture and employee experience by visiting our career site and Glassdoor.
About The Role
We are in the search of an ambitious Data Engineer to be part of our growing Data Engineering team here at HTB.  In this role, you will take over our data pipelines, maintain and develop new ones, improve data quality, make data available to the organisation and expand data usage across the organisation. Your day-to-day will include end to end involvement and design of ETL processes, including sourcing data from different HTB departments, checking them, transforming them into clear information and loading them for various uses. The creation of clean and articulated code will definitely help in this process. You will be involved with both consuming and developing REST APIs, manage data infrastructure and services, and have the opportunity to work with a plethora of key 3rd party tools like Fivetran, Snowflake and many more. We are looking for someone who is a constant learner and thinks outside the box. The team has been growing the last year, so there is a lot of room for innovative ideas and taking part in more creative tasks.  Responsibilities
 Combine, analyse and organise raw data from different sources Build data systems and pipelines Evaluate business needs and objectives by doing requirements capturing and analysis Prepare data for prescriptive and predictive modelling Explore ways to enhance data quality and reliability Identify opportunities for data acquisition 
Requirements
 Knowledge of a programming language, ideally Python Hands-on or theoretical experience with SQL Knowledge of writing ETLELT pipelines Previous experience as a Data Engineer or in a similar role e.g. Software Engineer Adequate knowledge of English language written and verbal  Additional Preferred Requirements
 Previous experience in PrefectAirflow or any similar orchestration tool Docker  Kubernetes experience will be considered a plus 
Benefits
Why work at Hack The Box?
 Youll have the chance to contribute to a product that is quite appreciated by its users and the overall cybersecurity community Youll work in a highly supportive and caring environment, while having flexibility and autonomy Youll be able to grow as we grow by solving challenging problems Youll definitely have fun while working at HTB  
We have also other benefits that will keep you happy
 Private insurance Free lunch  snacks at the office 25 annual leave days Dedicated budget for training and professional development, participation in conferences State-of-the-art equipment mac, iPhone, and mobile plan Full access to the Hack The Box lab offerings; so you can learn how to hack FlexibleHybrid working 
Selection Process
 A conversation with the Talent Acquisition team to know us better and to share more info about HTB and the role A bit more of a technical conversations with the hiring manager to get a better understanding of your experience and achievements A challenge related to the job and a conversation with the team about the way you have approached it. This will really help us understand your skillset and have a fair selection process C-Level interview Offer  

"
https://startup.jobs/data-engineer-iii-data-lake-pismo-2-4554132,Engineer,"Data Engineer III, Data Lake",@ Pismo ,,,"


Pismo
Weve been building since day one a diverse and ready to cross the oceans team and mindset with the most innovative professionals in their field. We are not only committed to delivering cutting-edge solutions to help our clients build their products. We want to promote change in how the world deals with financial services and impact millions of lives.
We are a Remote-first company, so yes, you can work pretty much from anywhere. But if you still need to go to the office every now and then, we currently have four offices located in Brazil So Paulo, The USA Austin, The UK Bristol and Singapore.
Investors
We are backed by some of the most prominent investment companies in the market, such as SoftBank, Amazon, Accel, B3, Falabella Ventures, Headline, PruVen and Redpoint eventures.
Read our Series B announcement here.
 

 
What youll do

Development and improvement of Data ETLELT using Spark and SQL Engines;
Propose and implement improvements in pipelines related to data CICD to improve usability and development cycle;
Proposal and development of solutions or changes in Software and Technological Architecture to evolve the Data Platform, reducing bottlenecks and enhancing its resilience, modularity, performance or scalability;
Ensuring that Data Pipelines follow testing, monitoring, observability, and security best practices;
Help manage incidents in production by performing root cause analysis with your troubleshooting skills like anyone on the team, but with a Data Engineering perspective; 
Agile methodologies and code review like anyone else on the team, but with a focus on Data Engineering;

Minimum Qualifications

SQL and NoSQL databases experience;
Experience with Python programming;
Cloud Computing AWS Preferably;
Experience with Spark or Parallel Data Processing MPP tool in production;
Experience with event processing SQS, SNS, Kakfa, Kinesis, pubsub;

Core Benefits

Remote work
Flexible hours
Gympass
Meal  Food vouchers
Remote work financial support
Life Insurance
Medical and Odontological Assistance
Employee child care benefit daycare
Vidalink partnership
Day off Birthday
Support for studying languages
50 off AWS and GCP certifications

Technologies that we apply in our day

Java, Groovy e Go
Automate Testing 
SQL   NoSQL
Git
APIs Rest e data Streaming
Cloud AWS and Google
Docker


--
More about Pismo
Jump into the waves of transformation
Our story started in 2016, back on the California seaside, a place called Pismo beach.
The place was not only an inspiration to our name but also to how we see our contribution to the world.The smallest part of an ocean is a drop. Several drops together form waves. And waves are the energy of movement, of transformation. They represent the power of change in a fluid and organic way.
At Pismo, each of us are powerful drops embodying a huge wave of change in payments and banking.
Our core values
Responsibility  Commitment

Responsibility and Commitment are key tenets of Pismos business. In all of our interactions, ethics and honesty serve as guiding principles. Transparency and alignment are essential at Pismo to foster a culture of accountability where people do not take shortcuts and dare to be vulnerable. We encourage ownership and enable our employees to be a part of the solution to our challenges.

Challenging the Status Quo to Drive Change

Pismo is a disruptive company that knows we cant count on what got us here to get us where we want to go. Instead of following trends, our team members create them. We strive for the best solution for every situation, not the easiest. Taking the ordinary or common sense route is insufficient. This is how we affect real change.

Minimum Friction Experience   

Pismos success depends on fluid, clear, and careful internal and external communication. Our solutions should address issues directly rather than causing new ones. We understand that learning and development is a positive outcome of necessary friction. We want our customers and employees to have a pleasant experience free of bureaucracy.

Collective Power

Pismo isnt simply mine, yours, or some other teams. All of our people have a good purpose for being here. Each of us, in our way, has the freedom to be ourselves and know we are accepted. Pismo values being a multicultural organisation. We are stronger, more efficient, and happier when we operate as a team.

Delivering Value and Excellence

Constant development is essential as it allows us to supply high-quality products and services while ensuring that we always bring value to our clients. Pismo recognises the importance of new information in breaking down boundaries and forging new possibilities. We pave the way and set trends by developing market-leading solutions.

Focus on People

People are a companys most valuable asset. That is why we prioritise attracting and hiring the best people, followed by personal and professional development. We invest in peoples growth and empower them to take charge of their careers. We value work-life balance because we understand how difficult it can be to distinguish between work and life.

--
Pismo is an Equal Employment Opportunity employer that proudly pursues and hires a diverse workforce. Pismo does not make hiring or employment decisions on the basis of race, color, religion or religious belief, ethnic or national origin, nationality, sex, gender, gender identity, sexual orientation, disability, age or any other basis protected by applicable laws or prohibited by company policy. Pismo also strives for a healthy and safe workplace and strictly prohibits harassment of any kind.


"
https://startup.jobs/data-engineer-redventures-4554044,Engineer,Data Engineer,Red Ventures ,"Charlotte, United States",,"

Job Title  Data Engineer 2 positions
Job Location 1101 Red Ventures Drive, Fort Mill, SC 29707
Description of Duties 
Two Data Engineers sought by RBUS, Inc., a Red Ventures Company, headquartered in Ft. Mill, SC to design, develop and maintain data warehouse solutions for analytics and business reporting. Job duties include manage cloud services, build  maintain data warehouse, develop data pipelines, and perform data manipulation. Telecommuting within the United States is permissible. Rate of Pay 145,000.00 to 155,000.00year
Minimum Job Requirements Bachelors degree in computer science or directly related STEM field or foreign equivalent degree is required. 1 year of experience in the job offered or directly related occupation is required and must include 1 year of experience using Hadoop  Spark working with DataFramesDatasets API and SparkSQL to query and perform data manipulation using Scala, Python and SQL; 1 year of experience using Terraform and Databricks Notebook to manage cloud services, Databricks File System and S3 data lake; 1 year of experience using SQL Relational Database and Redshift to maintain  optimize data warehouse and create tables for revenue analytics; 1 year of experience using CircleCi to setup  troubleshoot Production, QA and Development environment for batch processing and streaming data pipelines; and 1 year of experience using Airflow, Linux, and Command Line Interface to execute staging and reporting jobs on AWS. Telecommuting within the United States is permissible. Employer will accept any suitable combination of education, training and experience. 
Applicants are to Respond to RBUS, Inc., A Red Ventures Company, 1101 Red Ventures Drive, Fort Mill, SC 29707  Attn HR, Req. 1034.
 If you are based in California, we encourage you to read this important information for California residents by visiting httpswww.redventures.comlegalca-emp-privacy-notice 

"
https://startup.jobs/data-engineer-bdm-precision-for-medicine-4553885,Engineer,"Data Engineer, BDM",Precision for Medicine ,,,"

QuartzBio www.quartz.bio is a Software-as-a-Service SaaS solutions provider to the life sciences industry. We deliver innovative, data enabling technologies i.e., software that provide biotechpharma RD teams with enterprise-level access to samplebiomarker data management solutions  analytics, information, insight  reporting capabilities. 
Our end-to-end from sample collection to biomarker data suite of solutions are focused on providing sponsors information data with context  we do this by connecting biospecimen, assay as well as clinical data sources in a secure and scalable cloud-based infrastructure, enabling seamless, automated data management workflows, key insight development, improved collaboration, and the ability to make faster, more informed decisions.
Position Summary
The Data Engineer will support configuration and implementation of QuartzBios cutting edge technology solutions to enable management of a broad variety of data from various assay technologies and data sources. The main focus is to develop and implement automation and testing approaches to support product development working with an interdisciplinary team that includes a software engineering team as well as various data science teams.
Essential functions of the job include but are not limited to

Responsible for supporting implementationconfiguration of QuartzBios cutting edge enterprise Biomarker Data Management platform on client projects
Support the team in designing, automating, testing, documenting, deploying and maintaining automation workflows for
Support implementation of strategies for use of cloud to support automation in active collaboration with the team of data scientists, biologists and statisticians
Other duties as assigned

Qualifications
Minimum Required

Bachelors Degree Computer Science, Data Science, Bioinformatics, Computational Biology or a related field
3-5 years software development or data engineering experience
Programming language R experience, with additional Python experience preferred but not required 

 
Other Required

Experience with implementing automated workflows
Experience with command line tooling preferably in Linux
Experience with containerized systems Docker
Experience in programming concepts including reproducibility, consistency and test-driven development
Comfort in either R or Python
Experience with source-code management Git
Exercises judgment within generally defined practices selecting an approach for obtaining solutions
Excellent communication and interpersonal skills
Team player contributing to a positive, collaborative working environment
Must be able to read, write, speak fluently and comprehend the English language
Other preferred, but not required skills experience working with biological data such as data from NGS, immunophenotyping, gene expression, protein expression assay

Preferred

Experience with designing automated workflows
Comfort using multiple programming languages, including R and Python
 Working knowledge of AWS cloud computing capabilities
 Familiarity with SaaS solutions
 Familiarity with biological andor clinical data
 Experience in multiple coding languages, and comfort coming up to speed in new ones
 Ability to manage highly-varied, fast-paced work environment
 Experience in DevOps


Any data provided as a part of this application will be stored in accordance with our Privacy Policy. For CA applicants, please also refer to our CA Privacy Notice.
Precision Medicine Group is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, age, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status or other characteristics protected by law.  2020 Precision Medicine Group, LLC
If you are an individual with a disability and require a reasonable accommodation to complete any part of the application process or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact Precision Medicine Group at QuestionForHRprecisionmedicinegrp.com.


"
https://startup.jobs/data-engineer-hugeinc-4553797,Engineer,Data Engineer,HugeInc ,,,"

Location This position is remote within the United Kingdom.

Huge is looking for a Data Engineer to join our Data Products team.  
What Youll Be Doing.

Data Pipeline Monitoring Monitor the health and performance of existing data pipelines and ETL Extract, Transform, Load processes. Use monitoring tools and dashboards to identify and resolve any data processing issues, bottlenecks, or failures. Collaborate with the operations team or developers to troubleshoot and resolve these issues.
Data Transformation and Processing Develop, test, and maintain data transformation and processing jobs. Use cloud-native technologies and frameworks such as Apache Spark, Apache Beam, or cloud-specific tools like AWS Glue or Google Cloud Dataflow to transform raw data into usable formats for downstream applications. Write and optimize SQL queries or code for data transformations.
Data Quality and Governance Ensure data quality and adherence to data governance policies and standards. Collaborate with data analysts, data scientists, and other stakeholders to understand their data requirements and provide them with clean, reliable, and well-structured data. Implement data validation checks, data profiling, and data cleansing processes as necessary.
Infrastructure Management Collaborate with the DevOps or infrastructure team to manage and optimize cloud-native infrastructure resources. This includes provisioning and configuring data storage systems like AWS S3, Google Cloud Storage, or Azure Data Lake, as well as managing compute resources such as AWS EMR, Google Cloud Dataproc, or Azure HDInsight for big data processing.

What Wed Like To See.

Data Engineer Skills Google Cloud Platform GCP Expertise In-depth knowledge and hands-on experience with GCP services such as BigQuery, Dataflow, PubSub, Cloud Storage, Dataproc, Data Catalog, and others. 
Strong understanding of data engineering concepts, data modeling, data warehousing, ETLELT processes, data integration techniques, and best practices. 
Proficiency in designing and building scalable and efficient data pipelines using GCP tools and technologies, ensuring optimized data ingestion, transformation, and loading processes. 
Strong programming skills in languages such as Python, Java, or Scala for data engineering tasks, scripting, and automation.
Proficient in writing complex SQL queries, optimizing query performance, and utilizing query tuning techniques for efficient data retrieval and manipulation. 
Expertise in designing and optimizing data storage systems on GCP, including data partitioning, schema design, and performance tuning.

 
About Huge
Huge is a creative consultancy powered by human and AI collaboration. We partner with the worlds most ambitious brands to Make Huge Moves, which are creative solutions that deliver powerful outcomes. Huge helps clients unlock meaningful growth in areas ranging from AI business consulting and organizational strategy, brand and customer experience, technology advisory and strategy, to high-value audience analysis and product innovation. Founded in 1999 in Brooklyn, NY, Huge has more than 1,200 employees working across North America, Europe, Asia, and Latin America. The consultancy is part of the Interpublic Group of Companies. For more information, visit www.hugeinc.com. 
Huge is committed to creating an inclusive employee experience for all. Regardless of race, gender, religion, sexual orientation, age, disability, or if youre parenting the next generation of innovators, we firmly believe that our work is at its best when everyone feels free to be their most authentic self.
Huge is an equal opportunity employer EOE. We strongly support diversity in the workforce. We are committed to an inclusive, barrier-free recruitment and selection process and work environment. If you are contacted for a job opportunity, please advise us of any accommodation needed to ensure you have access to a fair and equitable process. Any information received relating to accommodation will be addressed confidentially.
LI-POST LI-Remote

"
https://startup.jobs/data-engineer-tech-lead-enable-4553676,Engineer,Data Engineer - Tech Lead,Enable ,"Toronto, Canada",Full-Time,"

Do you want to design new ways of processing Enterprise scale data at speed, introduce leading edge technologies, invent complex big-data algorithms, shape processes into a growing engineering organisation, all while helping to scale a Series C rocket ship to the next level?

Then welcome to Enable 

What is Enable

Enable is the SaaS rebate management platform that drives trusted relationships between B2B trading partners. We create money for our customers by providing them with the technology solutions to automatically detect and report on rebate due. Customers configure their deals, Enable ingests and process all their sales transactions, allowing them to find rebates they are owed that they would otherwise have missed.

All this has major challenges, we process enormous amounts of data in very short time frames, performing billions of calculations per customer and storing it all in Enterprise scale databases. We provide customers with reporting, deal editing and collaboration capabilities. There are no standard techniques for doing this, we are the market leader, and we create new solutions every day.

We launched our flagship product in 2016 and have raised 156m to date in Series A, B  C funding. We are continually growing our client base, product portfolio and hyper-talented team.

What our day looks like

       Working as part of an enthusiastic, fast paced and motivated agile data engineering team that takes pride in delivering a high-quality data platform.
       Coaching and mentoring other engineers.
       Design, develop, test, and deploy data pipelines and integrations using IPaaS technology.
       Lead the engineering our platform ingestionJA1 , orchestration, data warehousedata lake and API strategies for our data management ecosystem.
       Interface with other technology teams to design and implement robust products, services and capabilities for the data platform making use of infrastructure as code and automation. 
       Own and drive forward improvements to data engineering at Enable.
       Collaborate with our DevOps team to provide high availability data solutions that scale geographically.

All about You

Do you want to leverage your expertise and experience in a vibrant environment where teamwork, creativity, diversity, inclusivity, ownership, and technical excellence are expected and enjoyed? WeJA2  are seeking someone who is eager to take their data engineering career to the next level.

This is an exciting role that offers a competitive remuneration package with excellent career and growth prospects. An opportunity to join and help grow an organisation that is using modern technology to positively impact people and businesses all over the globe. You will work collaboratively with other engineers, from associates to principals, applying your technical and problem-solving capabilities to scale our data platform.

Youll work as a technical leader within the data engineering team to uphold and evolve common standards and best practices, collaborate to ensure that our data solutions are complementary and not duplicative. contributing to technical design, implementation, testing, deployment, and ongoing support and maintenance of our data platform. By going above and beyond implementing new features, we focus on customer experience, building high-quality, secure, and scalable software. Youll use your full range of skills and further develop them and those of your colleagues, including

       problem-solving, and the ability and confidence to hack their way out of tight corners.
       Peer code reviews.
       Modern big data architecture design.
       Ability to prioritise and meet deadlines.
       Attention to detail and solid written and verbal English communication skills.
       Willingness and an enthusiastic attitude to work within existing processes  methodologies.


We want all our people to be whoever they want to be and are committed to creating a truly inclusive culture at Enable. We believe that bringing your full authentic self to work helps us to build the best quality software, and by creating a truly diverse workforce we bring innovation into everything we do.

Skills and Experience

This is a leading technical role focused on the development of our SaaS products suited to a highly focussed, ownership driven Engineer. Development is a small part of our engineers responsibilities and youll be expected to contribute to all areas of our Engineering work including product and feature design, leading and mentoring, and helping us to continually improve.
Youll have focussed professional experience as a data engineer, preferably in Cloud-based SAAS products. Ideally, youll have at least 5 years of experience here, but we focus on skill and ability, not tenure.
 
       Big Data processing. We process enterprise scale volumes of data having experience of ways of working with these for example Snowflake, Databricks, big query.
 
       Architecture design. Experience in leading the design and implementation of lambda architectures for scalable and reliable data processing.
 
       Data Stores. Technical excellence in SQL, NoSQL, Blob, Delta Lake, and other enterprise scale data stores.

Data Orchestration. Enterprise scale usage of technology such as Azure Data Factory, Logic Apps, DBT, SnapLogic, Spark or similar tools.
 
       Software tooling. GITGitHub, CICD, deployment tools like Octopus, infrastructure as code and other DevOps practices.
 
       Scrum methodologies. experience of working within scrum frameworks and tools such as JIRA and Confluence.
 
       Cloud. We are Azure hosted and experience of this is a strong preference. However other Cloud platforms like AWSGCP are acceptable.
 
       Coding. Experience using Python with data Pandas, PySpark would be an advantage. Other languages such as C would be beneficial but not essential
 
Location

We are a hybrid-working engineering team, with a relaxed and fun culture. Our office is in Downtown Toronto.

Benefits

In addition to a competitive salary, Enable offers a comprehensive benefits package including

       Fantastic holiday entitlement
       Flexible and hybrid working
       Pension contributions and life insurance
       Employee equity scheme and colleague bonus plan
       Quarterly personal-wellness budget
       Regular social events
       Free lunches, snacks, and drinks
       Significant investment in skills development and training
       Enhanced Family Friendly Policy Maternity pay, Adoption and Paternity leave
       Join any of our 5 DEI workstreams that are helping to shape how we can continue to support and celebrate our people, truly embodying our key value of ""We Succeed Together""
 
Our Commitments

Do you need any reasonable adjustments throughout the interview process? Our Talent team will be happy to support you in the best way possible if you feel comfortable raising this. If you have any questions about any of the policies we have in place to support our employees just let our Talent team know.


Enable Global Inc provides equal employment opportunities EEO to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity, national origin, age, disability, genetic information, marital status, amnesty or status as a covered veteran in accordance with applicable federal, state and local laws. Enable complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.

Enable expressly prohibits any form of unlawful employee harassment based on race, color, religion, gender, sexual orientation, national origin, age, genetic information, disability or veteran status. Improper interference with the ability of Enable employees to perform their expected job duties is absolutely not tolerated.

LI-Hybrid

"
https://startup.jobs/data-engineer-hc1-4553258,Engineer,Data Engineer,hc1 ,"Indianapolis, United States",,"

SummaryThe Data Engineer will be a critical team member in designing, implementing, and documenting complex application integrations within the hc1 platform. This role reports to the VP of Integrations and works within the Engineering and Data  teams. This role requires the individual to be very methodical and have strong coding and troubleshooting skills.  If you work comfortably in fast moving situations and love to stay current with technology, then we want to hear from you!
What youll do

Design, develop, and maintain data pipelines, ETL processes, and data integration workflows using AWS Snowflake and MySQL databases
Collaborate with cross-functional teams to gather data requirements and ensure data quality and integrity throughout the data lifecycle
Optimize database performance, conduct tuning, and identify areas for improvement to enhance overall system efficiency
Work with application owners and developers to support development efforts

What you bring

A bachelors degree in Computer Science or equivalent work experience
Proven experience as a Data Engineer, working with AWS Snowflake and MySQL databases
3 years experience in Data Platform Administration, Engineering, or related field
Experience in performing database monitoring, maintenance, reorganizations, resource management, schema management, and capacity
Experience with modeling business requirements
Knowledge of AWS infrastructure including Redshift, Aurora, and Kinesis
Knowledge with writing code in Python or similar scripting language

Tech Stack You Will Be Using

MySQL
PostgreSQL
Python
JavaScript
Snowflake
AWS Data Processes
Linux


Who are wehc1 Insights hc1 is the leader in identifying real-time insights and risk signals in complex laboratory data. hc1 solutions empower thousands of laboratory locations to leverage the insights within their own data to optimize operations and inform testing and treatment decisions for millions of patients. 
Why work with us?

Unlimited PTO, remote friendly and flexible hours
Paid Medical  Parental Leave
401k Matching
Career planning, mentorships and professional development
Stock Options
Volunteer Opportunities
Wellness Programs

Compliance TrainingAll of our employees are responsible for adhering to federal and state privacy regulations. You will be required to Complete annual regulatory training and certification as required for the Healthcare Insurance Portability and Accountability Act HIPAA and other applicable regulations and policies as determined by hc1.
Join our Talent CommunityGet a glimpse into our culture and perks by visiting hc1.comcareers and follow us on Instagram hc1dotcom to get a day in the life at hc1.
hc1 is an equal opportunity employer committed to diversity and inclusion. We are pleased to consider all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected veterans status, AboriginalNative American status or any other legally-protected factors. Disability-related accommodations during the application process are available upon request.


"
https://startup.jobs/data-engineer-uworld-llc-4553212,Engineer,Data Engineer,"UWorld, LLC ","Hyderabad, India",Full-Time,"

UWorld is a worldwide leader in online test prep for college entrance, undergraduate, graduate, and professional licensing exams throughout the United States. Since 2003, over 2 million students have trusted us to help them prepare for high-stakes examinations.
We are seeking a Data Engineer who is passionate about creating an excellent user experience and enjoy taking on new challenges. The Data Engineer will be responsible for the design, development, testing, deployment, and support of our Data Analytics and the Data warehouse platform.
Requirements
Minimum Experience 
 MastersBachelors degree in Computer Science or a related field. 

Minimum Experience
 5 years of experience as a Data Engineer with experience in Data Analysis, ingestion, cleansing, validation, verification, and presentation ReportsDashboards 3 years of working knowledgeexperience utilizing the following Python, SparkPySpark, Big Data Platforms Data bricksDelta Lake, REST services, and MS SQL ServerMySQL, MongoDB, UnixLinux Shell scripting, Azure Cloud and machine learning. Experience with SQL, PLSQL, and Relational Databases MS SQL ServerMySQLOracle. Experience with TableauPower BI, NoSQL MongoDB, and Kafka is a plus. Experience with REST API, Web Services, JSON, Build and Deployment pipelines Maven, Ansible, Git, and Cloud environments Azure, AWS, GCS is desirable. 

Job Responsibilities
The software developer will perform the following duties
 Understand data services and analytics needs across the organization and work on the data warehouse and reporting infrastructure to empower them with accurate information for decision-making. Develop and maintain a data warehouse that aggregates data from multiple content sources, including Salesforce, NoSQL DBs, RDBMS, social media, other 3rd party web services RESTful, JSON, flat-file stores, and application databases OLTPs. Use Python, SparkPySpark, Data Bricks, Delta Lake, SQL Server, Maria DB, Mongo DB, Jira, GitBit Bucket, Confluence, Data BricksDelta Lake, REST services, Tableau, UnixLinux shell scripting, and Azure Cloud for data ingestion, processing, transformations, warehousing, and reporting. Develop scalable data pipelines using Data connectors, distributed processing transformations, schedulers, and data warehouse Understanding of data structures, analytics, data modeling, and software architecture Develop, modify, and test algorithms that can be used in scripts to store, locate, cleanse, verify, validate, and retrieve specific documents, data, and information Develop analytics to understand product sales, marketing impact, and application usage for UWorld products and applications Employ best practices for code sharing and development to ensure common code base abstraction across all applications. Continuously be up to date on the industry standard practices on big data and analytics and adopt solutions to the UWorld data warehousing platform. Work with QA engineers to ensure the quality and reliability of all reports, extracts, and dashboards by process of continuous improvement. Collaborate with technical architects, developers, subject matter experts, QA team, and customer care team to drive new enhancements or fix bugs in a timely manner. Work in an agile environment such as Scrum 

Soft Skills
 Working proficiency and communication skills in verbal and written English Excellent attention to detail and organization skills and ability to articulate ideas clearly and concisely Ability to work effectively within a changing environment that is going through high growth Exceptional follow-through, personal drive, and ability to understand direction and feedback Positive attitude with a willingness to put aside ego for the sake of what is best for the team 

"
https://startup.jobs/data-engineer-lyft-4552605,Engineer,Data Engineer,Lyft ,"Prague, Czechia",,"

At Lyft, our mission is to improve peoples lives with the worlds best transportation. To do this, we start with our own community by creating an open, inclusive, and diverse organization. Lyft is a global ecosystem of dynamic workplaces, and our Czech location is no different. We have an ambitious goal to strengthen our international presence by growing a life-changing product, and your efforts will play an essential role in our collective success. 
Your next contribution might allow access to business and user behavior insights, using huge amounts of Lyft data to fuel Marketplace.
Marketplace teams are at the heart of our products and decision-making. Were looking for passionate, driven engineers to build systems that empower our users both Drivers and Riders to make the most effective use of Lyfts products and experiences by making them more predictive, personalized, and adaptive. Were looking for someone who is passionate about solving problems with distributed computing, ML, data alongside building reliable systems, and is excited about working in a fast-paced, innovative, and collegial environment.
Responsibilities

Assemble and manage large, complex sets of data that meet non-functional and functional business requirements
Design and evolve data models and data schema based on business and engineering needs
Build and support ETL pipelines using tools like Spark, Trino Presto, Airflow, Flyte, and SQL technologies
Implement systems to track data quality and consistency
Build analytical tools that provide insight into key performance metrics across Lyft
Work with stakeholders across science, engineering, data infrastructure, ml, product, and our leadership, driving resolving data-related technical problems

Experience

4 years of experience in software engineering, ideally with a focus on Data EngineeringArchitecture
Ability to work with complex production-quality software
Technical expertise in data infrastructure, cloud computing, storage systems, and distributed computing frameworks in multi-petabyte scale systems

Knowledge of modern data infrastructure and tooling.Today we use S3, Spark, DynamoDB, Trino Presto, Kafka, ElasticSearch, Flyte, Airflow, Stackdriver, Amundsen, and SageMaker. We do not expect you to know them all, but would like for you to be familiar with some

Experience of system stability monitoring and incidents handling 
Openness to new or different ideas, and the ability to evaluate multiple approaches and choose the best one based on fundamental qualities and supporting data
Ability to communicate highly technical problems working along our cross-functional team
Ability to communicate in English in various forms e.g technical document, meetings, presentations

This role will be in-office on a hybrid schedule if an established Lyft Location is available to the region  Hybrid Team Members will be expected to work in the office 3 days per week on Mondays, Wednesdays, and Thursdays. Additionally, hybrid roles have the flexibility to work from anywhere for up to 4 weeks per year. 

"
https://startup.jobs/data-engineer-bullhorn-inc-4551485,Engineer,Data Engineer,"Bullhorn, Inc. ",,,"


Bullhorn is the global leader in software for the staffing industry. After more than 20 years, more than 10,000 companies rely on Bullhorns cloud-based platform to power their staffing processes from start to finish. Led by the original co-founder, partnered with venture capital, and powered by seasoned leaders across a global workforce with an eye toward innovation, Bullhorn has had year over year growth, making it the market leader in the recruitment software space while allowing for new opportunities for over 35 of our employees to advance their careers in 2021. 



We are a remote-first organization and over 30 of our employees reside outside the United States. Headquartered in Boston, we also have offices in St. Louis, London, Brighton, Rotterdam, Frankfurt and Sydney just in case youre in the area to stop by. Whether youre local or remote, our vision is to ensure every employee has a sense of belonging, a voice that is heard, and a clear path for success. Your incredible experience as an employee will consist of flexible work hours to ensure a positive work-life balance and use Zoom, Slack, and other tools to stay connected. 



As a Data Engineer, you will positively impact the business by transforming operational data into analytical data for the business to derive insights and solve complex problems. You will provide technical expertise to the Enterprise Data Team; assisting with writing schema changes, data cleansing and other database design work as well as building and maintaining ELT pipelines and data models for Analytics and Business Intelligence solutions.
A typical day might include

A stand-up with your team to review product specifications, requirements and progress
Assisting the data warehouse team with writing schema changes and other database design work, geared for scalability and performance.
Research and troubleshoot data inaccuracies and create plans to fix them
Creating and maintaining ELT pipelines with Fivetran, Python and dbt
Creating and maintaining Data Models in Snowflake for analyticsbusiness intelligence solutions
Champion a data-driven culture across the enterprise developing methods and controls to ensure consistent application and use of data
Interface directly with internal clients and project team members to understand the needs and objectives of the business

This role may be a fit for you if you have

3 years proven experience writing complex SQL queries
2 years proven experience with Python
2 years of extensive hands-on experience with designing, developing, and ongoing support of data warehouse environments
2 years of experience in Data Engineering, Data Analyst, BI Analyst or similar role
Comfort working independently in a fast-paced, flexible environment.
Strong customer relations and ability to deliver business value with technical excellence
Ability to clearly communicate data, context, and implications to business and technology stakeholders
Excellent verbal, written communication and presentation skills

Bonus points for

Bachelors degree in Computer Science, Information Technology, Business Analytics or related discipline
Experience with Cloud Data Warehouse platforms such as Snowflake, Big Query or Azure Synapse 
Experience with Qlik Sense, Tableau or other BI tools
Experience writing data transformations using dbt
Exposure to dealing with the challenges of large relational data sets, including load performance, query performance, etc.


divb What we offer...bdivdivbrdivdivspan -Benefits eligibility effective DAY ONE including Medical, Dental, Vision, 401k, 401k Match, and morespandivdivspan -Unlimited Vacationspandivdivspan -Mental health benefits EAP  98point6spandivdivspan -Full Access to LinkedIn Learningspandivdivspan -Quarterly paid volunteer daysspandivdivspan -Lucrative Employee Referral Program eligible for prior to your first dayspandivdivspan -Career development opportunities upacross Bullhorn spandivdivbrdivdivspan Bullhorns core purpose is to create an incredible customer experience, which starts with first creating an incredible employee experience. Our vision is for every employee to have a sense of belonging, a voice that is heard, and a clear path for success. We are committed to building diverse and inclusive teams, and our culture is shaped by our five core values Ownership, Energy, Speed  Agility, Service, and Being Human.spandivdivbrdivdivspan Were looking for real-life humans, each with their own unique set of thoughts, beliefs, cultures, identities, and a background and body that is completely individual. We also love humans who have taken less traditional paths of education and believe that experience and learning come in many forms. Together, all these unique individuals make Bullhorn stronger. If youre reading this, youre probably applying forconsidering applying for a job with us, and we want you to know that Bullhorn is an equal opportunity employer. For us, that means we always have, and will always, strive to be as inclusive as possible in all aspects of employment and that we do not and will not tolerate discrimination of any kind.spandivdiv

"
https://startup.jobs/data-engineer-capco-4551026,Engineer,Data Engineer,Capco ,"Houston, United States",,"

About the team
Capcos Data Team helps our clients transform every aspect of their business.  We are highly skilled at formulating data strategy, defining business and technology initiatives across the data management lifecycle, and aligning multi-year strategic roadmaps with clients business goals. As digital technologies advance and regulations tighten, todays consumers  and, therefore, todays businesses  are becoming more aware of the importance of good quality data. We work to establish holistic ways to effectively manage data through the modern data supply chain and facilitate consumption through analytics, modelling, AI, machine learning, dashboarding, and reporting.
About the Job
As a member of our Data Team, you will work across Capcos different domains and solution offerings to help break down large problems, develop approaches and solutions. As a Data Engineer, you will create analytics reporting and provide data-driven strategic insights, trends, and perspective to help drive transformation for our clients.
What Youll Get to Do

Work on hard problems with smart people.
Be highly motivated, result-oriented, and take pride in being a problem solver.
Work with new technology, focus on using the right tool for the job, rather than any sticky preference for a tool or technology.
Linux system administration, development and production environments. 
Learn and share knowledge across our engineering teams, so we can continue to iterate and improve.
Write reusable, testable, and efficient code as needed.
Design and implement of low-latency, high-availability, and performant database systems.
Work on implementation of data pipelinesdata warehousesdata martsODSsLakehouses.
Collaborate and work on integration of data storage solutions.
Cloud, container and microservices infrastructures. 
Software security.
Focus on performance tuning, improvement, balancing, usability and automation.

What Youll Bring with You

Data Analysis  Intermediate to expert level with algorithms and complexity analysis. 
Data Engineering -using Microsoft Azure cloud and on prem SparkHadoop Development.
Data Engineering Foundation  Certification nice to have, theory and practice.
Real-time event processing.
Proficient in Azure technologies such as Azure Data Factory ADF, Azure Data Bricks ADB, Azure Active Directory, Azure Storage, Azure data Lake Services ADLS, Azure key vault, Azure SQL DB, Azure HDInsight and OR AWS.
VBA, Python, PowerBi, Excel.
Functional programming.
Data Engineering using Microsoft Azure cloud and on prem SparkHadoop Development
Experience in Energy Trading andor experience in the competitive Gas and Power markets a plus.

Why Capco?
A career at Capco is a chance to help reshape the competitive landscape in financial services.  We launch new banks, transform existing ones, and help our clients navigate complex change.  As consultants, we work on the front-end business design all the way through to technology implementation.
We are the largest Financial Services focused consultancy in the world, serving everyone from global banks to emerging FinTechs, from strategy through digital transformation, design, business consulting, data and analytics, cyber, cloud, technology architecture, and engineering.
Capco is a young and growing firm. We maintain an entrepreneurial spirit and growth mindset and have minimal bureaucracy. We have no internal silos that get in the way of your career opportunities or ability to focus on our clients and make a difference to the business.  We offer the opportunity for everyone to learn rapidly, take on tough challenges, and get promoted quickly. We take pride in our creative, collaborative, diverse, and inclusive culture, where everyone can BYAW.
We offer highly competitive benefits, including medical, dental and vision insurance, a 401k plan, tuition reimbursement, and a work culture focused on innovation and creation of lasting value for our clients and employees.
Ready to take the Next Step
If this sounds like you, we would love to hear from you. This is an opportunity to make a difference and contribute to a highly successful company with a significant growth trajectory.  

"
https://startup.jobs/data-engineer-hybrid-nyc-wagmo-2-4550938,Engineer,Data Engineer - Hybrid NYC,Wagmo ,"New York, United States",,"

Please note while this role is remote, candidates in the NYC metro area are preferred. Unfortunately we cannot sponsor at this time
Wagmo is growing our product features, backend operations and the ways we continue to service our customers to ensure we make pet wellness and care available to all pets and pet parents. We have a diverse, lean team and a culture where we take work seriously without taking ourselves too seriously. As we grow, we are looking to bring on our first Data Engineer. 
This role is hybrid, 2 days a week in our NYC office.
Our Tech Stack Sigma BI, Python and Go Golang, Javascript, React, Next.js, Flutter, Google Cloud, Kubernetes
What Youll Do


Reporting to the Head of Engineering, research and recommend new BI tool


Evaluate current data models and business requirements


Collaborate and coordinate with engineering and business stakeholders to identify how we should capture data for new initiatives, as well as influence how we should implement changes that would affect existing data


Design, build, and maintain the data pipelines and infrastructure required to support the organizations analytics and BI initiatives, along with normalizing the data


Conduct AB in order to build state-of-the-art products


Initiate analysis and generate reports to assist with feature tracking, experiments, and productplatform health monitoring


What Youll Need to Be Successful


Cloud-based DWH experience


Solid understanding of Dbt - data build tool - highly preferred


Data scripting languages such as sql, jinja2, python desired


Understanding microservices architecture desired


Taking ownership, and responsibility for integrating user domain data


Knowledge of ELT tools such as Airbyte, Airflow, Talend stitch, 


Key Benefits


Pay Range 130-160k  Equity in the company


Company paid medical premiums


Dental, vision, voluntary life, short term disability and long term disability


Company paid Wagmo pet wellness and insurance plans


Unlimited paid time off


12 weeks parental time off


ClassPass subsidy


401k


Company wide open feedback model


 

"
https://startup.jobs/data-engineer-core-data-platform-getyourguide-2-4550259,Engineer,"Data Engineer, Core Data Platform",GetYourGuide ,"Berlin, Germany",,"

GetYourGuide is the place to book the best experiences in destinations across the globe. We are now looking for a full-time Data Engineer, Core Data Platform at our Berlin office.
You will play a key role in designing and developing our data ecosystem, big data-pipelines, data tooling and processes. You will also help make the vast amount of our data accessible, reliable and efficient to query. 
About GetYourGuide
GetYourGuide is the globally leading marketplace for unforgettable travel experiences. Travelers use GetYourGuide to discover the best things to do in a destination. Since its founding in 2009, travelers from over 170 countries have booked more than 80 million tours, activities, and attraction tickets through GetYourGuide. Powered by a global team of over 700 travel experts and technologists, we are headquartered in Berlin and have 17 local offices around the world. Visit our careers website to learn more.
Team mission
The Data Platform team plays the central role in our data-driven strategy, designing and developing our data ecosystem, data processes and analytics capabilities. We work on problems such as product events tracking, self-service ETL enablement, and real-time streaming applications. The team is also responsible for making the vast amount of data accessible, reliable and efficient to query, so we are empowered to make the best data-driven decisions. We work with a wide variety of tools and technologies ranging from Spark, Airflow, and Kafka to Databricks, Looker and Snowflake, making this team a great place to learn new skills.
You will

Design, build and refine our data and analytics infrastructure, scaling to petabytes of data
Grow our analytics capabilities with faster, more reliable data pipelines, and better tools
Work together with Business Intelligence, Analytics, and the whole Engineering department to ensure high data quality
Design and develop a real-time events pipeline and work with data scientists, backend engineers and product managers to find new ways to leverage this data
Develop complex and efficient pipelines to transform raw data sources into powerful, reliable components of our data lake
Optimize and improve existing features or data processes for performance and stability

Who you are

2-3 years of experience as a DataSoftware Engineer 
Experience designing, developing and maintaining systems at scale
Strong experience with Scala or Java
Well-versed in both big data e.g. Spark and backend frameworks
You write efficient, well-tested code with an keen eye on scalability and maintainability
Familiar with agile development
You have an analytical mind and bake your decisions with data
Excellent written and verbal communication skills in English

Nice to have 

Experience with Python
Experience working with Airflow
Experience with AWS and cloud technologies
Experience building stream-processing systems, using solutions such as Kafka and Spark Streaming

How we set you up for success

Invest in your development with an annual personal growth budget
Create a comfortable workspace at home with an annual home office budget
Become a part of our success with virtual stock options
Enjoy flexibility with a hybrid work-from-home and telecommuting policy
Save on transportation costs with discounted public transportation tickets
Support your loved ones with generous maternity and paternity leave policies

And more...
We look forward to hearing from you
Unlock your full potential and join our mission to create unforgettable experiences for millions around the world. If you have the skills and passion for joining our team, we invite you to apply by submitting your CVresume in English through the form below. Check out how we hire for tips and visibility into our process and check out life at GetYourGuide. If you have any further questions, please do not hesitate to contact us via jobsgetyourguide.com.
We are an equal opportunity employer
Our commitment is that every qualified person will be evaluated according to skills regardless of age, gender identity, ethnicity, sexual orientation, disability status, or religion. Please refrain from including your picture and age with the application. 
LI-Hybrid

"
https://startup.jobs/data-engineer-data-platform-grammarly-4549240,Engineer,"Data Engineer, Data Platform",Grammarly ,,,"


Grammarly is excited to offer a remote-first hybrid working model. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.
All roles have an in-person component Conditions permitting, teams meet 24 weeks every quarter at one of Grammarlys hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Krakw. This flexible approach gives team members the best of both worlds plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.

Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hubs where the team is based.
The opportunity 
Every day, tens of millions of people and 50,000 professional teams worldwide trust Grammarlys AI and human expertise to help ideate, compose, revise, and comprehend communications. Our team members have the autonomy to take on exciting challenges in pursuit of our mission to improve lives by improving communication. Together, were building on more than a decade of steady growth and profitability. Were defining the communication assistance with our tailored service offerings Grammarly Free, Grammarly Premium, Grammarly Business, and Grammarly for Education. Our latest product offering, GrammarlyGO, brings the power of generative AI to our users. It all begins with our team collaborating in an inclusive, values-driven, and learning-oriented environment.
To achieve our ambitious goals, were looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.
Grammarlys engineers and researchers have the freedom to innovate and uncover breakthroughsand, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.
Your impact
As a Data Engineer on our Data Engineering Platform team, you will  

Drive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users. 
Make analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.
Model structure, storage, and access of data at very high volumes for our data lakehouse.
Improve developer productivity and self-serve solutions by contributing components to our stream data processing frameworks.
Own data engineerings infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.
Build a world-class process that will allow our systems to scale.
Mentor other back-end engineers on the team and help them grow.
Build and contribute to AWS high-scale distributed systems on the back-end.

Were looking for someone who

Embodies our EAGER valuesis ethical, adaptable, gritty, empathetic, and remarkable.
Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.
Has experience with Python, Scala, or Java.
Has experience with designing database objects and writing relational queries
Has experience designing and standing up APIs and services.
Has experience with system design and building internal tools.
Has experience handling applications that work with data from data lakes.
Has at least some experience building internal Admin sites.
Has good knowledge of and at least some experience with AWS or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame.
Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.

Support for you, professionally and personally


Professional growth We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.


A connected team Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER ethical, adaptable, gritty, empathetic, and remarkable values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs. 


Compensation and benefits
Grammarly offers all team members competitive pay along with a benefits package encompassing the following and more 

Excellent health care including a wide range of medical, dental, vision, mental health, and fertility benefits
Disability and life insurance options
401k and RRSP matching 
Paid parental leave
Twenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days 
Home office stipends
Caregiver and pet care stipends
Wellness stipends
Admission discounts
Learning and development opportunities

Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic regions cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information. 
Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future. 
United States
Zone 1 167,000 - 242,000year USD
Zone 2 150,000  218,000year USD
Zone 3 142,000  206,000year USD
Zone 4 134,000  194,000year USD
We encourage you to apply
At Grammarly, we value our differences, and we encourage allespecially those whose identities are traditionally underrepresented in tech organizationsto apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program US. We also abide by the Employment Equity Act Canada.
Please note that EEOC is optional and specific to US-based candidates.
NA
LI-DT1

All team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19.
LI-Hybrid


"
https://startup.jobs/data-engineer-amino-inc-4549198,Engineer,Data Engineer,"Amino, Inc. ",,,"





About Amino - What We Do
At Amino, we are a leading innovator in healthcare, empowering individuals to take charge of their healthcare journey. Our powerful digital tools help navigate and unlock the full potential of health plan options. With Aminos cutting-edge solutions, health plan members can easily understand and make informed decisions about their benefits, considering factors such as available programs, provider network quality, and cost.
We are excited to announce that we have recently secured additional funding to fuel our growth and continue developing market-leading navigation tools. As part of this journey, we are looking for a talented and driven  Data Engineer to join us. You will play a vital role in helping millions of people understand and optimize their health plan options.
The Role
As a Data Engineer at Amino, you will be at the heart of our organization. Your primary responsibilities will involve processing, transforming, and organizing large and complex datasets that power our products. We are expanding our data sources and enhancing our current offerings, and we need a Data Engineer to help us build competitive and compliant healthcare guidance products.
Key Priorities and Projects

Collaborate with Product Managers, Architects, and senior leadership to translate high-level requirements into detailed plans.
Integrate new data sources from vendors and internal systems into our ETL Extract, Transform, Load pipelines.
Enhance our editorial tools for creating and maintaining accurate data about health entities.
Modernize our orchestration infrastructure using open-source tools like dbt and Dagster.

Impact you will have

Revamp and expand critical data pipelines that drive our existing and future products.
Mentor other junior data engineers and share your expertise.
Collaborate closely with data scientists and software engineers in related teams.
Influence the foundational aspects of data modeling and warehousing, benefiting various teams across the company.
Utilize modern data stack technologies to build the foundation for Aminos next-generation data assets.

Technologies you will work with
Snowflake, Python 3, dbt, Docker, AWS, Looker, Databricks, Spark, Jenkins CICD, Airflow, Dagster, Terraform, PostgreSQL, RDS, Elasticsearch, and Kinesis.
Skills and Experience you should possess

Familiarity with various relational and non-relational databases.
Proficiency in writing clean and well-tested code using Python.
Experience handling and working with large datasets and the associated tools.
Previous exposure to ETL pipelines and familiarity with data modeling and data warehousing best practices experience with dbt is preferred.
Strong collaboration and feedback skills, with a willingness to seek and incorporate input from others.
Excellent documentation and verbal communication skills, capable of conveying technical concepts to peers and non-technical stakeholders.

Bonus Skills

Experience handling PHI or PII
Deep understanding of the healthcare domain, particularly with healthcare claims data.

We offer


Were committed to helping you achieve your best work in a supportive, growth-oriented environment. We have seriously big goals, and expectations are high and well equip you with the tools and resources you need to be successful. 

Expected base salary 150k to 180k plus standard company benefits and a generous option grant. Amino values transparency and has included the reasonable estimate of the base salary range for this full-time role at any approved US location. Individual pay is determined by a range of factors, including job-related skills, experience, relevant education or training, licensure or certifications, and other business and organizational needs. Amino does not typically hire at or near the top of a salary range.

We offer full-time employees 100 paid employee healthcare premiums; dependent premium coverage depends on the plan.

401k and FSA programs

This position, like all roles at the firm, will have a good deal of autonomy. Were a remote-first team and have designed our culture for a balance of synchronous and asynchronous work with people operating from all over the country. To support your remote office, we provide every new Amino with a generous office set-up allowance plus a monthly stipend for internetphone.


PTO is non accrual and we expect Aminos to take a minimum of 15 days a year. 



We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We know the reputation and track record that the tech industry has, and work hard to be exceptional in this regard.



Our Culture
We are a small team who believes that success is a group activity. You should expect to learn from everyone at Amino, and be excited to share your knowledge. You will play a big part in influencing the shape of the product and be empowered to provide your thoughts and ideas.
We believe in collaboration, respect, and curiosity. We believe in having a growth mindset, and have a passion for solving problems that have never been faced before. Everyones input is valued, be it about code, data models, business models, or product ideas.
 
 

 

"
https://startup.jobs/data-engineer-digital-on-us-4548453,Engineer,Data Engineer,Digital On Us ,,,"



Who is Tech Mahindra?
At Tech Mahindra, we have a culture of driving positive change, celebrating each moment, and empowering all to Rise drives us to dream, do, and become more. By living our culture, both as individuals and as a team, we establish and advance our presence as a brand that is global, innovative, and caring. Our culture also leads the way for us to be and become a Company with a Purpose. We try to achieve this by making responsibility personal and adopting sustainability as a way of life at Tech Mahindra.
Our service offerings are aligned to the changing world of our customers. Our portfolio of services range from designing strategy to delivering impact. We are Software Engineers, Technical Architects, Cloud and DevOps specialists. But the most important, we are dreamers, creators and challengers. Each day, we strive to make great come alive.
Our technology partners are Hashicorp, Google Cloud, AWS, Oracle, SFDC, BMC, HPE, Pega, VMware, Cisco, IBM, SAP, Dell EMC, Microsoft and Service Now.
At Tech Mahindra, we are more than 140,000 employees with presence in more than 90 countries. Our DigitALL philosophy focuses on transforming clients businesses across Products, Services, Business Models and Reimagined Business Processes, leading to new Revenue Opportunities, Enhanced Customer Experience, Operational Efficiency, Reduced Risk, and a better Society.
We are always looking for the brightest candidates to come and we offer a work environment with everything you need to be your best. Does Ambition, Success, Fun, Friends  Learning define your idea of a career? Join us and be part of our family!
Were looking for a passionate, product-focused Data Engineer who has hacked systems in creative ways, and who are curious about new languages, technologies, and trends. We believe that working hard is a byproduct of loving what you do and not something that can be assured on a timesheet.
Location Mexico - Remote
Qualifications we are looking for
5 years of experience

Spark
Scala 

Nice to have

Azure

Responsibilities


Create, join, filter and transform Data frames efficiently
Implement Spark pipelines
Understand the primary types of objects  structures like Case Classes, Tuples and Collections
How to interact with Collections to transform them map, flat map, foreach, etc.
How to deal with NULLS that are discouraged in Scala.



What you can expect from us?
At TechMahindra, what distinguishes us from other teams is the comfortable environment which engenders trust within teams and with our customers. Trust and openness leads to quality, innovation, commitment to deliverables, efficiency and cost-effectiveness for all our customers.

Work with some truly remarkable IT engineers, architects, specialists and more.
Were growing at a phenomenal pace and wed like some company.
Hear your voice, nurture your talent and help you strengthen your foot print!
Benefits above the law
Mentorship, and opportunities to grow and learn

 
ID2551


If you apply for this opportunity we will get you resume and its contain personal data whose treatment has been authorized by its owner for Digital OnUs, S. de RL de CV the ""Company. If you are not the owner of this information or have no relation whatsoever with the subjects treated in it, you are requested in the most attentive way not to make copies of it and  or its attached files and delete it immediately, under the risk of being considered as responsible for the unauthorized treatment of personal data in accordance with the Federal Law on Protection of Personal Data Held by Private Parties, its Regulations, and other applicable regulations. If you are the owner of personal data in possession of the Company and wish to obtain further information regarding the processing of your personal data or the exercise of your ARCO rights, please consult our integral privacy notice on the website httpswww.digitalonus.comprivacy-policy

"
https://startup.jobs/data-engineer-freelance-remote-braintrust-4547351,Engineer,Data Engineer - Freelance [Remote],Braintrust ,,Contractor,"


ABOUT US
Braintrust is a user-owned talent network that connects you with great jobs with no fees or membership costsso you keep 100 of what you earn. 
 
ABOUT THE HIRING PROCESS
When you join Braintrust, you will be invited to a screening process for Braintrust to learn more about your previous work experiences. Once completed, you will have access to the employer for this role and other top companies that seek high-quality talent. Apply to this job to kick off the process. 



JOB TYPE Freelance, Contract Position no agenciesC2C - see notes below

LOCATION Remote - Peru TimeZone HASTCKT  Partial overlap

HOURLY RANGE Our client is looking to pay 70  90hr

ESTIMATED DURATION 40hweek - Long term
BRAINTRUST JOB ID 6736

THE OPPORTUNITY
Requirements


Must be able to overlap Hawaiian or Mountain time zones for at least 6 hours.
 
Basic Qualifications
Minimum 2 years of developing a fully operational production grade large scale data solution onSnowflake Data Warehouse.  3 years of hands-on experience with building productionized data ingestion and processingpipelines using standard data pipeline techniques Spark, Python, ADF, etc..  2 years of hands-on experience designing and implementing production grade data warehousingsolutions on large-scale data technologies such as SQL Server, Teradata, Oracle or DB2  Expertise in implementing role-based access controls on Snowflake.  Ability to write stored procedures and complex queries in Snowflake  Excellent presentation and communication skills, both written and verbal  Ability to problem solve and architect in an environment with unclear requirements  Preferred Skills  Minimum two years of experience developing data solutions on Snowflake and Airflow  Minimum two years of experience building and maintaining enterprise-grade object-orientedprogramming solutions Python, Java, C  Expertise in establishing and maintaining DataOps pipelines.
 
What youll be working on


Our client is looking for a Snowflake Developer.  Please see the job requirements below.

Snowflake data engineers will be responsible for designing and developing various artifacts in the effort of Data migration from legacy databases to the Snowflake cloud data warehouse.
A solid experience and understanding of architecting, designing, and operationalizing large-scale data and analytics solutions on Snowflake Cloud Data Warehouse is a must.
Developing solutions using a combination of Python, Airflow, and Snowflake
Developing scripts using SnowSQL  Unix  Python etc., to do Extract, Load, and Transform data
Provide support for Data Warehouse issues such as data load problems etc..
Understanding data pipeline requirements and which tools to leverage to get the job done
Testing and clearly documenting implementations, so others can easily understand therequirements, implementation, and test conditions.





Apply Now!

C2C Candidates This role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLCcorp around their consulting practice, this is well aligned with Braintrust and wed welcome your application.
Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.


"
https://startup.jobs/data-engineer-intern-moladin-4546823,Engineer,Data Engineer Intern,Moladin ,"South Jakarta, Indonesia",,"

 Support data-related projects by identifying user needs, collecting data, performing data analysis, interpretation, and visualization. Conduct data cleaning and data wrangling to prepare data for analysis and analytics modeling. Present insights and assist users in making informed decisions and identifying areas for improvement. 
Requirements
 Currently pursuing a diploma or degree in a technical or scientific field such as Software Engineering, Computer Science, Electrical Engineering, Data Science, or IT is preferred. A strong willingness to learn, work diligently, and independently is required. Familiarity with Microsoft Excel or Google Sheets. Having knowledge of coding or data science. You possess excellent problem-solving skills and analytical abilities. You are comfortable communicating in conversational English. 

"
https://startup.jobs/data-engineer-kumo-4546798,Engineer,Data Engineer,Kumo ,"Mountain View, United States",Full-Time,"

Come and change the world of AI with the Kumo team! 

The creation of the data warehouse emerged to solve the analytics problem over large amounts of data. Now, weve moved from megabytes to gigabytes to terabytes of data storage with no end in sight and companies invest millions of dollars to store and organize that data and only leverage a fraction of it for machine learning.

With Kumo, we are building the first data platform to seamlessly allow machine learning over data warehouses for faster, simpler, and smarter predictions to combat data waste and maximize data value. Query the future with Kumo.

The global data management software market is set to reach 137.6 billion by 2026, and were on a mission to make a significant impact. Were seeking intellectually curious and highly motivated Data Engineers to become foundational members of our Machine Learning and Data Platform team.
Required Qualifications for Ideal candidate

4 years of professional experience in SaaSEnterprise companies 
Strong experience with data ingestion and connectors
Experience in building end-to-end production-grade data solutions on AWS or GCP
Experience in building scalable ETL pipelines.
Ability to plan effective data storage, security, sharing, and publishing within an organization.
Experience in developing batch ingestion and data transformation routines using ETL tools.
Familiarity with AWS services such as S3, Kinesis, EMR, Lambda, Athena, Glue, IAM, RDS.
Proficiency in several programming languages Python, Scala, Java.
Familiarity with orchestration tools such as Temporal, Airflow, Luigi, etc.
Self-starter, motivated, with the ability to structure complex problems and develop solutions.
Excellent communication skills and ability to explain data and analytics strengths and weaknesses to both technical and senior business stakeholders.

Preferred Qualifications - good to have 

Deep familiarity with Spark andor Hive
Understanding of different storage formats like Parquet, Avro, Arrow, and JSON and when to use each
Understanding of schema designs like normalization vs. denormalization.
Proficiency in Kubernetes, and Terraform.
Azure, ADF andor Databricks skills
Experience with integrating, transforming, and consolidating data from various data systems into analytics solutions
Good understanding of databases, SQL, ETL toolstechniques, data profiling and modeling
Strong communications skills and client engagement


We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.

"
https://startup.jobs/data-engineer-intern-f-m-d-celtra-inc-4546346,Engineer,Data Engineer Intern (f/m/d),"Celtra, Inc. ","Ljubljana, Slovenia",Full-Time,"

Please note that a valid student status in Slovenia is required for this position. 
 
Are you curious, eager to learn, and passionate data enthusiast on a mission to help ingest and model data for better data-driven decisions in modern data architecture? Then this is a role for you!
Celtra is expanding the Data Insights team in Ljubljana, Slovenia with a talented Data Engineer Intern, who will have the opportunity to help improve our Data Landscape to enable data-driven decisions across all levels of our global company. You will have the opportunity to work with latest technologies for data integration and data wrangling while working closely with the data team colleagues as well as other engineers. You will be moving fast with the company strategy help to map Celtra data-environment.
Your responsibilities

Assist with the design, development, and maintenance of our data infrastructure using Matillion, Snowflake, Google Cloud, and AWS.
Assist with the implementing ETLELT pipelines to ingest data from a variety of sources into our data warehouse.
Assist with designing data warehouse data models.
Assist with keeping existing data sources fresh and ensuring quality data for further data manipulation.

A bit about our stack

We are based on AWS.
For ingesting data into data warehouse we are using Matillion.
We use Snowflake as our Data Stack and all functionalities that comes with Snowflake.
We use Power BI for Analysis.

You will thrive in this position if you have 

Experience with SQL and relational databases.
Programming skills in R, Scala or Python.
Basic knowledge working with various data sources SQL, Web API, XML, JSON, flat files.
Basic technical knowledge of BI system designs database designstructure and data structure star, snowflake schemas, denormalized designs.
Willingness to constantly learn and improve.
Team players.
Note valid student status in Slovenia is required for this position. 

What brings extra points?

Experience using ETL tools and knowledge of BI Systems dimensional model, data marts, etc.. 
Experience with Snowflake, Matillion.

If you do not fulfill all the criteria above, but you think you would be a great fit, we encourage you to apply either way!
What we offer?

Competitive salary.
Buddy and mentor that will help you grow professionally, plus a well structured development plan.
Unlimited remote working. You can choose to work from home indefinitely or join us in our Ljubljana office whenever you feel like it.
Flexible hours and work-life balance. We focus on results rather than schedules.
Top-notch working equipment.
We have fun. We organize regular team  company events.
Free breakfasts  beverages available in the office. 

About Celtra
Celtra helps enterprise advertisers, media, and agencies design, approve, and deliver digital creative across the ever-growing number of campaigns, markets, designs, and variants. Celtras Creative Automation  Enablement Software for helps brands move faster than ever while dramatically scaling content production. Companies like adidas, TripAdvisor, Spotify, Unilever, NBCU, Lululemon, YETI, Vice, and hundreds more partner with Celtra to cut production costs while increasing efficiencies and output in the cloud.
Empowering Creativity through Diversity  Inclusion
Our mission is to empower creativity - and we cannot fulfill our mission without different perspectives. Diversity drives innovation, and Celtra is committed to diversity, equity, inclusion, and belonging. 
Every employee is empowered at Celtra - no matter your race, age, religion, gender identity, sexual orientation, physical or mental ability, or ethnicity.  We hire the best, and develop our teams through continuous education and mentorship, in a community where everyone can bring their whole selves to work.
Benefits
If you have an interest in a company that is interested in both your wellness and your wallet medical, dental, vision, parental leave, education, fitness, 401kpension... you get the picture, you should apply. Celtra is a remote-first company with hubs located in Boston MA, New York City, London UK, Singapore, and Ljubljana SI. For more information, visit Celtra at httpwww.celtra.com or Celtra on Twitter.
 
LI-Remote
 
httpswww.celtra.comcandidate-privacy-policy

"
https://startup.jobs/data-engineer-hertility-health-4546173,Engineer,Data Engineer,Hertility Health ,"London, United Kingdom",Full-Time,"

Hertility is a womens health company built by women, for women. Were shaping the future of reproductive healthcare by pioneering unique diagnostic testing that provides data-driven and advanced insights into reproductive health, fertility decline and the onset of menopause. We provide expert advice, education and access to care - all from the comfort of your home.
31 of women will suffer from a reproductive health issue at some point and through our research, we aim to reduce the time to diagnosis through advanced at-home testing and specialist gynaecological care. We tailor pathways to the individuals reproductive goals, whether it be to explore their ovarian health and fertility options or overall hormonal health, such as confirming a PCOS diagnosis. Currently, we can screen for 18 of the most common gynae pathologies which may lead to reduced fertility and signpost gynae cancers.
Ultimately, our aim is to change attitudes around reproductive health, both for individuals and in the workplace, and to encourage women to be proactive by tracking their reproductive health. Were calling this the Reproductive Revolution! The role Were looking for a Data Engineer to help us build the worlds first data platform for women to manage their hormonal health. This is an exciting opportunity to work with a variety of data from multiple sources. You will be building out scalable data solutions for clinical services that are changing the lives of women everywhere.
Key responsibilities will include
 Structuring a data warehouse across multiple sources. Building data systems and pipelines. Developing ETL code for data cleaning and linking. Collaborating with data scientists, data analysts, product owners, backend and frontend engineers across several projects. 
Requirements
The skills, knowledge and experience were looking for include
 Experience with designing and building out database  data warehousing systems. Experience in building software using Python. Good understanding of software development principles and practices e.g. - version control, typing, TDD, structured code. Experience in using Docker containers. Hands-on experience with cloud computing services ideally AWS. Knowledge of Terraform would be a bonus Extensive experience with SQL database design We mainly use PostgreSQL. Knowledge of NoSQL and Graph databases. Knowledge of big data processing technologies e.g. - Spark, Redshift. Great numerical and analytical skills. Able to communicate complex technical details to non technical team members. Data as a product mindset. 5 years experience. Degree in Computer Science, IT, or similar field; a Masters is a plus. Experience working with healthcare data would be a bonus. In addition, any experience in advanced numerical computing e.g. - using GPUs to power Ai and machine learning would be desirable. 
Benefits
We are a young, dynamic team revolutionising womens health. As our Data Engineer, you will get a salary of between 75k - 85k  equity options as well as  33 days paid holiday allowance including public holidays. Company laptop. Flexible working hours - we trust that our team can manage their time and will get the job done. 5 pension scheme matched by 4 employee contribution. Mental health support from an in-house counsellor. Free access to Hertility Health products and services, including free of charge hormone and fertility tests and discount codes for employee friends and family. Opportunities for further learning and development. Enhanced Family Leave policy maternity leave  paternity leave adoption leave. Progressive health leave menstruation and menopause policies.  Equality  Diversity Hertility Health is an equal opportunity employer that is committed to diversity and inclusion both within the workplace and throughout our application process. We do not discriminate on the basis of race, religion, colour, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
We understand that applying for a new job takes a lot of work and we really value your time. We are really looking forward to reading your application.

"
https://startup.jobs/data-engineer-xevant-4546051,Engineer,Data Engineer,Xevant ,"Lehi, United States",,"

What Were About! 
Xevant solutions provide real-time data and insights that empower healthcare stakeholders to solve the access, quality, and cost issues plaguing the US healthcare system to positively impact over 100 million lives. 
We are dedicated to hiring individuals who are driven to make a positive impact and grow their career. Our goal is to hire people who want to learn, solve challenges, and contribute to our company culture. Xevants team of professionals portray leadership, commitment, and dedication daily. At Xevant, teamwork is the core factor that drives success. We are accountable for cultivating an innovative environment that reflects both diversity and inclusion where we can grow and learn together with a diverse team of employees. 
The Team Youll Join  
Youll be joining an innovative Data Solutions team, which currently owns many data pipelines for critical functions of Xevants business. In this role, youll be responsible for designing and implementing complex data solutions, ensuring the reliability and scalability of our data systems, creating and maintaining data models, and supporting a wide variety of data processes use cloud technologies and python. 
The Impact Youll Make 


Implement data solutions using Snowflake, Python, and other data technologies 


Ensure efficient and reliable flow of data from various sources to our data warehouse and troubleshoot any failures 


Develop logical and physical to support business requirements 


Work closely with analytics team to identify their needs and to ensure existing data models meet expectations 


Requirements 


Bachelors or Masters degree in Computer Science, Engineering, or a related field 


2 years of experience in data engineering  


Technical skills in Snowflake and data modeling, including experience with data warehousing. 


Proficiency in Python and SQL with knowledge of AWS ecosystem and SQL Server 


Excellent problem-solving skills and attention to detail 


Self motivated and ability to manage time efficiently 


Why you should choose Xevant! 


Competitive pay 


Stock Options 


MedicalDentalVision benefits 


Short and long-term disability 


Remote and hybrid environment 


Flexible time off 


Rewarding culture and employee recognition 


Bi-annual company retreats 

401K  company match
Parental leave  

Diversity, Inclusion, Belonging focus 


Xevant provides equal employment opportunities EEO to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, Xevant complies with all applicable state and local laws governing nondiscrimination in employment. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. 

"
https://startup.jobs/data-engineer-culture-amp-4545727,Engineer,Data Engineer,Culture Amp ,"Melbourne, Australia",,"


Join us on our mission to make a better world of work. 
Culture Amp revolutionizes how over 25 million employees across 6,000 companies create a better world of work. As the global platform leader for employee experience, Culture Amp empowers companies of all sizes and industries to transform employee engagement, develop high performing teams, and retain talent via cutting-edge research, powerful technology, and the largest employee dataset in the world. The most innovative companies across the globe, such as Salesforce, PwC, KIND, SoulCycle, Celonis and BigCommerce depend on Culture Amp every day.
Culture Amp is backed by 10 years of innovation, leading capital venture funds, and offices in the U.S, U.K, Germany and Australia. Culture Amp is recognized as one of the worlds top private cloud companies by Forbes and one of the most innovative workplace companies by Fast Company.
Learn more about how Culture Amp can help you create a better world of work at cultureamp.com.

How you can help make a better world of work
The Data Platform team is responsible for the infrastructure and tooling that enables Culture Amp to succeed with data. This includes our Data Lake, Data Warehouse, ML and Data Science tooling, and core ETL pipelines
Data Engineers in this team build and maintain the platform to be robust, scalable, and cost-effective, while empowering our users analysts, data scientists, ML engineers to do their best work.
As part of this team of amazing humans, 
You will 

Design and implement core functionality of the data platform across our Redshift warehouse and S3Presto data lake
Identify opportunities to improve the experience of analysts, data scientists, and ML engineers using the platform
Ensure our data platform is robust, reliable, cost-effective, and protects the privacy and security of our users data

You have 

Solid software engineering fundamentals such as version control, TDD
Experience working with with CICD and infrastructure-as-code, especially AWS CDK
Exposure to modern data management practices and tools, across ETLELT, data quality, governance, monitoring, etc
Familiarity with data offerings in the AWS cloud, such as Athena, Redshift, Glue, EMR, and SageMaker

You are

A team player who values collaboration above individual achievement
Self-driven and proactive in coming up with creative solutions to problems and new ideas to improve the value we get out of our data
Someone who loves having an impact through enabling others with platforms and tooling

We believe that inclusive businesses are better, not just for company results, but for the world. We have a strong commitment to Anti-Racism, and endeavour to lead by example. Every step we make as a business towards anti-racism is another step we can take to support our customers in making a better world of work. You can see our current commitments to Anti-Racism here.
We ensure you have the tools you need to thrive both in and out of work.

MacBooks for you to do your best work 
Share Options - its important to us that everyone is an owner and can share in our success
Excellent parental leave and in work support programme, - for those families to be
Flexible working schedule - where we can, lets make work, work for you
Fun and inclusive digital, and in the future in-person events

Most importantly, an opportunity to really make a difference in peoples lives.
Please keep reading...
Research shows that candidates from underrepresented backgrounds often dont apply for roles if they dont meet all the criteria  unlike majority candidates meeting significantly fewer requirements.
We strongly encourage you to apply if youre interested wed love to know how you can amplify our team with your unique experience!

Thank you for taking the time to read this advert. If you decide to apply, as part of your application, we will ask you to complete voluntary diversity questions excluding Germany. Please watch this video from our amazing DEI Leader, Aubrey Blanche to share more on why we collect the data and how we will use it. 

 
 


"
https://startup.jobs/data-engineer-sr-digital-on-us-4545520,Engineer,Data Engineer Sr.,Digital On Us ,,,"


Who is Tech Mahindra? 
Location Mexico- remote 
At Tech Mahindra we not only provide Agile and DevOps methodologies to our customers, we have adopted the same within the company as well. Our nimble processes are not mired in red tape, yet robust, flexible and result-oriented. We are Software Engineers, Technical Architects, Cloud and DevOps specialists. But the most important, we are dreamers, creators and challengers. Each day, we strive to make great come alive. Our lemma work smart and play hard
Our technology partners are Hashicorp, Cloudbees, Chef, Pagerduty, Docker, SAP and Google Cloud.
We are always looking for the brightest candidates to come and we offer a work environment with everything you need to be your best. Does Ambition, Success, Fun, Friends  Learning define your idea of a career? Join us and be part of our family!
Were looking for a Senior Data Engineer to work for an American multinational energy corporation predominantly in oil and gas.
Technical Capabilities

5 years of experience as a Data Engineer
Proficient in Python, C, Sparq, Azure Synapse, Azure Data Factory, Azure Data Bricks, ADO Repos, Azure Data Lake Services 
Design and create data storage, data flows and transformation processes within the data lake, including, ingestion, working within the framework defined by the Data Architect 
Apply deep knowledge of data integration, data quality and developer tools to develop useful data sets to solve technical storage and integration problems. 
English Advanced

What you can expect from us
At Tech Mahindra what distinguishes us from other teams is the comfortable environment which engenders trust within teams and with our customers. Trust and openness leads to quality, innovation, commitment to deliverables, efficiency and cost-effectiveness for all our customers.

Work with some truly remarkable IT engineers, architects, specialists and more.
Were growing at a phenomenal pace and wed like some company.
Hear your voice, nurture your talent and help you strengthen your foot print!
Benefits above the law
Mentorship, and opportunities to grow and learn


If you apply for this opportunity we will get you resume and its contain personal data whose treatment has been authorized by its owner for Digital OnUs, S. de RL de CV the ""Company. If you are not the owner of this information or have no relation whatsoever with the subjects treated in it, you are requested in the most attentive way not to make copies of it and  or its attached files and delete it immediately, under the risk of being considered as responsible for the unauthorized treatment of personal data in accordance with the Federal Law on Protection of Personal Data Held by Private Parties, its Regulations, and other applicable regulations. If you are the owner of personal data in possession of the Company and wish to obtain further information regarding the processing of your personal data or the exercise of your ARCO rights, please consult our integral privacy notice on the website httpswww.digitalonus.comprivacy-policy

"
https://startup.jobs/data-engineer-goldcast-4544787,Engineer,Data Engineer,Goldcast ,"Bengaluru, India",Full-Time,"

Goldcast is a digital events platform for B2B marketers. More than 300 global enterprises across tech, professional services, manufacturing, and finance use Goldcast to deliver engaging digital and hybrid event experiences for their customers and prospects and drive measurable results through them.

Goldast was founded in mid-2020 at Harvard Business School. Since then, we have raised over 40mn, built a bi-continental team of excellent teammates, and are one of the market leaders in the enterprise digital events space.  We count companies such as LG, Adobe, 6Sense, Workday, Zuora, BitSight, Drift, ThoughtSpot, and Clari as customers and were recently featured in G2s list of 100 fastest-growing software companies.

Our sharp market focus and positioning around efficiency  measurability means we are growing really fast, still hiring, have a comfortable runway of 30 months, and a clear line of sight to quadrupling in the next 2 years.

Get to know us better  httpswww.goldcast.io

In this role you will

 Engineer efficient, adaptable and scalable data pipelines to process structured and unstructured data
 Develop a deep understanding of emerging technologies in the data space 
Partner with product leads to evolve top level metrics for Goldcast cross event reporting, ROI etc. 
Partner with Goldcasts internal engineering team to to ingest data from multiple data sources Segment, Hevo, Kinesis etc to ingest relevant metrics. 
Help improve the data engineering infrastructure by helping reduce costs and contribute to infrastructure improvement. 
Automate data testing to and integrations with CI pipelines to produce bug free code
Participate in on-calls duty

About you

Passionate about intuitive data products 
Highly proficient in atleast one of Java or Python with at least 2 years if softwaredata engineering experience. 
Comfortable with advanced SQL 
Familiarity with data lakes, data warehouse 
Experienced in engineering data pipelines using Snowflake on medium to large scale data sets. 
Familiarity with AWS cloud resources such as S3, EC2, RDS etc 
Excel at taking business requirements and translate those into technical specifications and implementable data models and pipelines
Can communicate and work with diverse stakeholders 


We are committed to assembling an unrivaled team of operators, designers, technologists, and adventurers who aim to create something magical on the cross roads of video communication and martech. As an early crew member, youll have enormous impact on both our product and company culture. If youre excited about our mission, and believe you might be a fit, wed love to hear from you!

"
https://startup.jobs/data-engineer-beautyhaul-4544749,Engineer,Data Engineer,Beautyhaul ,"Jakarta, Indonesia",,"

About the company

BeautyHaul is one of the fastest-growing beauty companies in Indonesia that aspires to be the largest vertically integrated beauty ecosystem from supply to retail. Due to its exponential growth, the company has attracted investment from a reputable investor, Sequoia Capital.  

Our first brand Somethinc was founded in 2019, is currently the 1 skincare brand in e-commerce  modern trade with 1M social media followings. Our second brand Glowinc is breaking the boundaries by offering genderless products at affordable prices. Our offlineonline commerce BeautyHaul is the core for carrying multiple brands that we have directly to our customers.  We are incubating more and more business models in our platform and aim to be a strong regional player in the beauty, within the South Asia region.
Responsiblities

Create and maintain optimal data pipeline architecture.
Assemble large, complex data sets that meet functional  non-functional business requirements.
Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS big data technologies. 
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. 
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. 
Work with data and analytics experts to strive for greater functionality in our data systems.

Requirements

Education background Bachelor of Computer ScienceInformation System
Have min. 2 years of experience as Data Engineer or related field 
Fast learner 
Has a good analytical  logical thinking 
Has a good communication skills with people at all levels 
Has a good attitude and pleasant personality 
Experience building and optimizing big data data pipelines, architectures and data sets
Experience with AWS cloud services EC2, Glue, Lambda, RDS, Redshift 
Experience with object-orientedobject function scripting languages Python  java  c  scala 


We have a CREATE culture Collaborative, Responsible, Eccentric, Agile, Tough, Effective. We are looking for more people to join us, who are excited to disrupt the traditional beauty industry with product innovation and non-linear growth, who want to build a profitable start-up into a unicorn company, who will always learn and do the right thing.

"
https://startup.jobs/data-engineer-lyrahealth-4543835,Engineer,Data Engineer,Lyra Health ,"Burlingame, United States",Full-Time,"

About Lyra Health
Lyra is transforming mental health care through technology with a human touch to help people feel emotionally healthy at work and at home. We work with industry leaders, such as Morgan Stanley, Uber, Amgen, and other Fortune 500 companies, to improve access to effective, high-quality mental health care for their employees and their families. With our innovative digital care platform and global provider network, 10 million people can receive the best care and feel better, faster. Founded by David Ebersman, former CFO of Facebook and Genentech, Lyra has raised more than 900 million.

Lyra Health seeks a Data Engineer in Burlingame, CA responsible for developing data infrastructure, pipelines, and data services in support of the ongoing development of our innovative digital care software platform.
Responsibilities
Specific duties include i developing core pieces of our data infrastructure, pipelines, and data services underlying our product, including designing mental health-related data pipelines using Python software from third-party data sources, such as CDPs and external medical API data sources; developing mental health-related data models in the data warehouse for use by data consumers within the company; and conducting tests on data quality and the accuracy of data models in the data warehouse as well as building new data monitoring systems; ii building and leveraging data warehouses for all data use cases, including providing technical expertise to Lyra Healths product team with respect to data warehouse management and scaling and establishing data governance with respect to how data is leveraged for data analytics purposes, including with respect to domain knowledge in mental health-related data elements for our consumers; and iii defining technical requirements and solutions for data pipelines and data views in support of Lyra Healths development of mental health machine learning product line, including meeting with stakeholders on a regular basis to define and finalize technical data scopes and requirements for data pipelines and models and maintaining an optimal data backlog with respect to product prioritization and consumer insightexpectations.
Qualifications

Must have a bachelors degree in Computer Science or a directly computer-related academic discipline plus one 1 year of experience in a data engineering position.
Must have knowledge through any completed University-level coursework, seminars, workshops, or real-world, hands-on experience of i advance level Python; ii SQL coding; iii data visualization  validation; iv designing data pipelines using Python software from third-party data sources using technologies such as Airflow; and v defining technical requirements and solutions for data pipelines and data views.


We are an Equal Opportunity Employer. We do not discriminate on the basis of race, color, religion, sex including pregnancy, national origin, age 40 or older, disability,  genetic information or any other category protected by law.

"
https://startup.jobs/data-engineer-sumo-digital-4543833,Engineer,Data Engineer,Sumo Digital ,"South Yorkshire, United Kingdom",Full-Time,"

With a rich history of developing multiple AAA franchises such as Little Big Planet and Crackdown, as well as contributing to genre-defining titles like the Forza Motorsport and Hitman series, Sumo Sheffields vast development experience spans multiple genres and platforms. 
 
As winners of the TIGA Star Employment Award for 2021, recipients of the GI Biz 2020 Best Places to Work Award and listed as one of the UKs 100 Best Large Companies to Work for in 2021 and 2022, Sumo Digital has enjoyed 20 years of stability and growth since its formation in 2003 and continues to strive for excellence across every single project.
 
The team have a track record of working on some of the worlds biggest and best games and received two BAFTAs for their work on Sackboy A Big Adventure. As the scale and exclusivity of projects continues to grow, theres never been a more exciting time to join the Sumo Sheffield family!

SUMO Sheffield are currently looking for a Data Engineer to build scalable data systems to support the live operations of our products, covering live telemetry pipelines, ETL processes for materialisation of FACT and other aggregate tables, experimentation systems AB testing, and working with Data Scientists to implement production ML pipelines.

In this job you will

Work with the Live-Ops Manager, and wider code team to build, extend and maintain data pipelines and structures to support our live service titles.
Develop, maintain, and support ETL processes for data aggregation and cross-pollinating from multiple sources into a data warehouse.
Maintain large, multi-terabyte data warehouse which includes performance tuning and data retentionpurge processes.
Develop and manage various database scripts, processes, and tools to facilitate automation processes.
Design database systems to support liveOps, and recommend improvements for performance.
Work with Data Science team to build, implement, and maintain ML data processes churn prediction, for example.

To be successful you will have

MS or BS  Equivalent Experience in Mathematics, Statistics, Computer Science, Economics, or similar.
Experience in Data Engineering in digital products in a B2C environment, particularly games-as-a-service on any platform.
Knowledge and interest of live game terminology, trends, and patterns.
Experience with core AWS products
Experience working with big data solutions for example, Redshift, Spectrum, Athena, Glue
Experience streaming data technologies kinesis, firehose
Experience with AWS Lambda, and Serverless Application Model SAM
High proficiency in Python and SQL
A strong understanding of IAM and security best practises
Experience with monitoring systems Cloudwatch
Experience with data modelling, design, and data structures
Experience building ETL pipelines


What we offer

. Defined Annual Bonus Scheme
. ULEV car scheme 
. Monthly allowance to spend on enhancing benefits        
. Salary Sacrifice Pension
. Flexible Working   
. 21 days free-use holiday
  your birthday off
  5 learning days including 1 volunteer day
  a duvet day
  Christmas shutdown from Christmas Eve to New Year
  Bank Holidays
  extra allowance accrued for longer service
. Free dental insurance 
. Life Assurance 4x Salary
. Income Protection of 75 for 5 years            
. Enhanced MatPaternity  Adoption pay
. Employee Assistance Programme
. Help at Hand - Remote GP, Mental Health  Physio appointments
. Free financial  home buying support consultations
. Cycle to Work Scheme
. Payroll Giving Scheme
. Access to 4,000 Udemy Courses
. Career Development Pathway   
. Wrap Parties.
. Project Completion Awards
. GameCreative Jams
. Big Family Events
. Studio social activities, e.g. running, board games, film, yoga, life drawing
. Prism  Sumos employee diversity focus group 
. Free fruit and drinks in our studios
. Employee Referral Bonus      

Where do we stand on diversity?

At Sumo, we want to enable everyone to bring their full, authentic selves to work every single day. We acknowledge that the games industry is far from perfect and a global organisation with people all over the world, from all kinds of backgrounds, we know its part of our duty to provide everyone at Sumo with a safe, accepting and affirming working environment.
We encourage and welcome applicants from all backgrounds.
 
More about Sumo Sheffield
 
Sumo Sheffield is located just outside the city centre in Sumo Digitals HQ, just off J34 on the M1 and a stones throw from the citys primary shopping and leisure complex. With a multi-building campus comprised of industry-leading facilities, an onsite gym and subsidised canteen, Sumo Sheffield provides everything needed for all teams to create the worlds best games.
 
Sheffield is the fifth largest city in the UK and is easily accessible by road and rail from Leeds, Manchester and the Midlands. The city enjoys a wealth of facilities to suit a wide array of interests, including

o   World-class sports facilities
o   Iconic theatres and galleries
o   Multiple retail and shopping experiences 
o   Diverse and renowned restaurants and eateries
o   The award-winning National Video Game Museum

More about Sumo Digital
 
Sumo Digital is an award-winning international family of studios. Recent releases from Sumo Digital studios include BAFTA-Award winning Sackboy A Big Adventure, Hood Outlaws  Legends, Control Ultimate Edition on Google Stadia, Apple Arcade Award-winning Little Orpheus, Hot Shot Racing, WST Snooker and Spyder.

"
https://startup.jobs/data-engineer-appdirect-4542149,Engineer,Data Engineer,AppDirect ,"Pune, India",,"

About AppDirect
Become a digital, global citizen and enable the new generation of digital entrepreneurs around the world. AppDirect offers a subscription commerce platform to sell any product, through any channel, on any device - as a service. We power millions of subscriptions worldwide for organizations. We do this by our values-driven culture - one that enables you to Be Seen, Be Yourself, and Do Your Best Work.
About You
We are looking to hire a skilled and ambitious Data Engineer for the Digital engagement team. We love an entrepreneurial spirit and someone that has a Data as a Product mindset. Are you highly technical, with a strong drive to learn? Do you strive to connect data to business outcomes?  Then this might be the opportunity for you!
What Youll Do and How Youll Make an Impact

Partner with the Engineering Manager and Tech Lead to collaborate and build a modern data platform.

Manage and redesign existing data pipelines.

Improve efficiency by enabling internal stakeholders with self-served data solutions.
Research solutions to complex problems and drive proof-of-concepts.
Author and maintain a knowledge base through high-quality documentation.
Deliver quality solutionsprocess through CICD and automation.

What Youll Need

3 years experience creating and maintaining data pipelines.
2 years experience on AWS or Azure.
Experience with Databricks optional.

Experience with developing in modern programming language e.g., Scala, Java, SQL, Python.

Exposure to some of the following technologies Apache Spark or Flink, AWS  Azure, Docker  Kubernetes.
Understanding of Data Schemas e.g. SQL, JSON, Avro, Protobuf.
Experience automating, producing and consuming data in real time from event driven microservices using streaming platforms like Kafka.
Proven experience building data pipelines and data solutions aligned to Business Architecture, operational and analytics use cases.
Good knowledge and understanding of data structures and experience handling large data sets.
Comfortable in code reviewing other engineers code and drivingreviewing technical designs.
Ability to self-manage, think critically, handle multiple tasks andor projects and deliver solutions autonomously.
Ability to clearly communicate complex ideas to a technical and non-technical audience.

About AppDirect
AppDirect is the only end-to-end commerce platform for selling, distributing, and managing cloud-based products and services to succeed in the digital economy. The AppDirect ecosystem connects channels, developers, and customers through its platform to simplify the digital supply chain by enabling the onboarding and sale of products with third-party services, for any channel, on any device, with support. Powering millions of cloud subscriptions worldwide, AppDirect helps organizations, including Jaguar Land Rover, Comcast, ADP, and Deutsche Telekom connect their customers to the solutions they need to reach their full potential in the digital economy.
We believe that the unique contributions of all AppDirectors are the driver of our success. To make sure that our products and culture continue to incorporate everyones perspectives and experience we never discriminate on the basis of race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status.
At AppDirect we take privacy very seriously. For more information about our use and handling of personal data from job applicants, please read our Candidate Privacy Policy. For more information on our general privacy practices, please see AppDirect Privacy Notice link httpswww.appdirect.comaboutprivacy-notice

"
https://startup.jobs/data-engineer-remote-ad-hoc-4541356,Engineer,Data Engineer (Remote),Ad Hoc ,"Washington, United States",,"


This is a fully remote position. 
Work on things that matterAd Hoc is a digital services company that helps the federal government better serve people. Our teams use modern, agile methods to design and engineer government systems that connect Veterans with services, bring affordable health care to millions of people, and support important programs like Head Start. And as we work to make critical government services intuitive, accessible, and human-centered, were also changing how the government thinks about and uses technology. If you thrive on change, want to help close the gap between consumer expectations and government services, and can see the possibilities in ambiguity, then we want you here with us. 
What matters mostAd Hoc operates according to our commitment to inclusivity, acceptance, accountability, and humility. We arent heroes. We believe in missions larger than our individual selves and leave our egos at the door, learn from our mistakes, and iterate in order to better serve the people in our country. We prioritize building teams that represent the diversity of the people our government serves. We love the challenge of government-size projects. We want to bring skills to federal agencies, help them better meet the needs of their users, and close the gap between consumer expectations and government. 
Built for a remote lifeAd Hoc is remote-first and remote-always. Weve designed our culture, communications, and tools to support a nationwide distributed team since the beginning. Being remote by design allows Ad Hoc to be thoughtful and intentional about creating diverse teams and supporting them with a work environment that fits their lives. With a generous PTO policy and Slack channels for every interest from bird watching to space nerds to parenting our culture embraces the things happening in your life. Maybe you need to adjust your schedule to care for your family or take a bike ride. At Ad Hoc, thats embraced. 
What youll do
Data Engineers are responsible for working with the systems and infrastructure that enable data storage, processing, and analysis. They work closely with data scientists and analysts to ensure that data is properly collected, organized, enriched, refined, and available for analysis. They are the critical connectors between the teams that maintain existing legacy systems, and the data analysts and data scientists that will use aggregated data for analysis, reporting, and predictive analytics.

Shipping software that impacts the lives of millions of people
Using modern programming languages and frameworks to build scalable services that gracefully integrate with legacy systems
Building and working with APIs to support both the digital services we deliver as well as third-party usage
Helping us continuously, iteratively improve

What we hope youll bring
 

A minimum of four 4 years of professional software development experience
AWS experience
Understanding of ETLELT processes and tooling
Understanding of database technologies - setup maintenance  data loads  etc. not data modeling
Redshift experience preferred
Understand system security
API design and implementation
GIT and DevOps release process
Python or Scala, Python is preferred for ETL
Some experience with older file systems  file based processes such as MOVEit
Some experience with Mulesoft
Experience with agile software development practices emphasizing agility, flexibility, and iterative development

 
More than that, our ideal candidate wants to contribute to work that is bigger than themselves and wants to make a difference collaborating with their team. They care deeply about building better products, better relationships, and better trust in each interaction people have with their government. They believe in intuitive, easy-to-use government services. They collaborate well with designers, stakeholders, and other teams. They mentor and guide more junior engineers. Theyre human-centered.
And if you dont check every box on the list? That doesnt mean you cant help us in our mission to deliver critical government services. Talk to us!
Some basic requirements

All work must be conducted within the U.S., excluding U.S. territories. Some federal contracts require U.S. citizenship to be eligible for employment.
You must be legally authorized to work in the U.S now and in the future without sponsorship.
As a government contractor, you may be required to obtain a public trust security clearance.
Bachelors Degree in a technical field is preferred
4 years of professional software development

Our technical screening involves completing a homework assignment that is then graded blind to remove bias. We do not do tricky, unreliable whiteboarding tests. You can read more about our homework here.


Learn more about engineering at Ad Hoc.
Benefits

Company-subsidized Health, Dental, and Vision Insurance
Use What You Need Vacation Policy
401K with employer match
Paid parental leave after one year of service

Ad Hoc LLC is an Equal OpportunityAffirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, national origin, ancestry, sex, sexual orientation, gender identity or expression, religion, age, pregnancy, disability, work-related injury, covered veteran status, political ideology, marital status, or any other factor that the law protects from employment discrimination.   
In support of the Colorado Equal Pay Transparency Act, and others like it across the country, Ad Hoc job descriptions feature the starting range we reasonably expect to pay to candidates who would join our team with little to no need for training on the responsibilities weve outlined above. Actual compensation is influenced by a wide range of factors including but not limited to skill set, level of experience, and responsibility. The range of starting pay for this role is 101,570 - 136,994 and information on benefits offered is here. Our recruiters will be happy to answer any questions you may have, and we look forward to learning more about your salary requirements.
job reference  2015

"
https://startup.jobs/data-engineer-scala-factoredai-4540681,"Engineer,Scala",Data Engineer (Scala),Factored ,,Full-Time,"

Who we are
Factored was conceived in Palo Alto, California by Andrew Ng and a team of highly experienced AI researchers, educators, and engineers to help address the significant shortage of qualified AI  Machine-Learning engineers globally. We know that exceptional technical aptitude, intelligence, communication skills, and passion are equally distributed around the world, and we are very committed to testing, vetting, and nurturing the most talented engineers for our program and on behalf of our clients.

We are seeking a skilled and experienced Data Engineer proficient in Databricks and Scala to join our dynamic team. As a Data Engineer, you will play a crucial role in designing, developing, and maintaining our data infrastructure and pipelines. Your expertise in Databricks and Scala will be essential in building scalable and efficient data solutions that enable data-driven decision-making across the organization. LI-Remote

What you will be doing

Create and maintain optimal data pipeline architecture across multiple data sources, including licensed and scraped data.
Design and develop optimal data processing techniques automating manual processes, data delivery, data validation and data augmentation.
Design, develop, and optimize data pipelines and ETL processes using Databricks and Scala.
Manage analytics tools that provide actionable insights into usage, customer acquisition, operational efficiency and other key business performance metrics.
Design and develop a API integrations in order to feed different data models.
Architect and implement new features from scratch, partnering with AIML engineers to identify data sources, gaps and dependencies.
Identify bugs and performance issues across the stack, including performance monitoring and testing tools to ensure data integrity and quality user experience.
Build a highly scalable infrastructure using SQL and AWS big data technologies.
Keep data secure and compliant with international data handling rules.

What you must bring

3 years of professional experience shipping high-quality, production-ready code.
Strong computer science foundations, including data structures  algorithms, OS, computer networks, databases, algorithms, and object-oriented programming. 
Experience in Scala.
Experience setting up data pipelines using relational SQL and NoSQL databases, including Postgres, Cassandra, or MongoDB.
Experience with cloud services for handling data infrastructure such as Databricks, Snowflake, GCP, Azure, or AWS.
Proven success manipulating, processing and extracting value from large heterogeneous datasets.
Strong analytic skills related to working with unstructured datasets.
Expertise with version control systems, such as Git.
Excellent verbal and written communication skills in English.

Nice to have

BSc in Computer Science, Mathematics or similar field; Masters or PhD degree is a plus.
Experience with design of ETLs using Apache Airflow.
Experience with real-time scenarios, low-latency systems and data intensive environments is a plus.


At Factored, we believe that passionate, smart people expect honesty and transparency, as well as the freedom to do the best work of their lives while learning and growing as much as possible. Great people enjoy working with other passionate, smart people, so we believe in hiring right, and are very selective about who joins our team. Once we hire you, we will invest in you and support your career and professional growth in many meaningful ways. We hire people who are supremely intelligent and talented, but we recognize that intelligence is not enough. Perhaps more importantly, we look for those who are also passionate about our mission and are honest, diligent, collaborative, kind to others, and fun to be around. Life is too short to work with people who dont inspire you.  

We are a transparent workplace, where EVERYBODY has a voice in building OUR company, and where learning and growth is available to everyone based on their merits, not just on stamps on their resume. As impressive as some of the stamps on our resumes are, we recognize that human talent and passion exist everywhere, and come from many backgrounds, so stamps matter much less than results. All of us are dedicated doers and are highly energetic, focusing vehemently on execution because we know that the best learning happens by doing. We recognize that we are creating OUR COMPANY TOGETHER, which is not only a high-performing fast-growing business, but is changing the way the world perceives the quality of technical talent in Latin America. We are fueled by the great positive impact we are making in the places where we do business, and are committed to accelerating careers and investing in hundreds and hopefully thousands of highly talented data science engineers and data analysts. 

In short, our business is about people, so we hire the best people and invest as much as possible in making them fall in love with their work, their learning, and their mission.  When not nerding out on data science, we love to make music together, play sports, play games, dance salsa, cook delicious food, brew the best coffee, throw the best parties, and generally have a great time with each other.

"
https://startup.jobs/data-engineer-freelancer-f-m-d-kolibri-games-4539495,Engineer,Data Engineer - Freelancer (f/m/d),Kolibri Games ,"Berlin, Germany",Contractor,"

Kolibri Games is looking for a Data Quality Engineer specialist to help us maintain our pipelines and improve the quality and reliability of our ETLs.


Start August 1st, 2023, preferably earlier
Length 4 months extension possible
Location Fully Remote Possible, EU Timezone preferred critical hours are between 7am and 10am Germany time
Language English  German

Your tasks include

Identify common failures in data pipelines and ETL and resolve them quickly
Brainstorm and implement techniques that allow us to minimize the work needed to handle those incidents through automation or machine learning.
Train the team to handle data incidents cleanly and efficiently for future Data Platform generations.
This position requires the following key competencies
Airflow
DBT
Python
Jupyter Notebooks

Nice to have

Databricks
Great Expectations, Big Eye, DBT Expectations, Monte Carlo, or a similar observability Platform

Are you interested and are you available promptly? Then please submit your application with your rate.
We can then talk about the project details over the phone.



"
https://startup.jobs/data-engineer-luminus-4539013,Engineer,Data engineer,- - Luminus ,"Brussels, Belgium",,"

At the confluence between generation electricity production and retail electricity consumption, optimization is in charge to manage Luminus position on the financialenergy market to ensure that at all times there is a matching between the electricitygas supply and demand. To do this, it is needed to optimize the volumes to be processed on the market and to extract the maximum value from its assets through market engineering. Such activities rely extensively on data and insights that could be extracted from them. The quality, relevance and quantity of these insights will be key to help optimization to continue to take the right decisions.

Within the Data Intelligence team of the Optimization department, we are seeking a Data Engineer. You will play a crucial role as you will actively participate in the development of the data engineering capabilities and expertise within the data intelligence team. It is expected from the applicant to take over ambitious data engineering projects such as building robust and maintainable data pipelines with PySpark on our data platform, improve internal processes, develop data tools to support the business in its activities. 
The first two years will be dedicated to a specific project related to market price fetching. Rebuild to full activity from extracting data from external sources to the implementation of data internally, create new pipelines on the cloud and provide visualization capabilities on it. 

Responsibilities
Data Integration  You identify the most appropriate data sources to use for a given purpose and understand their structures and contents, in collaboration with subject matter experts.

Data Infrastructure Development You design and implement batch and real-time data pipelines on our data platform AWS data lake or at the level of our data warehouse for our data scientists and data analysts. You identify, design and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.

Continuous Learning and Knowledge Transfer You educate the Data Intelligence Team on new market andor technology trends in the domain of data analytics and data engineering. You document your code, share and transfer your knowledge

Collaboration and Communication You work with data and analytics experts to strive for greater functionality in our data systems data quality, data lineage, data catalogue,. You work closely with a team of enthusiastic colleagues and share their knowledge and experience.


"
https://startup.jobs/data-engineer-acumen-llc-4538959,Engineer,Data Engineer,"Acumen, LLC ",,Full-Time,"

Acumen, LLC has multiple positions available for Data Engineer in Burlingame, CA and various unanticipated locations throughout the USA. Develop, maintain, and optimize data pipelines for healthcare research. Create, maintain, and possess thorough knowledge of specific areas of the companys data infrastructure for linkage, Medicare claims and enrollment data warehouse. Integrate new data sources or data designs into the companys data management systems. Verify the integrity of data by performing data validation checks across multiple data sources and develop authoritative validation methods through building metadata tables to check for internal and external data consistency. Create and maintain documentation that helps both internal and external users understand the companys data and available research tools including ETL pipeline documentation, query application user guidedocumentation, and linkage methodology.100 Telecommuting permitted within the U.S.

This is a full-time 40 hoursweek position in our Burlingame, CA headquarters 500 Airport Blvd., Burlingame, CA 94010 and various unanticipated locations throughout the USA. Salary Range 129,833yr-135,000yr.

To apply, upload your resume and transcripts and reference JobMSDE23. EOE. Principals only. applytoday

"
https://startup.jobs/data-engineer-mid-level-remote-pulse_data-4538401,Engineer,Data Engineer (Mid Level/Remote),pulseData ,,Full-Time,"

Join us as we pursue our vision to eliminate preventable sickness and transform kidney care using AI and data!

At pulseData, we provide value-based healthcare organizations and their care teams a patented data intelligence platform that helps deliver better care at less cost. As a company, we leverage data science and machine learning for Hospitals and Healthcare companies, to identify patients at risk of costly and avoidable medical outcomes.

In this role, youll become a key member of pulseDatas engineering team. As our Data Engineer, you will be working on developing, deploying, and maintaining our product pipelines and work in an environment of growth and collaboration. 

The Role
Youll be developing, deploying, and maintaining our production data pipeline which produces risk scores and patient reports vital to the workflows of doctors and care coordinators.
Youll be ensuring product deliverables are executed reliably and accurately on a regular basis.
Youll be managing and monitoring client interfaces to ensure timely delivery of data.
Youll be growing your skillset in a diverse and challenging environment that offers the latest technologies.

Your Background
You have completed a Bachelors Degree or above.
You have at least 2 years of professional experience as a softwaredata engineer.
You have experience managing a data pipeline in a cloud environment.
Youre comfortable using open-source technologies like Apache Spark, Kubernetes, Airflow in addition to Google Cloud services. 
Youre comfortable working with python.
You possess a growth mindset with the ability to work autonomously and with teammates remotely.
If you have experience working on healthcare data sets that is a plus!
This role does not offer sponsorship

The Perks
Earn a salary between 90k to 140k
Work fully remote - were a distributed team w a HQ office in NYC.
We offer a potential bonus and equity for this role depending on experience.
Get Unlimited Paid Time Off - thats unlimited time off to help you stay at your best!
Receive top tier benefits through Aetna including medical, dental, and vision insurance - 85 of which is covered by pulseData.
Take part in planning your retirement with our 401k
Youll receive a new laptop  get to work flexible hours we generally work ET but are flexible!
We have 2 Company offsite adventures throughout the US that are fully sponsored and paid for.
Youll get 300 annually to spend on improving your home office so it feels good!
We offer TONS of room for growth and the ability to learn new things in our startup environment.

Our culture is devoted to diversity, growth, and collaboration. We believe in trusting our employees and encourage you to trust yourself and apply to this role today if you think you can be a great fit!

Chronic Kidney Disease CKD is a substantially under-diagnosed condition that affects 37 million people in the US. Today, clinical and care management teams use our platform to identify the most at risk-individuals and proactively deliver care in advance of adverse events, delivering better clinical outcomes at a lower cost. Our clients are risk-bearing healthcare enterprises ACOs, kidney clinics and nephrology practices, large health systems, managed care and health insurance companies. Together, we use data science to deliver more human care.

pulseData is backed by some of the best investors in the industry, recently raising a 16.5M Series A led by Bain Capital and Two Bear Capital. 

"
https://startup.jobs/data-engineer-farfetch-2-4538286,Engineer,Data Engineer,Farfetch ,"Porto, Portugal",Full-Time,"

FARFETCH exists for the love of fashion. Our mission is to be the global platform for luxury fashion, connecting creators, curators and consumers.
Were a positive platform for good, bringing together an incredible creative community made up by our people, our partners and our customers. This community is at the heart of our business success. We welcome differences, empower individuality and celebrate diverse skills and perspectives, creating an inclusive environment for everyone. We are FARFETCH for All.

TECHNOLOGY
Were on a mission to build the technology that powers the global platform for luxury fashion. We operate a modular end-to-end technology platform purpose-built to connect the luxury fashion ecosystem worldwide, addressing complex challenges and enjoying it. Were empowered to break traditions and revolutionise, with the freedom and autonomy to make a difference for our customers all over the world.

PORTO
Our Porto office is located in Portugals vibrant second city, known for its history and its creative yet cosy environment. From Account Management to Technology and Product, whatever your skills are, youll find your fit here. You can have an informal meeting in the treehouse or play the piano in your lunch break!

THE ROLE
You will be integrated in the Data Engineering team, being responsible for helping maintain and develop the custom tools needed to build the demanding data pipelines Farfetch needs.
Farfetchs Data Teams are focused on everything related to data. Their main purpose is to harness the power of Farfetchs data to deliver insights and reports that support business decisions and also analyze and discover new ways to amaze our customers. These teams cover multiple areas related to data, such as Business Intelligence, Software and Data Engineering, Data Science and Data Analytics. Just as the rest of Farfetch, Data Teams are committed into turning the company into a leading e-commerce platform. As so, they are constantly looking for brilliant people who like the challenges that a fast growing, data driven company faces in its path to become a market leader.
WHAT YOULL DO

Design scalable, end-to-end data pipelines to consume and integrate large volumes of complex data from multiple sources.
Maintain and develop the tools needed for a self-serving data architecture approach.
Provide Data Engineering expertise to multiple teams across our organization.
Research and implement new data technologies.
Work with the team to set and maintain standards and development practices.

WHO YOU ARE

You have 2 years of experience as a professional with solid technical background building and maintaining data pipelines in a custom or commercial ETL tool.
Proficient in one of the following programming languages Scala plus, C, Java, Python.
Knowledgeable of distributed systems and distributed data processing frameworks like Spark plus, Flink.
Experienced with git version control and CICD deployment principles.
Experience working with cloud environments is a plus eg. AWS, GCP, Azure.
Experience in working with Databricks environments and airflow is a plus.
You have an intermediate level in English, both written and spoken.
You have good analytical and problem solving skills, the ability to work in a fast moving operational environment and you are enthusiastic and with a positive attitude.

REWARDS  BENEFITS

Health insurance for the whole family, flexible working environment and well-being support and tools
Extra days off, sabbatical program and days for you to give back for the community
Training opportunities and free access to Udemy
Flexible benefits program
FARFETCH Equity plan

EQUAL OPPORTUNITIES STATEMENT
FARFETCH is an equal opportunities employer ensuring that all applicants are treated equally and fairly throughout our recruitment process. We are determined that no applicant experiences discrimination on the basis of sex, race, ethnicity, religion or belief, disability, age, gender identity, ancestry, sexual orientation, veteran status, marriage and civil partnership, pregnancy and maternity, or any other basis prohibited by applicable law. We continue to build our consciously inclusive culture as part of our Positively FARFETCH strategy throughout our business, partnerships and communities.


"
https://startup.jobs/data-engineer-hybrid-usa-san-mateo-ca-local-candidates-only-guidewire-software-4537428,Engineer,"Data Engineer- (Hybrid)- USA, San Mateo, CA- (Local Candidates only)",Guidewire Software ,"San Mateo, United States",Full-Time,"

Guidewire is searching for a Data Engineer who is passionate about all things data, from polishing data under the hood to transforming complex data sets in mind blowing ways. You will help build our next generation of data analytics tools to help insurance companies glean actionable insights into their operational performance.

Who We Are, What We Believe,  What We Build
Guidewire is the market leader that 400 insurance companies trust to run their critical platform. Every second, we support underwriters crafting policies and agents settling claims. We believe that making a great decision should not require 100 in-house data scientists. Our products range from cyber risk quantification to potent ML sandboxes. We are a post-IPO company with the vision to change insurance. There is a lot to be done!

Who You Are
You are an enthusiastic engineer who is passionate about building high-quality software with a great team and improving the customer experience for all of customers who use our products every day. You have a fine attention to detail and love data in all forms. You can quickly build new features in an agile environment. You are a self-starter and able to perform with minimal supervision while working alongside intelligent colleagues whove checked their egos at the door in a team environment, communicating effectively with everyone. Weve built a strong culture, and were looking for engineers who can help us maintain it!

Responsibilities 

Design and Development of cloud-based microservices
Responsible for developing and maintaining CICD pipelines
Maintain and stabilize existing data pipelines and infrastructure to provide timely, quality and accurate data
Analysis, Performance Improvement, Development, Deployment and Contribute to overall product requirements
Perform code review and develop best coding practices
Indulge in brainstorming sessions to help team gain technically knowledge and grow equally
Define release process improvements along with management of pipeline changes
Collaborate with peer teams and support each others on deliverables

Qualification

5 to 8 years experience in role developing complex data-oriented applications
Excellent skills in Python and other programming languages, coupled with a deep understanding of object-oriented software design patterns
Proficient with Core Java and frameworks like SpringSpring Boot
Experience with PySpark is a must
In depth knowledge of databases and SQL, particularly Oracle and PostgreSQL
Experience building and managing AWS services especially, EKS, EMR, EC2, S3, Athena, and AirflowMWAA. Experience in other AWS services are added advantage
Experience in Linux and Shell scripting required
Experience in Docker required and Kubernetes experience added advantage
Experience in Airflow preferred
Prior experience building MicroService add advantage
Expertise in version control tools Git, Bitbucket preferred


LIRemote
feature
dataengineer java python pyspark spring

About Guidewire
Guidewire is the platform PC insurers trust to engage, innovate, and grow efficiently. We combine digital, core, analytics, and AI to deliver our platform as a cloud service. More than 450 insurers, from new ventures to the largest and most complex in the world, run on Guidewire.


As a partner to our customers, we continually evolve to enable their success. We are proud of our unparalleled implementation track record with 1000 successful projects, supported by the largest RD team and partner ecosystem in the industry. Our Marketplace provides hundreds of add-ons that accelerate integration, localization, and innovation.

For more information, please visit www.guidewire.com and follow us on Twitter GuidewirePandC.

Guidewire Software, Inc. is proud to be an equal opportunity and affirmative action employer. We are committed to an inclusive workplace, and believe that a diversity of perspectives, abilities, and cultures is a key to our success. Qualified applicants will receive consideration without regard to race, color, ancestry, religion, sex, national origin, citizenship, marital status, age, sexual orientation, gender identity, gender expression, veteran status, or disability. All offers are contingent upon passing a criminal history and other background checks where its applicable to the position.

Disability Accommodations and Guidewires Appeals Process. Guidewire provides accommodations to the hiring process to create a fair opportunity for candidates with disabilities to contend for open positions. Accommodation requests should be directed to 650 356-4940 or Accommodationsguidewire.com. If things do not go as hoped, we invite you to use our appeals process. Guidewire promises to independently review any denied accommodation and any decision not to offer you the position. The appeals process is the same in either case. Within five business days of receiving a notice of denial of an accommodation, or receiving a notice of your non-selection for a vacancy, call 650 356-4940 or e-mail Accommodationsguidewire.com to make an appeal. Guidewire will assign a new decision-maker to review the request andor hiring decision, who will then notify you in writing of a decision within 10 business days.

"
https://startup.jobs/data-engineer-italy-cedacri-group-iongroup-4535231,Engineer,"Data Engineer, Italy (Cedacri Group)",ION Group ,"Province of Verona, Italy",Full-Time,"

About Us
Cedacri is part of ION Group, a community of visionary innovators, dedicated to providing pioneering software and consultancy services to financial institutions, trading firms, central banks, governments, and corporations around the world. We strive to simplify the way people work. We do that by providing workflow and process automation software, as well as providing real-time data and business intelligence to help people make better decisions. We are 13,000 employees, 60 global offices, and over 4,800 customers worldwide.
For the strengthening of the Innovation Area Team, we are looking for brilliant professionals with proved experience in Big Data environments, preferably bankingfinance, to join the team. 
The ideal candidate will experience a dynamic, fast-paced and innovative environment, and will contribute to offer best in class services to our customers.
 
Your role
Your duties and responsibilities
 

         Developing, building, testing and maintaining data pipeline architectures


         Identifying solutions to improve data reliability, efficiency and quality


         Preparing data for predictive and prescriptive modelling


         Ensuring compliance with data governance and security policies


         Managing principles of big data and data warehouse platforms


         Creating programs and systems capable of acquiring, aggregating, transforming and structuring companys data


         Creating solutions that enable the work of data analysts and data scientists by using the tools, languages and structures of data engineering


         Integrating, consolidating, and cleaning data for application analysis.


Other duties
We might ask you to perform other tasks and duties as your role expands.
 
Your skills, experience, and qualifications required
 

         Bachelors degree with honors in technicalscientific areas computer science, mathematics, physics, computer engineering


         Knowledge of and ability to use programming languages e.g. Python, Java, Javascript and R


         Applied skills in relational databases e.g. SQL Server and Oracle


         Expertise in programming and technical analysis


         Knowledge of distributed systems Hadoop


         At least five years of experience in big data


         Excellent knowledge of the English language


         Excellent interpersonal and communication skills

 
Considered a plus knowledge of machine learning such as PyTorch, TensorFlow or similar, ETL tools and APIs for creating and managing data integration processes, Data processing Apache Kafka, Distributed systems Cloudera, BI tools RStudio, PowerBI, Jupyter.
 
What we offer
Permanent employment contract.
 
Location 
Assago MI, Collecchio PR, Verona VR.
 
Important notes

According to the Italian Law L.6899 please note that candidates from the disability list will be given priority.



"
https://startup.jobs/data-engineer-remote-in-portugal-possible-languagewire-4533758,Engineer,Data Engineer (Remote in Portugal possible),LanguageWire ,,,"

Do you want to work with a range of Microsoft Azure tools and take another step into Data Engineering  then you should definitely read on! The role youll play We are looking for an independent-minded Data Engineer who can help us further develop our Datawarehouse-infrastructure, maintain data discipline and act as a data-broker across the company. Your individual expertise, broad knowledge of data, a drive to learn, and your ability to collaborate with your teammates will be essential for your success. The team youll be a part of In Business Finance, we are a small team that is based on data, aims to deliver insights  knowledge - in an easy and intuitive way, to all of LanguageWire. We are always open to new ideas and using new technology to help us along the way.  If you want to make a difference, make it with us by  Being responsible for a data pipeline infrastructure for ETL in Azure ADF, using a wide variety of data sources relational-  non-relational  Perform tasks such as  Integrate new data sources via APIs, Data transfers etc.  Data cleaning  -aggregation  Maintain and Build new Tabular Cubes  Power BI Reports DAX   Assemble large and complex data sets to meet functionalnon-functional business requirements. Identify and automate process improvements  Optimizing data-loads etc.  Ensure operations stability, perform tests and evaluations regularly to ensure data integrity In one year, youll know you were successful if  You have added cool new data sources for our Machine Learning- or Marketing teams You have changed our Tabular model architecture carve-out, build new cubes You have removed logic from a PowerBI report and pushed it to the Tabular model. Youve identified improvement points of the architecture and applied solutions to optimize them. Youve learned and grown professionally.  Desired experience and competencies  What does it take to work for LanguageWire? What youll need to bring  1-3 years of experience in a similar position Bachelors or Masters Degree in Computer Science, Computer Engineering or similar. Exceptional problem-solving skills Experience with Microsoft data stack on-premise and Azure Knowledge about TSQL coding, stored procedures, views, troubleshooting  debugging. DAX, Tabular Editor  Power BI ETL  Data Management Tools CICD, DevOps   Being able to convey your results through conversations, written communications minimum English C1  This will make you stand out   Knowledge of Python, Databricks, Machine Learning or similar General coding skills to independently gather data from various sources  A genuine interest in exploring and utilizing new technologies and platforms Be a self-starter and comfortable working in a fast-paced environment  About LanguageWire  Most important things to know about us  We eliminate language barriers so people understand each other. Everywhere.  We are a tech company  We are an AI company  We are stable, and we are growing   More detail about us At LanguageWire, we are leaders in the world of Language Service Providers, but we are cooler than our competitors. Want to know why? Read on. We are fueled by the most advanced technology AI and our goal is to make customers lives easier by simplifying their communication with any audience across the globe.  We are proud of our unique AI technology and our exquisite customer care. We are even prouder of our people. From our language experts, tech nerds, and customer success drivers to the core of heroes in the background. They all have a stake in supporting the delivery of translations, editing, desktop publishing, and other crazy cool multilingual communication services.  We listen and we care. We adapt our solutions to customer needs and integrate them with customer-specific tech stacks, streamlines and automated workflows. All of this while ensuring all data is protected thoroughly in a secure, meteorite-proof infrastructure. Basically, weve got the whole package! Our values  We are curious. We are trustworthy. We are caring. We are ambitious.  At LanguageWire, we are curious and intrigued by what we dont understand. We believe relationships are based on honesty and responsibility, and being trustworthy reinforces an open, humble, and honest way of communicating. We are caring and respect each other personally and professionally. We encourage authentic collaboration, invite feedback and a positive social environment. Our desire to learn, build, and share knowledge is a natural part of our corporate culture. Technology is a big part of what keeps LanguageWire ahead of the competition, but we never forget its a business built and powered by people. We ensure that our entire team is happy with what they do and we always incite each other to be their best. Because at the end of the day, thats why our customers love to work with usthey enjoy working with a team of enthusiastic  ambitious people!  Working at LanguageWire  why we like it  Our purpose and values are flowing through LanguageWire in many ways in the collaboration between teams and colleagues, in the relation with our customers and our mindset in how we approach challenges. From an HR perspective, its great to see how curious, ambitious, trustworthy and caring all colleagues and leaders are, which makes LanguageWire an amazing place to work. All of that makes it easier to contribute to LanguageWires purpose to wire the world together with language. Julia, HR Manager, Valencia  To be part of Languagewire is to belong to a higher collective mindset of empowerment, bold approaches, and skyrocketing ambitions. I believe our success is made possible by streamlined communication, clear processes, and meaningful collaboration across multiple teams, time zones, and hierarchical positions. Everyone is easily reachable and enthusiastic to help. From my perspective, it is impossible to detach it from our drive to provide simpler, smarter and more reliable translation tools. And what better engine do we have than our core values and our purpose to share with the world. Patricia, Group Controller, Valencia  What are the perks?  We dont stand still. We dont look back; we drive forward! Are you the next Wire in our community? Consider a career with LanguageWire, and lets realize your potential together. These are some of the perks of becoming a Wire... General perks  Scale-up by facts  start-up by heart With us, you can enjoy flat hierarchies, responsibility and freedom, direct feedback, and room to stand up for your own ideas We are a fast-growing tech company that enables you to be part of the decision-making processes, and suggest new and fresh ways to come up with solutions Besides your actual paycheck, we offer benefits such as personal and professional development opportunities, ongoing support from your People  Culture Partner, internal career development and an inclusive and fun company culture International company. Offices in Copenhagen, Aarhus, Stockholm, Varberg, London, Leuven, Lille, Munich, Hamburg, Zurich, Kiev, Gdansk, Atlanta and Valencia Over 410 employees with great gender and cultural diversity At LanguageWire, we offer three flexible work options tailored to how you work best. Depending on your team, you may have the option to work full-time from the office as an ""Office Bee,"" part-time from the office as a ""Nomad,"" or full-time from home as a ""Homey."" We take care of our people and initiate many get-togethers from online fitness classes like yoga to Christmas parties. We have fun!   Want to know more? We cant wait to meet you! So, why wait til tomorrow? Apply today!  If you have any questions, please reach out to Ana Catal, People Acquisition Partner ancalanguagewire.com

"
https://startup.jobs/data-engineer-remote-in-spain-possible-languagewire-4533757,Engineer,Data Engineer (Remote in Spain Possible),LanguageWire ,"Valencia, Spain",,"

Do you want to work with a range of Microsoft Azure tools and take another step into Data Engineering  then you should definitely read on! The role youll play We are looking for an independent-minded Data Engineer who can help us further develop our Datawarehouse-infrastructure, maintain data discipline and act as a data-broker across the company. Your individual expertise, broad knowledge of data, a drive to learn, and your ability to collaborate with your teammates will be essential for your success.  The team youll be a part of In Business Finance, we are a small team that is based on data, aims to deliver insights  knowledge - in an easy and intuitive way, to all of LanguageWire. We are always open to new ideas and using new technology to help us along the way.  If you want to make a difference, make it with us by  Being responsible for a data pipeline infrastructure for ETL in Azure ADF, using a wide variety of data sources relational-  non-relational  Perform tasks such as  Integrate new data sources via APIs, Data transfers etc.  Data cleaning  -aggregation  Maintain and Build new Tabular Cubes  Power BI Reports DAX   Assemble large and complex data sets to meet functionalnon-functional business requirements. Identify and automate process improvements  Optimizing data-loads etc.  Ensure operations stability, perform tests and evaluations regularly to ensure data integrity In one year, youll know you were successful if  You have added cool new data sources for our Machine Learning- or Marketing teams You have changed our Tabular model architecture carve-out, build new cubes You have removed logic from a PowerBI report and pushed it to the Tabular model. Youve identified improvement points of the architecture and applied solutions to optimize them. Youve learned and grown professionally.  Desired experience and competencies  What does it take to work for LanguageWire?  What youll need to bring  1-3 years of experience in a similar position Bachelors or Masters Degree in Computer Science, Computer Engineering or similar. Exceptional problem-solving skills Experience with Microsoft data stack on-premise and Azure Knowledge about TSQL coding, stored procedures, views, troubleshooting  debugging. DAX, Tabular Editor  Power BI ETL  Data Management Tools CICD, DevOps   Being able to convey your results through conversations, written communications minimum English C1  This will make you stand out   Knowledge of Python, Databricks, Machine Learning or similar General coding skills to independently gather data from various sources  A genuine interest in exploring and utilizing new technologies and platforms Be a self-starter and comfortable working in a fast-paced environment  About LanguageWire  Most important things to know about us  We eliminate language barriers so people understand each other. Everywhere.  We are a tech company  We are an AI company  We are stable, and we are growing    More detail about us At LanguageWire, we are leaders in the world of Language Service Providers, but we are cooler than our competitors. Want to know why? Read on. We are fueled by the most advanced technology AI and our goal is to make customers lives easier by simplifying their communication with any audience across the globe.  We are proud of our unique AI technology and our exquisite customer care. We are even prouder of our people. From our language experts, tech nerds, and customer success drivers to the core of heroes in the background. They all have a stake in supporting the delivery of translations, editing, desktop publishing, and other crazy cool multilingual communication services.  We listen and we care. We adapt our solutions to customer needs and integrate them with customer-specific tech stacks, streamlines and automated workflows. All of this while ensuring all data is protected thoroughly in a secure, meteorite-proof infrastructure. Basically, weve got the whole package! Our values  We are curious. We are trustworthy. We are caring. We are ambitious.  At LanguageWire, we are curious and intrigued by what we dont understand. We believe relationships are based on honesty and responsibility, and being trustworthy reinforces an open, humble, and honest way of communicating. We are caring and respect each other personally and professionally. We encourage authentic collaboration, invite feedback and a positive social environment. Our desire to learn, build, and share knowledge is a natural part of our corporate culture. Technology is a big part of what keeps LanguageWire ahead of the competition, but we never forget its a business built and powered by people. We ensure that our entire team is happy with what they do and we always incite each other to be their best. Because at the end of the day, thats why our customers love to work with usthey enjoy working with a team of enthusiastic  ambitious people!   Working at LanguageWire  why we like it   Our purpose and values are flowing through LanguageWire in many ways in the collaboration between teams and colleagues, in the relation with our customers and our mindset in how we approach challenges. From an HR perspective, its great to see how curious, ambitious, trustworthy and caring all colleagues and leaders are, which makes LanguageWire an amazing place to work. All of that makes it easier to contribute to LanguageWires purpose to wire the world together with language. Julia, HR Manager, Valencia   To be part of Languagewire is to belong to a higher collective mindset of empowerment, bold approaches, and skyrocketing ambitions. I believe our success is made possible by streamlined communication, clear processes, and meaningful collaboration across multiple teams, time zones, and hierarchical positions. Everyone is easily reachable and enthusiastic to help. From my perspective, it is impossible to detach it from our drive to provide simpler, smarter and more reliable translation tools. And what better engine do we have than our core values and our purpose to share with the world. Patricia, Group Controller, Valencia    What are the perks?  We dont stand still. We dont look back; we drive forward! Are you the next Wire in our community? Consider a career with LanguageWire, and lets realize your potential together. These are some of the perks of becoming a Wire...  General perks  Scale-up by facts  start-up by heart With us, you can enjoy flat hierarchies, responsibility and freedom, direct feedback, and room to stand up for your own ideas We are a fast-growing tech company that enables you to be part of the decision-making processes, and suggest new and fresh ways to come up with solutions Besides your actual paycheck, we offer benefits such as personal and professional development opportunities, ongoing support from your People  Culture Partner, internal career development and an inclusive and fun company culture International company. Offices in Copenhagen, Aarhus, Stockholm, Varberg, London, Leuven, Lille, Munich, Hamburg, Zurich, Kiev, Gdansk, Atlanta and Valencia Over 410 employees with great gender and cultural diversity At LanguageWire, we offer three flexible work options tailored to how you work best. Depending on your team, you may have the option to work full-time from the office as an ""Office Bee,"" part-time from the office as a ""Nomad,"" or full-time from home as a ""Homey."" We take care of our people and initiate many get-togethers from online fitness classes like yoga to Christmas parties. We have fun!    Valencia Perks  More than 200 great colleagues in different business departments. Working in an international environmentmore than 20 different nationalities A dog friendly atmosphere  Big kitchen with free access to organic fruits and coffee    Excellent location in cool and modern offices in the city center, with a great rooftop terrace and a view over the Town Hall Square    Want to know more?  We cant wait to meet you! So, why wait til tomorrow? Apply today!  If you have any questions, please reach out to Ana Catal, People Acquisition Partner ancalanguagewire.com

"
https://startup.jobs/data-engineer-consultant-italy-milan-capco-4533745,"Consulting,Engineer",Data Engineer Consultant - Italy Milan,Capco ,,,"

Data Engineer Consultant - Financial Services Consultancy - Italy Milan 
Joining Capco means joining an organisation that is committed to an inclusive working environment where youre encouraged to BeYourselfAtWork. We celebrate individuality and recognize that diversity and inclusion, in all forms, is critical to success. Its important to us that we recruit and develop as diverse a range of talent as we can and we believe that everyone brings something different to the table  so wed love to know what makes you different. Such differences may mean we need to make changes to our process to allow you the best possible platform to succeed, and we are happy to cater to any reasonable adjustments you may require. You will find the section to let us know of these at the bottom of your application form or you can mention it directly to your recruiter at any stage and they will be happy to help.
 
About Capco
Capco is a global technology and business consultancy, focused on the financial services sector. We are passionate about helping our clients succeed in an ever-changing industry. Capco is going through a significant growth journey, making this an exciting time to join us as we expand our consulting team in Italy.
We are

Experts in capital markets, banking and payments and wealth and asset management
Deep knowledge in financial services offering, including e.g. Finance, Risk and Compliance, Financial Crime, Core Banking etc.
Committed to growing our business and hiring the best talent to help us get there
Focus on maintaining our nimble, agile and entrepreneurial culture

As part of the consulting team of Capco, the Data Engineer will have

The ability to process and rationalize structured data, message data and semiunstructured data and ability to integrate multiple large data sources and databases into one system
Proficient understanding of distributed computing principles and of the fundamental design principles behind a scalable application
Strong knowledge of the Big Data eco system, experience with HortonworksCloudera platforms
Practical experience in using HDFS
Practical expertise in developing applications and using querying tools on top of Hive, Spark PySpark
Strong Scala OR Python skills, particularly the Anaconda environment and Python based ML model deployment
Experience of Continuous IntegrationContinuous Deployment JenkinsHudsonAnsible
Experience with using GITGITLAB as a version control system.
Experience in working in Teams using the Agile Methods SCRUM and ConfluenceJIRA
Good communication skills written and spoken, ability to engage with different stakeholders and to synthesise different opinions and priorities

 
Desirable Skills

Knowledge of at least one Python web framework preferably Flask, Tornado, andor twisted
Basic understanding of front-end technologies, such as JavaScript, HTML5, and CSS3 would be a plus
Good understanding of global markets, markets macrostructure and macro economics
Knowledge of Elastic Search Stack ELK
Experience with Google Cloud Platform Data Proc  Dataflow

Domain Knowledge 

Knowledge of and experience using data models and data dictionaries in a Banking and Financial Markets context.  Knowledge of Trade Finance or Securities Services particularly useful.
Knowledge of one or more of the following domains including market data vendors

PartyClient
Trade
Settlements
Payments
Instrument and pricing
Market andor Credit Risk



Experience using below languagestools 

Java
HQL, SQL
Querying tools on top of Hive, Spark PySpark
Scala
Python, particularly the Anaconda environment
GITGITLAB as a version control system

 
Why Join Capco?
You will work on engaging projects with some of the largest banks in the world, on projects that will transform the financial services industry.
We offer

A work culture focused on innovation and building lasting value for our clients and employees
Ongoing learning opportunities to help you acquire new skills or deepen existing expertise
A flat, non-hierarchical structure that will enable you to work with senior partners and directly with clients
A diverse, inclusive, meritocratic culture

Next Steps
If youre looking forward to progressing your career with us, then were looking forward to receiving your application.

"
https://startup.jobs/data-engineer-atomic-3-4532270,Engineer,Data Engineer,Atomic ,,Full-Time,"

Villa is building Americas leading next-generation homebuilding platform. With a mission to be the easiest, fastest, and most cost-efficient way to build homes, Villa is a highly scalable new approach to offsite homebuilding and is playing a critical role in solving the many problems facing the U.S. housing market. Villa provides end-to-end services for clients that span feasibility, design, permitting, and construction of high-quality homes built using modern offsite construction. Villa is currently the largest ADU builder in California and is growing rapidly into other housing products and geographies.

Role Overview
We are revolutionizing the real estate industry by leveraging data-driven solutions. Our innovative platform helps investors, developers, and construction professionals make informed decisions about project feasibility, site work costs, and more. We are seeking a talented Data Engineer to join our dynamic team, report to the Head of Engineering, and contribute to our mission of transforming the way the industry operates.

As a Data Engineer, you will play a crucial role in the development and maintenance of our data infrastructure, ensuring the efficient collection, storage, and processing of diverse data sources. You will collaborate with cross-functional teams, including data scientists, software engineers, and business stakeholders, to build robust models that estimate site work costs and assess project feasibility. This role requires a strong blend of technical expertise, problem-solving skills, and a passion for working with large datasets to extract valuable insights. If you are a highly motivated and skilled Data Engineer, we would love to hear from you.
What Youll Do

The Data Engineer role involves the following responsibilities
Data Collection and Integration
Identify and evaluate relevant data sources, both internal and external, to support site work cost estimation and project feasibility analysis.
Design and implement data collection strategies, ensuring data quality, consistency, and integrity.
Develop and maintain data pipelines, leveraging ETL Extract, Transform, Load processes to efficiently integrate data from multiple sources.
Data Modeling and Analysis
Collaborate with data scientists and domain experts to understand requirements and develop data models that support site work cost estimation and project feasibility analysis.
Design and implement scalable and efficient algorithms to extract insights from complex datasets.
Apply statistical methods and machine learning techniques to develop predictive models that estimate site work costs and assess project feasibility.
Data Infrastructure and Optimization
Build and maintain scalable and robust data infrastructure, including data warehouses, databases, and data processing systems.
Identify and implement improvements to enhance data pipeline efficiency, data quality, and overall system performance.
Monitor data pipelines, proactively identifying and resolving issues to ensure uninterrupted data flow.
Collaboration and Communication
Work closely with cross-functional teams, including data scientists, software engineers, and business stakeholders, to understand requirements and deliver high-quality data solutions.
Communicate complex technical concepts and findings to non-technical stakeholders effectively.
Stay up-to-date with industry trends and advancements in data engineering, sharing knowledge and insights with the team.

What You Have

Bachelors or Masters degree in Computer Science, Data Engineering, or a related field.
Proven experience 2 years as a Data Engineer or similar role, working with large datasets and building data pipelines.
Strong programming skills in languages such as Python, SQL, and Scala.
Proficient in working with databases and data warehousing technologies e.g., SQL, NoSQL, PostgreSQL, Amazon Redshift.
Familiarity with data processing frameworks and tools e.g., Apache Spark, Hadoop, Airflow.
Experience with cloud platforms such as AWS, Azure, or Google Cloud.
Knowledge of statistical analysis, machine learning, and data modeling techniques.
Strong problem-solving skills and attention to detail.
Excellent communication and collaboration skills.


We are focused on building a diverse and inclusive workforce. If youre excited about this role, but do not meet 100 of the qualifications listed above, we encourage you to apply.

Villa is an Equal Opportunity Employer and considers applicants for employment without regard to race, color, religion, sex, orientation, national origin, age, disability, genetics or any other basis forbidden under federal, state, or local law. Villa considers all qualified applicants in accordance with the San Francisco Fair Chance Ordinance.

Please review our CCPA policies here.

LI-Remote

"
https://startup.jobs/data-engineer-rockstar-games-4532171,Engineer,Data Engineer,Rockstar Games ,"Bengaluru, India",,"

At Rockstar Games, we create world-class entertainment experiences.
A career at Rockstar Games is about being part of a team working on some of the most creatively rewarding and ambitious projects to be found in any entertainment medium. You would be welcomed to a dedicated and inclusive environment where you can learn, and collaborate with some of the most talented people in the industry.
Rockstar India is seeking a Data Engineer to join a team focused on building a cutting-edge game analytics platform and tools to better understand our players and enhance their experience in our games. This is a full-time permanent position based out of Rockstars unique game development studio in Bangalore. 
The ideal candidate will be skilled in developing complex ingestion and transformation processes with an emphasis on reliability and performance. In collaboration with other data engineers, machine learning engineers, and software engineers, the candidate will empower the team of analysts and data scientists to deliver data driven insights and applications to company stakeholders.
WHAT WE DO

The Rockstar Analytics team provide insights and actionable results to a wide variety of stakeholders across the organization in support of their decision making.
We are currently adding team members to multiple verticals including; Machine Learning and Game Data Pipeline.

RESPONSIBILITIES

Implement and maintain real-time and batch Data Models.
Deliver real-time and non-real-time data models to analysts and data scientists who create insights and analytics applications for our stakeholders.
Implement and support streaming technologies such as Kafka, Spark, Cassandra  AzureML.
Assist in the development of deployment automation and operational support strategies.
Assist in the development of a big data platform in Hadoop using pipeline technologies such as Spark, Airflow, and more to support a variety of requirements and applications.
Set the standards for warehouse and schema design in massively parallel processing engines such as Hadoop and Snowflake while collaborating with analysts and data scientist in the creation of efficient data models.
Maintain and extend our CICD processes and documentation.

QUALIFICATIONS

3 years of work experience with data modeling, business intelligence and machine learning on big data architectures.
2 years of experience with the Hadoop ecosystem HDFS, Spark, Oozie, Impala, etc. and big data ecosystems Kafka, Cassandra, etc..
2 years of experience with the Azure ecosystem Azure ML, Azure Data Factory.
Expert in at least one SQL language such as T-SQL or PLSQL.
Experience developing and managing data warehouses on a terabyte or petabyte scale.
Strong experience in massively parallel processing  columnar databases.
Experience building Real-Time andor Near-Real-Time ML pipelines.
Experience with Python, Scala, or Java.
Experience with shell scripting.
Experience working in a Linux environment.

SKILLS

Deep understanding of advanced data warehousing concepts and track record of applying these concepts on the job.
Ability to manage numerous projects concurrently and strategically, prioritizing when necessary.
Good communication skills.
Dynamic team player.
A passion for technology.

PLUSES
Please note that these are desirable skills and are not required to apply for the position.

Experience with Python based libraries such as Scikit-Learn.
Experience with Databricks.
Experience with Spark-ML, Jupyter Notebook, AzureML.
Experience in Lambda architecture.
Experience with CICD.
Familiar with Restful APIs.
Experience with Artifact Repositories.
Knowledge of the video game industry.

HOW TO APPLY
Please apply with a resume and cover letter demonstrating how you meet the skills above. If we would like to move forward with your application, a Rockstar recruiter will reach out to you to explain next steps and guide you through the process.
Rockstar is proud to be an equal opportunity employer, and we are committed to hiring, promoting, and compensating employees based on their qualifications and demonstrated ability to perform job responsibilities.
If youve got the right skills for the job, we want to hear from you. We encourage applications from all suitable candidates regardless of age, disability, gender identity, sexual orientation, religion, belief, race, or any other protected category.

"
https://startup.jobs/data-engineer-in-pilsen-czech-republic-unacast-4531978,Engineer,Data Engineer in Pilsen (Czech Republic),Unacast ,"Pilsen, Czechia",Full-Time,"

What does Unacast do?

Unacast is a company that strives to understand our world and human behavior in it. We do so by analyzing petabytes of data about mobility, adjacent location information, population statistics, maps, textual sources, and many other useful datasets. What drives us is the fact that our products - such as Migration Patterns or Venue Traffic estimates - help others in making informed decisions when location understanding matters. The company, originally founded in Norway, is currently based in three locations - Oslo, New York and Pilsen. All three branches collaborate very closely together in order to build and provide our solutions to clients who mainly come from the US. True to its origin, the company is a proud advocate of Scandinavian work culture and ethical and privacy standards across all its locations.

You are a Data Engineer, but are you an inventor?

At Unacast, we are developing tools for rapidly growing data industries. We innovate to create new tools which leverage sources of data that are only now reaching maturity. As a result, we are not only developers, we are inventors! Majority of the challenges that we are working on have not been tackled before. Therefore, it is up to us to come up with novel solutions, embrace new technologies and be able to defend our approaches.

If what you just read excites you, come and join us!



We are hiring a Data Engineer 


We are looking for a Data Engineer to join one of our teams in the Czech Republic Pilsen. The team is fully responsible for development of one of the core products of the company - Migration Patterns - which is a dataset that continuously analyzes the population shifts in the US. Good understanding of this phenomenon backed by solid data and sound processing helps vendors, municipalities, large corporations, but also government agencies to assess potentials and risks in areas of their interest early on, when no other data is available.

Together with this product, we are in charge of exploration, processing and making sense of contextual data that provide important insights about locations and portray a more vivid picture about the reasons for mobility we observe. Since there is an endless list of interesting data sources, it is crucial that our team and you identifies the quality, impact and value of the data before it becomes part of our products.

You will be involved in all these challenges. From making sure that Airflow pipelines are running when they should, our production data is being smoothly rolled out, our monitoring is able to spot any integrity or quality issue, to designing and building an entire framework for onboarding and processing datasets that range from GPS data, map sources, Census tables, and tons of others diverse sources. Our vision is that Unacasts platform will be enriched with thousands of live data sources that will allow us to distill the essence of every single place.
This is an on-site hybrid position with 2-3 days in the week or more if you want working in our Pilsen office located in the city center. We use these days to plan, discuss solutions, collaborate and do activities together we like to go running and do yoga together. Being part of Unacast means joining a friendly and efficient workplace with ample opportunities for growth within the company.

What you will be doing on a daily basis?
- Designing and developing the ecosystem that ensures seamless operation of our contextual data sources.
- Building and maintaining pipelines that ingest and process data on an ongoing basis.
- Productionizing statistical and ML models in cooperation with data scientists.
- Coding and reviewing PRs of your colleagues.
- Be in charge of our environment.

Who we think you are?
- You are a person with a positive and proactive attitude, who can tackle problems independently, yet is a good collaborator and a team player.
- You have a good understanding and experience with processing large quantities of data.
- You are familiar with Python and SQL for data processing.
- You are familiar with a Cloud platform GCP, AWS, Azure, etc. 
- You are proficient and comfortable in English companys primary language.

... and it is positive if you have experience with
- Geospatial data
- GCP
- Airflow
- MLOps
- Graph databases
- PyTorch

Our current tech stack
Google Cloud Platform GCP suite, Python, SQL BQ dialect, Git, Apache Airflow, Kubernetes, OpsGenie, ElasticSearch  Kibana, etc.

What we expect of you?
- You like to continuously learn and work on yourself.
- We are a startup, so things can change fast and you need to be agile and proactive on this journey.
- We have customers using the product daily, so some on-call may be required.
- Be your own CEO, and have fun seriously! Unacast guiding principles.

What you can expect of us?
- Interesting work on a unique challenge
- Amazing colleagues from many countries
- Cozy office shared with a young architecture studio
- Competitive salary
- 5 weeks of vacation
- Hybrid work mode 2 days from the office
- A personal development manager
- Monthly hackathons
- Latest Macbook Pro or equivalent with corresponding peripherals
- Free lunches when at the office
- Common activities with your colleagues
- Snacks and good coffee at the office
- Budget for work-from-home equipment
- Budget for socks
- Travels to the other company branches and conferences based on need




Who is Unacast?

Verified, actionable data facilitates fundamental insights in industries such as research, retail, real-estate, finance, advertising technology, city planning, transportation, and artificial intelligence. Location data is a key component, the core intelligence, that creates value for the players in these industries.

Unacast is a leader in transparent, contextualized location data. Like Google indexed the web, Unacast is indexing the physical world, designing products that merge multiple types of data on human mobility to empower companies and individuals alike to make smarter decisions. 


Unacast is a fast-growing Nordic company based on Nordic values with offices in Oslo Norway, NYC United States, and Pilsen Czech Republic. We have been named ""The 1 Small Company To Work At In NYC"" and have been featured everywhere from Techcrunch to Forbes.


Our company values are deeply embedded into everything we are and everything we do


Be your own CEO Everyone at Unacast can and is expected to take charge of their own situation, and make sure the company as a whole moves forward. Waiting for some feedback? Go get it! Want something done? Do it! 



Trust through transparency Dare to be vulnerable and show ourselves and the world who we really are. The trust we build through transparency will help to lift us all higher than we can rise alone.



Have fun, seriously Create an unexpected moment that surprises and delights team members and partners. Everyone needs fun in their life, especially while doing something important.


Read more about our company and culture at httpswww.unacast.comcareers

"
https://startup.jobs/data-engineer-valence-digital-4530482,Engineer,Data Engineer,Valence Digital ,,,"

Valence harnesses the power of Web3 to build deeper value exchanges between industries, businesses, and consumers. Were a data enablement company that leverages public main-net blockchains to connect businesses to consumer data. Our solutions allow users to store, trade, authenticate, and prove the origin of data and assets, no matter where the data or the user transacts.
We are seeking a Data EngineerAnalyst who will be responsible for managing and analyzing our data. You will set up and maintain data lakes, develop data pipelines, and perform data modeling and analysis to support our decision-making and product development.
What youll do

Design and implement data lake architecture to store and manage large volumes of structured and unstructured data.
Configure and optimize data lake infrastructure, including storage, access controls, and security protocols.
Ensure data quality and integrity within the data lake environment.
Design and implement efficient and scalable data pipelines to extract, transform, and load ETL data from various sources into data repositories.
Perform data modeling to ensure optimal data storage and retrieval.
Optimize data pipelines for performance, reliability, and scalability.
Work closely with cross-functional teams and software engineers, to understand their data needs and requirements.
Provide technical guidance and support on data engineering best practices, tools, and technologies.
Collaborate on the development of data governance policies and data management practices.
Identify and resolve issues related to data ingestion, transformation, and storage.
Implement data monitoring and alerting mechanisms to proactively detect and address data quality or performance issues.
Continuously evaluate and optimize data engineering processes, systems, and infrastructure to improve efficiency and reliability.

 
Who you are

Bachelors degree in Computer Science, Information Technology, or a related field
3 years of experience working in a data engineering or data analysis role, preferably with a focus on building scalable data solutions.
Hands-on experience in designing and implementing data pipelines, data models, and data integration processes.
Strong understanding of data management principles, data governance, and data quality assurance.
Proficiency in programming languages such as Golang, Python, Java, or Scala for data manipulation and transformation. 
Experience with SQL and database technologies for data querying and optimization.
Familiarity with data warehousing concepts, data modeling techniques, and ETLELT processes.
Hands-on experience with data pipelines frameworks and tools, such as Apache Airflow, Apache Kafka, or AWS Glue.
Knowledge of data ingestion techniques, data extraction, transformation, and loading ETL processes.
Demonstrated ability to collaborate with cross-functional teams, including data scientists, analysts, and software engineers, to achieve common goals.
Proven experience working in an Agile or collaborative team environment, participating in meetings, and contributing to project discussions.
Familiarity or interest with the Web3Blockchain space is a bonus, but not required.

 
If you have a passion for working on Web3 projects, are interested in cryptocurrency, and want to be part of something truly innovative in this space, then we are the perfect fit for you. Our current roadmap is designed to provide opportunities for developers to leverage an efficient, secure, and cost-effective chain.The approximate compensation range is 140,000 to 180,000. The actual offer, reflecting the total compensation package and benefits, will be determined by a number of factors including the applicants experience, knowledge, skills, and abilities, as well as internal equity among our team.

Valence is a dynamic, diverse, innovative and friendly place to work. We embrace our differences and believe they fuel our creativity. We come from varied backgrounds and think thats important. But whether its taking ideas from previous lives and applying them in different ways or creating something completely new, we are all trail-blazing team players who think big and want to make an impact. 
 
The Perks!
Here are just some of the many benefits of becoming a Valence employee
 

Health, Dental  Vision Insurance Your health is number 1, so we offer 100 company-paid health, dental, and vision insurance starting on day 1 for yourself and any dependents.

 

Unlimited Vacation We offer Unlimited PTO, plus additional paid company holidays.
 

Commuter Benefits Valence offers WageWorks commuter benefits. 
 

Parental Leave We believe that family comes first, so we provide parental leave to all new parents. 
 

Food All Day Fully stocked refrigerator, and lunch provided every day that youre in the office!


"
https://startup.jobs/data-engineer-remote-keystone-peer-review-organizat-4530449,Engineer,Data Engineer (Remote),"Keystone Peer Review Organization, Inc. ","Cary, United States",Full-Time,"

Data Engineer Remote   Are you an experienced Data Engineer looking for a new challenge?  Do you value care management and quality improvement? Are you motivated, energetic, and excited to become part of the Acentra Health team?  If so, you might be our next new team member! Who we need  The Data Engineer will design, develop, and maintain scalable data pipelines and systems to support the collection, integration, and analysis of healthcare and enterprise data. The primary responsibilities of this role include designing and implementing efficient data pipelines, architecting robust data models, and adhering to data management best practices. In this position, you will play a crucial part in transforming raw data into meaningful insights, through development of semantic data layers, enabling data-driven decision-making across the organization. The ideal candidate will possess strong technical skills, a keen understanding of data architecture, and a passion for optimizing data processes.   Why us? Acentra Health is a rapidly growing national quality improvement and care management organization. We work to ensure that over 20 million people receive the right care, at the right time, in the right setting.  People Focused. Mission Driven.   Shape the future of healthcare with us. We are mission driven to improve lives through healthcare quality and clinical expertise.  We do this through our people. At Acentra Health, you can do meaningful work that makes a real difference for the lives of individuals across the country. We are an organization that cares deeply about our employees and we provide the training and support to do the best work of your career. Benefits are a key component of your rewards package at Acentra Health. These benefits are designed to provide you and your family additional protection, security, and support for both your career and your life away from work. They are comprehensive and fit a variety of needs and situations. Our benefits include comprehensive health plans, paid time off, retirement savings, corporate wellness, educational assistance, corporate discounts and more.  What youll do   Design and implement scalable and efficient data pipelines to acquire, transform, and integrate data from various sources, such as electronic health records EHR, medical devices, claims data, and back-office enterprise data Develop data ingestion processes, including data extraction, cleansing, and validation, ensuring data quality and integrity throughout the pipeline Collaborate with cross-functional teams, including subject matter experts, analysts, and engineers, to define data requirements and ensure data pipelines meet the needs of data-driven initiatives Design and implement data integration strategies to merge disparate datasets, enabling comprehensive and holistic analysis  Implement data governance practices and ensure compliance with healthcare data standards, regulations e.g., HIPAA, and security protocols Monitor and troubleshoot pipeline and data model performance, identifying and addressing bottlenecks, and ensuring optimal system performance and data availability  Design and implement data models that align with domain requirements, ensuring efficient data storage, retrieval, and delivery  Apply data modeling best practices and standards to ensure consistency, scalability, and reusability of data models Implement data quality checks and validation processes to ensure the accuracy, completeness, and consistency of healthcare data Develop and enforce data governance policies and procedures, including data lineage, architecture, and metadata management  Collaborate with stakeholders to define data quality metrics and establish data quality improvement initiatives Document data engineering processes, methodologies, and data flows for knowledge sharing and future reference  Stay up to date with emerging technologies, industry trends, and healthcare data standards to drive innovation and ensure compliance   What youll need  Required Qualifications  Bachelors or Masters degree in computer science, information systems, or a related field.  Knowledge, Skills, Abilities    Strong programming skills in object-oriented languages such as Python  Proficiency in SQL  Hands on experience with data integration tools, ETLELT frameworks, and data warehousing concepts  Hands on experience with data modeling and schema design, including concepts such as star schema, snowflake schema and data normalization Familiarity with healthcare data standards e.g., HL7, FHIR, electronic health records EHR, medical coding systems e.g., ICD-10, SNOMED CT, and relevant healthcare regulations e.g., HIPAA Hands on experience with big data processing frameworks such as Apache Hadoop, Apache Spark, etc. Working knowledge of cloud computing platforms e.g., AWS, Azure, GCP and related services e.g., S3, Redshift, BigQuery  Experience integrating heterogeneous data sources, aligning data models and mapping between different data schemas Understanding of metadata management principles and tools for capturing, storing, and managing metadata associated with data models and semantic data layers Ability to track the flow of data and its transformations across data models, ensuring transparency and traceability Understanding of data governance principles, data quality management, and data security best practices  Strong problem-solving and analytical skills with the ability to work with complex datasets and data integration challenges  Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams 
The list of accountabilities is not intended to be all-inclusive and may be expanded to include other education- and experience-related duties that management may deem necessary from time to time. Experience

Proven experience as a Data Engineer or similar role with a focus on healthcare data.   Thank You!  We know your time is valuable and we thank you in advance for applying for this position. Due to the high volume of applicants we receive, only those who are chosen to advance in our interview process will be contacted. We sincerely appreciate your interest in Acentra Health and invite you to apply to future openings that may be of interest. Best of luck in your search!  The Acentra Health Talent Acquisition Team Mental and Physical Requirements  The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations will be made as required by law in an attempt to enable an individual with a disability to perform the essential functions of this job. While performing the duties of this job, the employee is regularly required to sit for prolonged periods of time; key andor control objects; interact extensively with internal and external customers; occasionally lift andor move objects weighing up to 10 pounds; and occasionally travel within the state.  EOE AA MFVetDisability  Acentra Health is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. 

"
https://startup.jobs/data-engineer-r01526767-brillio-4529477,Engineer,Data Engineer - R01526767,Brillio ,"San Ramon, United States",Full-Time,"

About Brillio 
Brillio is the partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Backed by Bain Capital, Brillio is one of the fastest growing digital technology service providers. We help clients harness the transformative potential of the four superpowers of technology - cloud computing, internet of things IoT, artificial intelligence AI, and mobility. Born digital in 2014, we apply Customer Experience Solutions, Data Analytics and AI, Digital Infrastructure and Security, and Platform and Product Engineering expertise to help clients quickly innovate for growth, create digital products, build service platforms, and drive smarter, data-driven performance. With delivery locations across United States, Romania, Canada, Mexico, and India, our growing global workforce of over 6,000 Brillians blends the latest technology and design thinking with digital fluency to solve complex business problems and drive competitive differentiation for our clients. Brillio was awarded Great Place to Work in 2021 and 2022

Data Engineer
Primary Skills
Cloud front, Aurora, Athena, AWS EC2, Data Warehousing, ETL Fundamentals, Glue, Lake Formation, Lambda, Modern Data Platform Fundamentals, PLSQL, Python, S3, Spark - Pyspark, SQL Basic  Advanced, Step Functions, EBS EFS 
Specialization
AWS Data EngineerIng Basic Data Engineer 
Job requirements
create curated views for analytics, strong sql is mandatory AWS Basics is good to have 

 

Know what its like to work and grow at Brillio Click here


"
https://startup.jobs/data-engineer-contract-netomi-4527933,Engineer,Data Engineer (Contract),Netomi ,"Gurugram, India",Contractor,"

At Netomi AI, we are on a mission to create artificial intelligence that builds customer love for the worlds largest global brands.

Some of the largest brands are already using Netomi AIs platform to solve mission-critical problems. This would allow you to work with top-tier clients at the senior level and build your network. 

Backed by the worlds leading investors such as Y-Combinator, Index Ventures, Jeffrey Katzenberg co-founder of DreamWorks and Greg Brockman co-founder  President of OpenAIChatGPT, you will become a part of an elite group of visionaries who are defining the future of AI for customer experience. We are building a dynamic, fast growing team that values innovation, creativity, and hard work. You will have the chance to significantly impact the companys success while developing your skills and career in AI.

Want to become a key part of the Generative AI revolution? We should talk. 

Job Description

Netomi is seeking a highly analytical and detail-oriented candidate to join the Analytics team in Gurugram. As part of the team, you will work with product, engineering, and customer success teams to drive complex data and trend analyses to propose ways to improve and, thereby contributing to improve the experience. You will also be responsible for benchmarking and measuring the performance of various product operations projects, building and publishing detailed scorecards and reports, identifying and driving new opportunities based on customer and business data.

We are looking for a Data Engineer with a passion for using data to discover and solve real-world problems. You will enjoy working with rich data sets, modern business intelligence technology, and the ability to see your insights drive the features for our customers. You will also have the opportunity to contribute to the development of policies, processes, and tools to address product quality challenges in collaboration with teams.
Responsibilities

You will partner with teammates to create complex data processing pipelines in order to solve our clients most ambitious challenges.
 You will collaborate with Data Scientists in order to design scalable implementations of their models.
You will pair to write clean and iterative code based on TDDLeverage various continuous delivery practices to deploy, support and operate data pipelines.
Advise and educate clients on how to use different distributed storage and computing technologies from the plethora of options available.
Develop and operate modern data architecture approaches to meet key business objectives and provide end-to-end data solutions.
Create data models and speak to the tradeoffs of different modeling approaches.
Seamlessly incorporate data quality into your day-to-day work as well as into the delivery process.

Requirements

3 Years of Experience
Expertise in SQL, PLSQL  General Software Engineering proficiency coding in PythonJava any one language
Experience in MySQL 8.0, and AWS Aurora required
Expert SQL query optimization
Have a good understanding of data modeling and experience with data engineering tools 
You are comfortable taking a data-driven approach and applying data security strategy to solve business problems
Youre genuinely excited about data infrastructure and operations with familiarity working in cloud environments
Working with data excites you you can build and operate data pipelines, and maintain data storage, all within distributed systems
Assure effective collaboration between Netomi and the clients teams, encouraging open communication and advocating for shared outcomes
Experience writing data quality units and functional tests. 
Strong Experience with any Relational Database preferably Aurora MySQL
Experience with AWS Lambda, Kinesis, RDS, EC2, Quicksight
Experience with Streaming Platforms such as Kafka, Kinesis, etc.


Netomi is an equal opportunity employer committed to diversity in the workplace. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, disability, veteran status, and other protected characteristics.

"
https://startup.jobs/data-engineer-sword-4526831,Engineer,Data Engineer,Sword ,"Glasgow, United Kingdom",Full-Time,"

Sword propels organisations on their data driven journey; where decisions are based on facts, metrics are used to motivate, data is openly shared, and trust and transparency are prioritised above all else.

In response to our continued high-growth trajectory, Swords Digital Platforms Team have a new opening for an experienced and motivated Data Engineer to support a strategic managed service partner. Glasgow based but remote 4 days per week.

Working within our Professional Service Business Unit as a Data Engineer, this position will suit someone who can provide architectural and data engineering subject matter expertise in our client projects.

You will join a highly experienced team in our Data  AI practice who are supporting Swords clients on their journey to become data driven organisations.
Requirements
The person must have
 Good communication skills with experience in dealing with senior stakeholders and business ambassadors. Knowledge of Agile delivery methodologies and Azure DevOps Good analytical and problem-solving skills A challenging and enquiring mind Attention to detail and tenacity. Solid knowledge of Data  AI Good business consultancy skills Team player and confident individualist 
Technical Certifications
 Azure Data Engineering Azure Data Science Azure Data Analyst 

Responsibilities include
 Working on high profile projects with well-known clients across various sectors Desire to continue professional development to be able to support with the latest technology  Working directly with clients to translate business requirements into data solutions  Taking ownership for the delivery of core solution components 
Technology
 Microsoft Purview or similar Microsoft Azure Cloud Technology Azure Synapse Analytics, Data Factory Datalake, Big Data, Cosmos DB Proficient in data modelling  Power BI Proficient in SQL, Python, Scala, T-SQL 

Other relevant info   Working on high profile projects with well-known clients across various business sectors Constantly developing technical skills using the latest cloud technology - achieving  maintaining Microsoft certification Working directly with clients to understand requirements and presenting solutions to customer sponsors Taking ownership for the delivery of core solution components   This role is for ambitious technologists who wish to work with the latest cloud technology from Microsoft. We do not expect everyone to have all the required technical expertise listed but it is essential that you have worked with Microsoft Azure and data.
Benefits
Sword offers career paths in rapidly evolving technology spaces including Data  AI, Modern Managed Services, Information Management, Digital Services, Content Services, and Modern Workplace Transformation.

Our team culture is based on building inclusive teams, investing in training and development, the quality of our interactions with our customers and our position as an employer of choice in the areas in which we operate.

All Sword Group colleagues are supported and encouraged to develop their career with Sword through our personal training and development plan alongside a competitive salary, pension, private healthcare, and employee assistance programme.

"
https://startup.jobs/data-engineer-l4-games-netflix-4525108,Engineer,Data Engineer (L4) - Games,Netflix ,,Full-Time,"

Now is an amazing time to join Netflix as we seek to entertain the world. We have over 200 million paid members in over 190 countries, and we wont stop there. Games are our next big frontier and an incredible opportunity for us to deliver new experiences to delight and entertain our quickly growing membership. You will be jumping in at the very beginning of this adventure and be in a position to help us redefine what a Netflix subscription means for our members around the world.

Data Science and Engineering DSE at Netflix is aimed at using data, analytics, and sciences to improve various aspects of our business. We are looking for a Data Engineer to join our Games DSE team, supporting key pipelines to determine both individual game, and overall game portfolio performance. This role will be partnering closely with our Game studio stakeholders to understand the kinds of data that will need to be collected.  As a member of this team, you may also work directly with our Game Development teams, as they work to collect telemetry from specific games.

The ideal candidate will have a strong background in distributed data processing, have great and demonstrable data intuition, and share our passion for continuously improving Netflix Games.
In this role, you will

Engineer efficient, adaptable, and scalable data pipelines to process structured and unstructured data 
Develop a deep understanding of the emerging games domain at Netflix 
Partner with our game studio leads to evolve the top-level metrics for games retention etcPartner with Netflixs internal game studios Night School, Next Games, Boss Fight and Spry Fox to ingest relevant metrics and game data to the Netflix ecosystem
Partner with the Netflix SDK team and Netflix external game partners i.e Gameloft to bring data in via the Netflix Games SDK
Join a stunning team of data engineers with diverse skill sets, partnering closely with analytics engineering and data science counterparts

About you

Passionate about building intuitive data products 
Highly proficient in at least one of Java, Python or Scala with at least 2 years of softwaredata engineering experience
Comfortable with advanced SQL
Experienced in engineering data pipelines using big data technologies Hive, Presto, Spark, Flink on medium to large-scale data sets
Conceptually familiar with AWS cloud resources S3, EC2, RDS etc 
Excel at taking requirements and implementing scalable data models and pipelines 
Excited about demonstrating excellence, learning new technologies and growing your career


At Netflix, we carefully consider a wide range of compensation factors to determine your personal top of market. We rely on market indicators to determine compensation and consider your specific job family, background, skills, and experience to get it right. These considerations can cause your compensation to vary and will also be dependent on your location. 

The overall market range for roles in this area of Netflix is typically 150,000 - 750,000

This market range is based on total compensation vs. only base salary, which is in line with our compensation philosophy. Netflix is a unique culture and environment. Learn more here. 

"
https://startup.jobs/data-engineer-consumeraffairs-4524171,Engineer,Data Engineer,ConsumerAffairs ,"Austin, United States",Full-Time,"

ConsumerAffairs helps consumers make smart buying decisions in moments of need. Every month millions of consumers turn to our site and tools for help with their considered often emotional purchases. 
We educate them about their options, learn about their specific needs, and connect hundreds of thousands of them directly to brands. These brands use our SaaS tools to manage their reviews and communicate directly with consumers to serve them better. Our business thrives when the consumers who trust us get matched with the right brands for them.
Were fast-paced and our core values are the bedrock of who we are and who we want to be. 
Our employees believe in raising the bar through data-driven innovation, intellectual curiosity, and grit. We have a team-first mentality, and manifest wins by putting the team first. Collaboration and teamwork are in our hearts; we believe winning together is the most fun. But, above all else, we care. We have servant hearts for our consumers, customers, and colleagues. If you want to be part of a globally diverse team focussing on helping people, in an environment where we raise the bar, win as a team, and care above all elsethen ConsumerAffairs may be just the place for you!

ABOUT THE JOB
The Data Engineer is responsible for monitoring, expanding, and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems as well as building them from the ground up.
The Data Engineer supports our solution architects, data analysts, and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.
Candidates should have education andor experience in data modeling, requirements analysis, and best practices in analytics. Experience with BI applications is a definite plus.RESPONSIBILITIES  EXPECTATIONS 
These responsibilities are not to be construed as a complete statement of all duties performed. Employees will be required to perform other job-related duties as required
 Create and maintain optimal data pipeline architecture Collect and refine business requirements to create technical specifications Collaborate with other DA teams to integrate disparate data sources and systems Collaborate with other CA teams to identify and refine requirements Develop and implement comprehensive data ingestion processes Develop and implement comprehensive data integration processes Develop and implement data models to support operations and analytics Keep data separated and secure at rest and in transit Maintain and support cloud data architecture Maintain job processes managed in Airflow and DBT Maintain data integration processes via Airbyte, Stitch, Fivetran, and Talend Create, maintain, and update technical documentation Help troubleshoot, document, and maintain system processes Assemble large, complex data sets that meet business requirements Constantly look for ways to improve monitoring, uncover issues, and deliver value Meet professional obligations through efficient work habits such as meeting deadlines, honoring agreed schedules, coordinating resources in an effective and timely manner, and demonstrating respect for others 
Requirements
Qualifications  Credentials
EducationLicensureCertification
BScBA in Computer Science, Engineering or relevant fie. 
Experience
 5 years experience in Data Integration and Integration 5 years experience in Data Warehousing, notably Cloud Data Experience in SQL across relational and dimensional databases Experience in AWS and Snowflake implementation and administration Industry experience is beneficial 
Knowledge, skills and abilities
 Fluent in English Must be highly curious, driven, and disciplined Understanding of data models, data warehouses, and data mining Familiarity with BI technologies Knowledge of SQL is required Familiarity with cloud analytics is beneficial AWS, Snowflake Proven abilities to take initiative and be innovative Analytical mind with a problem-solving aptitude Stands up for decisions and takes responsibility for results Shares both good and bad outcomes transparently Obsessed with ensuring an exceptional customer experience- for both internal and external customers. Stands up for decisions, takes responsibility for results, and shares both good and bad outcomes transparently. Demonstrates a relentless focus on results with a commitment to deliver; Takes decisive action, and confidently changes course if unsuccessful. Displays a growth mindset to continually improve; encourages everyone around them to be tenacious and never settle. Constantly seeks feedback to improve; Focuses on solving issues through teamwork, and collaboration Acts with urgency; delivers top results in hours and days instead of weeks and months. Relentless in their pursuit of success and possessing the willpower to embrace challenges as opportunities  
Specific Measures of Success  Expected Outcomes
Start Date to Start Date 1 Year  Familiarity and proficiency with our Airflow and Snowflake installations and our Python codebase Full participation in our on-duty rotation that handles recurring tasks and customer requests during business hours Full understanding of our data ingestion process for hundreds of partners with the ability to onboard new partners, debug and fix failures, and suggest improvements to the system based on the experience of running and maintaining it Recommenddeploy improvements to overall team processes and educational opportunities 
Core Values
Raise The Bar 
We raise the bar through innovation, intellectual curiosity, and grit. We are not satisfied with yesterday and our hearts thirst to be better tomorrow. 
Win As A Team
We manifest wins by putting the team first. We have collaboration and teamwork in our hearts and believe winning together is the most fun.
Care Above All Else
We care above all else. We have servant hearts for our consumers, customers, and colleagues. 
Physical Requirements  Environmental Conditions
 Location Remote Tulsa Frequency of travel Occasional travel may be required for meetings, training andor conferences.  Light physical activities and efforts required in working within an office environment.  
Reasonable accommodations will be made in accordance with existing ADA requirements for otherwise qualified individuals with disabilities. ConsumerAffairs provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training. 
Benefits
 Health Care Plan Medical, Dental  Vision Retirement Plan 401k, IRA Life Insurance Basic, Voluntary  ADD Paid Time Off Vacation, Sick  Public Holidays Family Leave Maternity, Paternity Short Term  Long Term Disability Training  Development Work From Home Free Food  Snacks Stock Option Plan 

"
https://startup.jobs/data-engineer-snowflake-tiger-analytics-4523773,Engineer,Data Engineer - Snowflake,Tiger Analytics ,"Dallas, United States",Full-Time,"

Tiger Analytics is a fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning and AI. We are the trusted analytics partner for several Fortune 100 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent as we continue to build the best analytics global consulting team in the world.
The Data Engineer will be responsible for architecting, designing, and implementing advanced analytics capabilities. The right candidate will have broad skills in database design, be comfortable dealing with large and complex data sets, have experience building self-service dashboards, be comfortable using visualization tools, and be able to apply your skills to generate insights that help solve business challenges.We are looking for someone who can bring their vision to the table and implement positive change in taking the companys data analytics to the next level.
Requirements
8 years of overall industry experience specifically in data engineering5 years of experience building and deploying large-scale data processing pipelines in a production environment.Strong experience in building ETL data pipelines and analysis using Python, SQL, and PySparkCreating and optimizing complex data processing and data transformation pipelines using pythonKnowledge of programming languages in data engineering such as Python or PySparkKnowledge of big data platforms like Snowflake, DBT, AWS Redshift, Postgres, MongoDB, and HadoopExperience with Snowflake Cloud Datawarehouse and DBT toolExperience with data pipeline and workflow management toolsAdvanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databasesUnderstanding of Datawarehouse DWH systems, and migration from DWH to data lakesSnowflakeUnderstanding of ELT and ETL patterns and when to use each. Understanding of data models and transforming data into the modelsStrong analytic skills related to working with unstructured datasetsBuild processes supporting data transformation, data structures, metadata, dependency and workload managementExperience supporting and working with cross-functional teams in a dynamic environment
Benefits
Significant career development opportunities exist as the company grows. The position offers a unique opportunity to be part of a small, challenging, and entrepreneurial environment, with a high degree of individual responsibility.

"
https://startup.jobs/data-engineer-aws-tiger-analytics-4523772,Engineer,Data Engineer - AWS,Tiger Analytics ,"Dallas, United States",Full-Time,"

Tiger Analytics is a fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning and AI. We are the trusted analytics partner for multiple Fortune 500 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent as we continue to build the best global analytics consulting team in the world.
The Data Engineer will be responsible for architecting, designing, and implementing advanced analytics capabilities. The right candidate will have broad skills in database design, be comfortable dealing with large and complex data sets, have experience building self-service dashboards, be comfortable using visualization tools, and be able to apply your skills to generate insights that help solve business challenges. We are looking for someone who can bring their vision to the table and implement positive change in taking the companys data analytics to the next level.
Requirements
 Bachelors degree in Computer Science or similar field 5 years of experience in a Data Engineer role Strong experience with advanced SQL Experience with AWS cloud services EC2, EMR, Athena Experience with scripting languages Python, Java, Scala, etc. Experience extractingqueryingjoining large data sets at scale A desire to work in a collaborative, intellectually curious environment Strong communication and organizational skills 
Benefits
This position offers an excellent opportunity for significant career development in a fast-growing and challenging entrepreneurial environment with a high degree of individual responsibility.

"
https://startup.jobs/data-engineer-wichita-kansas-emprise-bank-4523039,Engineer,"Data Engineer - Wichita, Kansas",Emprise Bank ,"Wichita, United States",Full-Time,"

At Emprise Bank, everything we do is focused on empowering people to thrive. We proudly work to provide an extraordinary customer experience to help our customers achieve their goals.
We are currently seeking a Data Engineer to join our Innovation and Strategy team in Wichita, KS. This role plays an integral part in supporting Emprise Banks data strategy. A successful candidate will have
  Confident and articulate communications skills Initiative and strong work ethic Ability to effectively manage and prioritize tasks to meet project deadlines Problem resolution, critical and analytical thinking skills Strong attention to detail Demonstrative knowledge of IT architectural concepts, design, development, and deployment The ability to treat highly confidential information with utmost urgency An understanding of and commitment to our values Attitude and aptitude to engage in continuous development  Essential functions of the role
 Determine architectural approaches for data environments and help ensure that the data needs of the enterprise are being met Develop, test, and maintain data pipelines Develop and apply scalable data integration ETLELT processes including ingestion, cleansing and curation Maintain the banks data lake and data warehouse located in the Azure cloud environment Create documentation and procedures on all processes necessary for functionality of the data architecture Other duties as assigned within the scope of the role 


Requirements
 Bachelors degree or level of education that, together with industry experience, enables the applicant to meet the job requirements Familiarity with medallion data architecture preferred Proficiency with multiple report writing tools and data management tools is required Understanding of Medallion data architecture is a plus Experience with SQL server database Experience in Python, SQL and Pyspark language is preferred Proficiency with large server based applications and typical desktop software preferred 

This role is not eligible for VISA sponsorship.
Benefits
In addition to a competitive salary and benefits, Emprise offers professional growth, a rewarding and challenging environment, opportunities to be involved in our communities, and a culture of integrity, passion, and success. We also offer shift differential pay for bilingual candidates!
At Emprise Bank, empowering people to thrive means having an all-inclusive culture that honors our commitment to all dimensions of diversity in our workforce and embraces inclusion of all people. People of color, women, LGBTQIA, veterans, and persons with disabilities are encouraged to apply.
To learn more, please visit our website at www.emprisebank.com.
Emprise Bank is an EEOAAADAVeteran EmployerMember FDICDrug Free Workplace.

"
https://startup.jobs/data-engineer-cloudflare-4522237,Engineer,Data Engineer,Cloudflare ,"Lisbon, Portugal",,"


About Us

At Cloudflare, we have our eyes set on an ambitious goal to help build a better Internet. Today the company runs one of the worlds largest networks that powers approximately 25 million Internet properties, for customers ranging from individual bloggers to SMBs to Fortune 500 companies. Cloudflare protects and accelerates any Internet application online without adding hardware, installing software, or changing a line of code. Internet properties powered by Cloudflare all have web traffic routed through its intelligent global network, which gets smarter with every request. As a result, they see significant improvement in performance and a decrease in spam and other attacks. Cloudflare was named to Entrepreneur Magazines Top Company Cultures list and ranked among the Worlds Most Innovative Companies by Fast Company. 
We realize people do not fit into neat boxes. We are looking for curious and empathetic individuals who are committed to developing themselves and learning new skills, and we are ready to help you do that. We cannot complete our mission without building a diverse and inclusive team. We hire the best people based on an evaluation of their potential and support them throughout their time at Cloudflare. Come join us! 


About the team
The Business Intelligence team at Cloudflare is responsible for building a centralized cloud data lake and an analytics platform that enables our internal Business Partners and Product teams with actionable insights and also provides a 360 view of our business. Our goal is to democratize data, support Cloudflares critical business needs, provide reporting and analytics via self-service tools to fuel existing and new business critical initiatives.
About the role
We are looking for an experienced Data Engineer to join our Lisbon team to scale our data platform and product insights initiatives. You will work with a wide array of data sources to build integrated data pipelines that process billions of records each day and influence our critical business initiatives. 
Success in this role comes from marrying a strong data engineering background with product and business acumen to deliver scalable data pipelines and analytics solutions that can enable advanced analytics via a self-service user interface.
What youll do

Design, implement and support end to end scalable data pipelines for multiple data products 
Work closely with a cross functional team of data scientists and analysts and internal stakeholders on strategic initiatives 
Contribute in improving an evolving data platform for scalability, observability and reliability
Partner with product and other data engineering teams to extract, transform and load data from various sources using big data technologies
Build rich data sets that drive innovation in data driven insights at scale within the company
Understand data landscape i.e tooling, tech stack, source systems etc. and work closely with the data engineering team in Austin and San Francisco to improve the data collection and quality

Examples of desirable skills, knowledge and experience

B.S. or M.S in Computer Science, Statistics, Mathematics, or other quantitative fields
Minimum 3 years of industry experience in software engineering, data engineering, data science or related field with a track record of extracting, transforming and loading large datasets 
Hands on programming experience with Python, Go or any JVM based programming language
Strong command in writing advanced SQL queries
Knowledge of data management fundamentals and data storagecomputing principles
Solid understanding of big data technologies such as Spark, BigQuery, Kafka etc. 
Strong communication skills

Bonus Points

Familiarity with container based deployments such as Docker  Kubernetes
Familiarity with Google Cloud Platform or something similar
Experience in building RESTful and microservices applications is a plus


What Makes Cloudflare Special?
Were not just a highly ambitious, large-scale technology company. Were a highly ambitious, large-scale technology company with a soul. Fundamental to our mission to help build a better Internet is protecting the free and open Internet.
Project Galileo We equip politically and artistically important organizations and journalists with powerful tools to defend themselves against attacks that would otherwise censor their work, technology already used by Cloudflares enterprise customers--at no cost.
Athenian Project We created Athenian Project to ensure that state and local governments have the highest level of protection and reliability for free, so that their constituents have access to election information and voter registration.
Path Forward Partnership Since 2016, we have partnered with Path Forward, a nonprofit organization, to create 16-week positions for mid-career professionals who want to get back to the workplace after taking time off to care for a child, parent, or loved one.
1.1.1.1 We released 1.1.1.1 to help fix the foundation of the Internet by building a faster, more secure and privacy-centric public DNS resolver. This is available publicly for everyone to use - it is the first consumer-focused service Cloudflare has ever released. Heres the deal - we dont store client IP addresses never, ever. We will continue to abide by our privacy commitment and ensure that no user data is sold to advertisers or used to target consumers.
Sound like something youd like to be a part of? Wed love to hear from you!
This position may require access to information protected under U.S. export control laws, including the U.S. Export Administration Regulations. Please note that any offer of employment may be conditioned on your authorization to receive software or technology controlled under these U.S. export laws without sponsorship for an export license.
Cloudflare is proud to be an equal opportunity employer.  We are committed to providing equal employment opportunity for all people and place great value in both diversity and inclusiveness.  All qualified applicants will be considered for employment without regard to their, or any other persons, perceived or actual race, color, religion, sex, gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship, age, physical or mental disability, medical condition, family care status, or any other basis protected by law. We are an AAVeteransDisabled Employer.
Cloudflare provides reasonable accommodations to qualified individuals with disabilities.  Please tell us if you require a reasonable accommodation to apply for a job. Examples of reasonable accommodations include, but are not limited to, changing the application process, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment.  If you require a reasonable accommodation to apply for a job, please contact us via e-mail at hrcloudflare.com or via mail at 101 Townsend St. San Francisco, CA 94107.


"
https://startup.jobs/data-engineer-babylist-2-4521324,Engineer,Data Engineer,Babylist ,,,"

Who We Are
Babylist is the leading vertical marketplace and commerce destination for baby, driving purchase decisions for more than 8 million people each year. Utilizing robust proprietary data, patented technology, and unbiased editorial guidance, Babylist recommends expert-tested products to those starting their parenting journey so loved ones can offer their support. Babylist registries connect new parents and their community of family and friends who help plan, prepare, and shop for a childs arrival. Babylist is the generational brand in baby, leading the 67 billion baby products market as the trusted go-to solution for growing families. To learn about Babylists registry options, editorial content, and more, visit www.babylist.com.
Our Ways of Working
We have team members located across the United States spanning multiple time zones. This means we put in extra effort to make sure we connect and collaborate in ways that make sense for us. We know how valuable the flexibility of remote work is for our employees. 
We know that personal connection is the foundation for the great work we do together. In order to build those relationships with team members in other cities, we meet with coworkers in person two times a year at a full company offsite and a departmental offsite. These offsites are expected of employees and are great opportunities to meet the people you work with every day and to do some of the work that is much more difficult to do virtually.
Our Tech Stack

Snowflake
Airflow
DBT
AWS

What the Role Is
The Data Team at Babylist is looking for a Data Engineer to build and scale our core data platform. This work and the role is foundational for our team and strategic for our business. This person is a dedicated professional whose interests lie on the cusp of data engineering, data architecture and DevOps. You will work cross-functionally with our product managers, analytics engineers, data scientists, and business functions. As a Data Engineer, you will be joining a growing data engineering team at Babylist and will directly shape the data vision for the company through your work. You will hone your craft as a data professional while working on our state-of-the-art data stack. You will be responsible for building and scaling data tools for our analysts and data scientists that enable product growth at Babylist.
Who You Are

You have 3 years hands-on experience writing and deploying production grade code
You have solid experience with a language like Python and familiarity with data orchestration tools like Airflow, dbt, etc
You have a strong understanding of data modelingETL principles and modern data warehousing systems like Snowflake
You are a DevOps in disguise Comfortable with the AWS ecosystem including managing and deploying cloud data resources EC2, S3, Lambda, EKS, Sagemaker

How You Will Make An Impact

You will build custom and third-party data pipelines for data ingestion into the data warehouse
You will optimize our data platform as we continue to grow
You will improve data monitoring coverage for complex data flows between multiple systems
You will operationalize ML Models and build internal APIs to surface our machine learning products
You will ensure data quality, monitoring and SLA

Why You Will Love Working At Babylist

We invest in the infrastructure youll need to be supported and successful tools, opportunities to connect with colleagues, and a stipend to help you set up your office
We build products that have a positive impact on millions of peoples lives
We work at a sustainable pace which means worklife balance is a real thing here
We believe technology and data can solve hard problems 
We believe in exceptional management 
We are an antiracist organization and doing the work to support differences of all kinds
We offer competitive pay and meaningful opportunities for career advancement
We have great benefits like company paid medical, dental, and vision, a generous paid parental leave policy, and 401k with company match
We care about employee wellbeing with perks for physical, mental and emotional health, parenting, childcare, and financial planning

Babylist takes a market-based approach to pay, and pay may vary depending on your location. Your actual base salary will depend on factors such as your skills, qualifications, experience, and work location. 
The estimated pay range for this role is 
125,000 - 179,000 USD126,000 - 183,000 CAD
In addition, Babylist offers equity, bonus, and benefits, including company paid health, dental and vision insurance, 401k matching, flexible spending account, and paid leave including PTO and parental leave in accordance with our applicable plans and policies.
If your experience is close to what were looking for, please consider applying. Experience comes in many forms  skills are transferable, and passion goes a long way. We know that diversity makes for the best problem-solving and creative thinking, which is why were dedicated to adding new perspectives to the team and encourage everyone to apply.
 
 
bi-remote

"
https://startup.jobs/data-engineer-kyivstar-4519279,Engineer,Data engineer,Kyivstar ,"Kyiv, Ukraine",Full-Time,"

Kyivstar Big Data team is looking for talented Data Engineer


Necessary skills
          2 years of experience in software development
          Strong knowledge of CICD principles
          Strong knowledge of SQL
          Experience with Docker and Kubernetes
          Experience in process design and developing ETL processes for ML pipelines
          Experience with programming in Python or Scala for Apache Spark
          Experience in test-driven development and integration testing
Would be a plus
          Experience with Spark ML and Python ML libraries
          Experience with Apache NiFi, Airflow
          Familiarity with Hortonworks or Cloudera distributions
          Experience with Cloud Technologies
          Experience with performance testing and tuning
Responsibilities
          Develop ETL processes for ML pipelines
          Creation of automated tooling and models deployment
          Automate processes of model quality monitoring
          Develop automatic model refresh procedures to avoid staleness
          Optimize pipelines and review PythonSpark code
Proposed
          Working with huge amounts of data at the largest operator in Ukraine
          Hands on experience with Big Data technologies and tools
          Great diversity of tasks and projects to utilize your knowledge and skills; flexibility in tools and approaches
          Opportunity to grow within the team of talented data engineers
Project description
As part of the digital transformation strategy, Kyivstar invests resources in the development of a data management platform DMP based on Big Data technologies, the Hadoop ecosystem, and cloud computing platforms.
One of the directions of this development is the development and implementation of products using Big Data and machine learning technologies. To achieve this, company is looking for Data Engineers - specialists in the field of developing software solutions, with practical skills in working with Open Source tools of the Hadoop ecosystem, understanding of ETL processes and specialties of implementing data pipelines, which include machine learning.

  
   -- httpsmedium.comkyivstar-careers

      LinkedIn -- httpswww.linkedin.comcompanykyivstar



    ,     ,  c.           ! 

  ,      VEON,        ,  ,       VEON,    .      ,      ,   .               .     ,  ,   ,        .

 
 
More about us
Discover moments of awesomeness and our careers blog -- httpsmedium.comkyivstar-careers

For the latest updates follow us on LinkedIn -- httpswww.linkedin.comcompanykyivstar

 
Working in Kyivstar VEON Group demands a high standard of business ethics, adherence to our legal obligations, our values and our Code of Conduct of VEON Group and supporting compliance policies and procedures. We strive to be truthful and transparent, which requires us to act ethically, honestly, and with integrity. We understand the importance of the protection of your Personal Data and are committed to using good practices in how we handle such Data. By clicking on ""Apply for this job"", you confirm that you have read, understood and explicitly agree to our Applicants Privacy Policy.

"
https://startup.jobs/data-engineer-adyen-4518752,Engineer,Data Engineer,Adyen ,"Madrid, Spain",,"

This is Adyen
Adyen provides payments, data, and financial products in a single solution for customers like Meta, Uber, HM, and Microsoft - making us the financial technology platform of choice. At Adyen, everything we do is engineered for ambition. 
For our teams, we create an environment with opportunities for our people to succeed, backed by the culture and support to ensure they are enabled to truly own their careers. We are motivated individuals who tackle unique technical challenges at scale and solve them as a team. Together, we deliver innovative and ethical solutions that help businesses achieve their ambitions faster.
Data Engineer
At Adyen, we treat data and data artifacts as first-class citizens. They form our backbone and drive the business and product insights we generate. To this end, Adyen is looking for a Data Engineer to join our development team in Madrid, a person that understands the business context and the data needs behind it, and knows how to implement quality data pipelines on our Big Data Platform. Your  pipelines will fuel stunning visuals, reports, and raw or refined datasets for internal or external use. You will be responsible for creating performant and high-quality ETLELTs and ensuring users have access to correct, accurate and performant data and insights. In this role, you will

Work with product teams, product specialists and data analysts, analyzing their data and transcribing their data needs that render into data pipelines.
Design, develop, deploy and operate production ETLELT pipelines in PySpark.
Build data products and tooling according to quality principles such as performance, code quality, data validation, data governance and discoverability.
Establish and spread data analytics best practices across the organization.
Organize data analytics related training sessions.
Develop linters and other tooling bespoke to Adyen to help ensure a high-quality pipeline codebase

Who you are

You have experience building ETLsELTs, ideally in PySpark and Airflow.
You have interest in data engineering practices and product knowledge, specifically in how to leverage the power of analytics into business.
You have a good understanding of Software Engineering practices and Data Engineering principles.
You are able to communicate complex outcomes with clarity over a wide range of audiences.
Technology Python and SQL are a must.
Mentality An experimental mindset with a launch fast and iterate mentality. A strong statisticsmathematics background is a plus

Our Diversity, Equity and Inclusion commitments 
Our unique approach is a product of our diverse perspectives. This diversity of backgrounds and cultures is essential in helping us maintain our momentum. Our business and technical challenges are unique, and we need as many different voices as possible to join us in solving them - voices like yours. No matter who you are or where youre from, we welcome you to be your true self at Adyen. 
Studies show that women and members of underrepresented communities apply for jobs only if they meet 100 of the qualifications. Does this sound like you? If so, Adyen encourages you to reconsider and apply. We look forward to your application!
Whats next?
Ensuring a smooth and enjoyable candidate experience is critical for us. We aim to get back to you regarding your application within 5 business days. Our interview process tends to take about 4 weeks to complete, but may fluctuate depending on the role. Learn more about our hiring process here. Dont be afraid to let us know if you need more flexibility.

"
https://startup.jobs/data-engineer-healthverity-4517688,Engineer,Data Engineer,HealthVerity ,"Philadelphia, United States",Full-Time,"

How you will help
You will support the engineering teams data endeavors, diving in to fix issues, optimize processes, and automate what you do more than once.  Youll use the best tools for the job, whether modern and revolutionary or time tested and proven, to deliver elegant, scalable solutions that meet business and technical needs.

What you will do
 Work with internal stakeholders to load data into HealthVeritys data warehouse
 Troubleshoot and resolve issues relating to data integrity
 Help establish procedures and best practices for transforming and storing data
 Lead requirements gathering around data pipeline automation improvements
 Work with some of the most exciting open-source tools like Spark, Hadoop, Docker, Airflow, Zeppelin
 Leverage distributed computing and serverless architecture such as AWS EMR  AWS Lambda, to develop pipelines for transforming data
 Enjoy the peace that comes with working in a mature software development environment 
 Marvel at the speed with which your creation makes it into production
 Research and implement new technologies with a team of developers to execute strategies and implement solutions
 Produce peer reviewed quality software
 Solve complex problems related to the real-time discovery of large data

How success will be defined
 Consistent operations maintained by following policies and procedures; reporting needed changes 
 Create and design clear documentation and reference designs for pipelines for a repeatable process
 Resolve 90 production issues within SLAs
 Performance tune PySpark code for pipelines to reduce load times by 10
 Become a data subject matter expert in one asset within 12 months

Required skills and experience
 1 years of healthcare data medical claims, pharmacy claims, lab testsresults
 3 years of experience with Python and Scala
 3 years of experience with PySpark and Spark-SQL writing, testing, debugging spark routines
 1 years of experience with AWS EMR, AWS S3 service

Desired skills and experience
 Comfortable using AWS CLI and boto3
 Comfortable working in remote environments
 Comfortable using nix command line shell scripting, AWK, SED
 Experience with MySQL and Postgres
 Experience with Apache Airflow
 Experience with Databricks 
 Experience with data modeling to they can collaborate with Data Architects and Data Analysts

Base salary for the role is commensurate with experience and can range between 63,000 - 250,000  annual bonus opportunity.

Hiring Locations
While HealthVerity does support remote work with quarterly travel to our Philadelphia headquarters, our strong preference is to hire team members in the areas below as well as approved states in the Eastern Time Zone. Expansion beyond these markets will occur only when necessary.


 Boston, Massachusetts


 New York City, New York


 Philadelphia, Pennsylvania


 Baltimore, Maryland


 Washington D.C


 Charlotte, North Carolina


 Raleigh-Durham, North Caroline

 Atlanta, Georgia


Approved States in the Eastern Time Zone include CT, DE, FL, GA, IN, MA, MD, MI NC, NJ, NY, OH, PA, RI, TN, and VA.

About HealthVerity

HealthVerity synchronizes transformational technologies with the nations largest healthcare and consumer data ecosystem to power previously unattainable outcomes and fundamentally advance the science. We offer a comprehensive, yet flexible approach, based on the foundational elements of Identity, Privacy, Governance and Exchange IPGE, that synchronizes unparalleled Identity management with built-in Privacy compliance and Governance, providing the ability to discover and Exchange a near limitless combination of data at a record pace. Together with our partners in life sciences, government and insurance, we are Synchronizing the Science. To learn more about HealthVerity, visit healthverity.com.


Why youll love working here


We are making a difference  Our technology is at the forefront of some of the biggest healthcare challenges in the world. 

We are one team  Our people define our culture and always will. We take time out to celebrate each other at the end of every week through company-wide shout outs, and acknowledge the value that each of us adds towards our greater mission. Come share all you have to offer.

We are learners  Every team member is continually learning, no matter if weve been in a role for one year or much longer. We are committed to learning and implementing what is best for our clients, partners, and each other.

Benefits  Perks
 Compensation competitive base salary  annual bonus opportunity for non-commissioned roles
 Benefits comprehensive benefits with coverage on Day 1, medical, dental, vision, 401k, stock options
 Flexible location our HQ is in Philadelphia. We offer both hybrid roles and those with quarterly travel. 
 Generous PTO Take time off as needed, targeted at 4 weeks per year, including vacation, personal and sick time, plus paid maternity and paternity leave.
 Comprehensive and individualized onboarding mentorship program, departmental talks, and a library of resources are available beginning day 1 for each new team member to minimize the stress of starting a new job
 Professional development biweekly 11s, hands-on leadership that is goal-and growth-oriented for each team member, and an annual budget to support professional development pursuits


HealthVerity is an equal opportunity employer devoted to inclusion in the workplace. We believe incorporating different ideas, perspectives and backgrounds make us stronger and encourages an environment where ageism, racism, sexism, ableism, homophobia, transphobia or any other form of discrimination are not tolerated. All qualified job applicants will be given consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or on the basis of disability. At HealthVerity, were working towards an innovative and connected future for healthcare data and believe the future is better together. We can only do that if everyone has a seat at the table. Read our Equity Inclusion and Diversity Statement.

If you require a reasonable accommodation in completing this application, interviewing, completing any pre-employment testing, or otherwise participating in the employee selection process, please direct your inquiries to careershealthverity.com


Remote opportunities are not available in all areas and require team members to work from a fixed location due to tax and labor law implications - specific questions about remote positions can be discussed during the interview process with your recruiter. 

"
https://startup.jobs/data-engineer-fortune-4517660,Engineer,Data Engineer,FORTUNE ,"New York, United States",,"

Fortune MediaFortune MediaData Engineer, Data Engineering Job descriptionWe are looking for a Data Engineer to join our growing, versatile Data Engineering team. This person will be a key member of analytical projects and will contribute to data discovery, pipeline implementation, and analytics modeling. This role requires a person with hands on technical experience who is comfortable with delivering scalable solutions that meet and exceed! business needs and is comfortable working withvarious tools and methodologies.Responsibilities Work closely with Data Engineering Lead to scope, document, and ingest data from structured and unstructured sources into Fortunes data warehouse. Collaborate with BI  Analytics stakeholders to fully understand business and technical requirements, and be able to communicate a clear path of work.Design, develop, and deploy data models to meet BI  Analytics use cases.Explore and structure web analytics data capture with our Product Analytics lead.Identify opportunities for optimization with our existing pipelines and data warehouse automations.Document and provide support for existing implemented solutions.Collaborate across the Data Engineering team to add value where necessary
Fortune Media
Data Engineer
 Job description
We are looking for a Data Engineer to join our growing, versatile Data Engineering team. This person will be a key member of analytical projects and will contribute to data discovery, pipeline implementation, and analytics modeling. This role requires a person with hands on technical experience who is comfortable with delivering scalable solutions that meet and exceed! business needs and is comfortable working with various tools and methodologies.
 Responsibilities 

Work closely with Data Engineering Lead to scope, document, and ingest data from structured and unstructured sources into Fortunes data warehouse.
Collaborate with BI  Analytics stakeholders to fully understand business and technical requirements, and be able to communicate a clear path of work.
Design, develop, and deploy data models to meet BI  Analytics use cases.
Explore and structure web analytics data capture with our Product Analytics lead.
Identify opportunities for optimization with our existing pipelines and data warehouse automations.
Document and provide support for existing implemented solutions.
Collaborate across the Data Engineering team to add value where necessary

Requirements
2 years relevant professional experience in a technical, data centric role analyst, modeler, engineer. Preferably in digital media or publishing.
Technical Skills

Experience with web analytics platforms ex. Google Analytics, Amplitude  Piano Analytics is a plus.
Deep knowledge of SQL fundamentals  PostgreSQL and PLPGSQL are a plus.
Proficiency in Python and ETL  data processing dependent libraries think psycopg2, requests, pyspark  dask  pandas etc..
Experience in Data Modeling  taking raw data and surfacing as a usable data object by adding structure and implementing business logic.
Experience with structuring ETL  ELTs andor using ETL tools ex. AWS Glue, DBT
Experience with at least one of the major cloud platforms ex. AWS, GCP  we operate on an AWS stack
Familiarity with OLTP databases
Familiarity with Jira or other agile project management platforms
Bonus  familiarity with NOSQL databases
Bonus  familiarity with Shell  Bash scripting
Bonus  familiarity with Google Tag Manager.

Soft Skills

Experience in digesting business requirements and translating those needs into technical implementation.
Communicating architecture and implemented solutions in understandable terms to non-technical partners.
Agency  owning and delivering on your responsibilities.
Coachability and willingness to learn.
Team player and champion for collaboration.
Transparent in communicating work.

Location NYC hybrid. Must be commutable to our office in Lower Manhattan
A few of Fortunes perks and benefits  


22 vacation days, 11 paid holidays and an honor-based sick leave policy 


Health, dental, and vision coverage 90 paid for individuals and families, plus a high-deductible health plan option where Fortune contributes to a Health Savings Account HSA 

401k plan
Generous parental leave 
Dependent carehealth carecommuter FSAs, and cell phone benefits
Tuition reimbursement program
A commitment to an open, inclusive and diverse work culture 


For Residents of California  Our California Resident Applicant and Prospective Applicant Privacy Notice is located at this link  httpsboards.greenhouse.iofortunejobs4766493004.
Salary 90,000 - 110,000
 
 
Familiarity with Jira or other agile project management platforms.

"
https://startup.jobs/data-engineer-milk-moovement-4517303,Engineer,Data Engineer,Milk Moovement ,"Halifax Regional Municipality, Canada",Full-Time,"

ABOUT THE COMPANY
Milk Moovement is building a world-class team focused on getting the right milk to the right place at the right time. 

Our herd of over 65 employees and growing is driven to provide our clients with the data they need to make critical decisions that impact their operations and ultimately your favorite dairy products. 

Who is Milk Moovement you might ask? We are a young VC-backed company with humble roots and massive ambitions to disrupt the dairy supply chain. We think differently, act nimbly, and always leave things better than we found them.  

Were adding to our team as we continue significant growth in 2023. Check us out on Twitter, Instagram, LinkedIn milkmoovement, and our home page to learn more or hit apply below!

THE ROLE 

Were hiring a Data Engineer to join our team! As an Engineer working on our Platform value stream you will help develop, implement and maintain systems and processes that will transform raw data so that end users and stakeholders can easily work with it. Here are some examples of things you might do  
 Develop and maintain new and existing ETL pipelines
 Continuously improve overall performance of all data infrastructure and technology stack
 Engage in design sessions to build new data products for the platform
 Assist with best practices for development and deployment

You will be part of a cross-functional team distributed across North America but with offices in Halifax and St. Johns. You should be comfortable coming to our offices if nearby, and not shy on Zoom and Slack if remote. 

WHAT WE ARE LOOKING FOR

Milk Moovement seeks to have a diverse, inclusive, team-oriented, and curiosity-driven herd. Our technical team lives to find unique solutions to the challenges inherent to digital supply chains, and we expect you will be excited to do so as well. You must have prior experience with data engineering, but experience working in the dairy industry is not required! We will teach you all there is to know about the industry beginning with our Dairy 101 course. It is definitely more complicated than you think and that is why we do what we do! You will likely have been working with big data to help businesses solve real-world problems. You have an aptitude for engineering, big data architecture, and experience implementing various data engineering patterns including but not limited to Warehousing, Modeling, Analytics and ingestion from multiple data sources. You like getting your hands dirty with data, and are passionate about turning raw data into something useful for business. You are proficient in SQL and Python, and are eager to be part of an area of growth for Milk Moovement.

WHAT ELSE

 A good understanding of Data Warehousing concepts and cloud ETLELT design patterns real time, batch, event-driven workflows
 Comfortable in designing efficient and robust ETL workflows
 Monitoring  e.g Datadog, AWS CloudWatch,
 Visualization tools Metabase, Tableau, or similar
 Experience building with AWS, especially using data engineering services such as Athena, Kinesis, DMS
 Experience working with Cloud Data Warehousing and Orchestration Tools such as Snowflake, DBT, Dagster, Airflow
 Experience with Terraform along with other AWS specific IaC Tools CloudFormation or CDK 
 Experience standing up CICD pipelines for data infrastructure
 Knowledge of API technologies REST, Authentication and AuthorizationA Team Player  collaborative, approachable,  Ability to prioritize and work in a dynamic workplace
 Experience using container-based services Docker, ECS, Kubernetes is preferred but not required
 Real time streaming experience e.g Kafka is preferred but not required

WHAT WE OFFER
 Competitive salaries - were constantly reevaluating market trends to ensure we meet or exceed industry standards.
 Equity - Stock option plan on a standard 4 year vesting schedule with a 1 year cliff.
 Unlimited paid vacation and flex time - unlimited vacation can be vague and difficult to track; we strongly encourage everyone to take at least 2 weeks off per year plus public holidays. The rest is up to you.
 Health mental  physical, dental,  HSA coverage across North America.
 Remote work environment - work from home or from one of our hubs in Halifax and St. Johns.
 Flexible hours - night owl or early riser? No problem.
 Tools - need the latest and great software to perform more efficiently? Ask and you shall receive.
 Quarterly culture events - trivia, robot building, hackathons, etc. We like to keep it fresh and exciting. 

ABOUT OUR CULTURE
 Well drop everything to ensure our customers feel supported.
 Transparency is ingrained in everything we do.
 Respect is paramount.
 We win and lose as a herd - lessons learned are equally as important as the wins.
 Were all in this together - our company wide thirst for knowledge is unquenchable.
 Want to learn a bit more about what makes us moo-nique? Check out our About Us page for company mission, purpose, and values.
 Did we mention we love puns?!


HOW TO APPLY 
Send your resume via our Careers page and dont forget to fill out our Get To Know The Candidate form; we love hearing what your favourite dairy products are! 

We conduct remote interviews always. This role is flexible given the candidates location and work preference and well work closely with you as we understand every person has unique circumstances.

Dont meet every single requirement? Studies have shown that women and people of colour are less likely to apply to jobs unless they meet almost every qualification. At Milk Moovement were committed to continually improving our approach to building a diverse, inclusive, and value driven workplace. If youre excited about this role but your past experiences dont align perfectly with our job description, we encourage you to apply regardless. You could just be the right candidate for this role or others!

"
https://startup.jobs/data-engineer-concurrency-4516534,Engineer,Data Engineer,Concurrency ,"Brookfield, United States",Full-Time,"


Who We Are 
We are change agents. We are inspired technologists. We are unlike any other technology consulting firm. Our team fearlessly challenges the status quo, relentlessly pursues whats next and pushes the limits of whats possible. A Microsoft Gold Partner and multiple time Partner of the Year award recipient, Concurrency is renowned for its ability to turn unmatched technology expertise into client outcomes. Have we inspired the technologist in you? Come be a change agent at Concurrency. 
 

Who Were Looking For 
Were excited to add a Data Engineer to our Data  AI team. In this role, youll work with a team of customer-focused professionals who are committed to defining technical strategy, architecting, designing, and delivering end-to-end digital transformation. youll demonstrate strong technical competence and business acumen through engaging in senior-level technology decision-making discussions related to agility, business value, data warehousing, and cloud-oriented data solutions. Youll empower other consultants by sharing subject matter expertise in large enterprise implementations, as well as overseeing the delivery of large, complex, and strategic projects for enterprise customers. 
Position Responsibilities

Data Engineers for various and unanticipated worksites throughout the U.S. HQ Brookfield, WI. 
Lead requirements and design sessions with customers and internal teams. 
Author functional requirements and technical design documentation. 
Build, automate, and modify ADF pipelines. 
Create or modify ELTETL procedures and scripts in T-SQL. 
Create or modify Python, Scala, and SQL programs. 
Develop Power BI Tabular Models, Reports and Dashboards. 
Work with the solution team to help set standard architectures, processes, and best practices. 
Technical Environment Data Analysis, Data Migration, Data Mining, Machine Learning, Data Modeling, ETL, Power BI, MS Azure ML, Azure SQL Database, SQL Server, R Studio, Python NumPy, Pandas.

Position Requirements

Bachelors degree in Computer Science, Management Information Systems, or a related field plus 3 years of experience in the job offered or in data analytics required. 
Required skills Data Analysis, Data Migration, Data Mining, Machine Learning, Data Modeling, ETL, Power BI, MS Azure ML, Azure SQL Database, SQL Server, R Studio, Python NumPy, Pandas. 100 telecommuting permitted.


Concurrency takes pride in bringing a different mindset to consultingthat takes a diversity of thought, collaboration and resilience. We are an innovation-obsessed yet a fun and progressive place to work. We offer flexible work schedules, competitive compensation, and great benefits for our people and their families.
In addition, all employees are eligible for several rewards and recognition programs, excellent training programs, and bonus opportunities to encourage our people to be the best versions of themselves in and out of work.

"
https://startup.jobs/data-engineer-i-r-14521-dun-bradstreet-4516027,Engineer,Data Engineer I (R-14521),Dun & Bradstreet ,"Jacksonville, United States",Full-Time,"

Why We Work at Dun  Bradstreet
Dun  Bradstreet unlocks the power of data through analytics, creating a better tomorrow. Each day, we are finding new ways to strengthen our award-winning culture and accelerate creativity, innovation and growth. Our 6,000 global team members are passionate about what we do. We are dedicated to helping clients turn uncertainty into confidence, risk into opportunity and potential into prosperity. Bold and diverse thinkers are always welcome. Come join us!

The Data Engineer I will work with the senior data engineers to build and maintain applications that are responsible for the ingestion of data into the Dun and Bradstreet Contact Pipeline.

The Data Engineer I will work on analyzing performancethroughput blockers of the applications and will work with other team members to remove the bottlenecks in the applications.

The Data Engineer I will also work with creating metrics from the ingestion applications to help determine the areas that need work in terms of performance andor throughput.
Responsibilities

Create and maintain applications in python.
Take ownership of existing applications for further developmentimprovements
Work closely with related groups to ensure business continuity
Perform analysis on code bases to increase performance.
Work as part of the team to code review and test other members code changes.
Work as a member of one or more agile teams, using lean principles and SCRUM methodology


Requirements

Bachelors degree preferable in computer science, mathematics, data science, or a related field
Experience with Python for application development 2-5 years
Experience with SQL for data analysis and querying 2-5 years
Ability to work independently to deliver critical projects on time
Ability to work closely with others to problem solve
Experience with hosted environments, AWS, Azure, or other cloud service providers preferred


Benefits We Offer

 Generous paid time off in your first year, increasing with tenure.

 Up to 16 weeks 100 paid parental leave after one year of employment.

 Paid sick time to care for yourself or family members. 

 Education assistance and extensive training resources.

 Do Good Program Paid volunteer days  donation matching.  

 Competitive 401k  Employee Stock Purchase Plan with company matching. 

 Health  wellness benefits, including discounted Gympass membership rates.

 Medical, dental  vision insurance for you, spousepartner  dependents.

 Learn more about our benefits httpbit.ly41Yyc3d.


All Dun  Bradstreet job postings can be found at httpswww.dnb.comabout-uscareers-and-peoplejoblistings.html. Official communication from Dun  Bradstreet will come from an email address ending in dnb.com.


Equal Employment Opportunity EEO Dun  Bradstreet is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex, age, national origin, citizenship status, disability status, sexual orientation, gender identity or expression, pregnancy, genetic information, protected military and veteran status, ancestry, marital status, medical condition cancer and genetic characteristics or any other characteristic protected by law.  View the EEO is the Law poster here and its supplement here. View the pay transparency policy here. 


Global Recruitment Privacy Notice

"
https://startup.jobs/data-engineer-ii-globalization-partners-4513369,Engineer,Data Engineer II,Globalization Partners ,,,"

At G-P, our mission is to break down barriers to global business, enabling opportunities for everyone, everywhere. With remote-first and diverse teams all around the world, our people are key to achieving this mission. Thats why we trust our Dream Team members with the flexibility and autonomy to do their best and most innovative work, encourage and support their personal growth and career development, and believe in recognition for a job well done. 
Our industry-leading SaaS-based Global Employment Platform enables our customers to expand and grow into 180 countries, creating more opportunities for global success  without requiring entity or subsidiary setup. The technical opportunities youll experience here have a positive impact on people and their worklife possibilities around the world. Beyond the power of our platform, we never forget that behind every hire is a human being. And that brings us to you.
If you have a passion for automation, are a deep innovator, and want to solve complex problems that lead to a world of positive results, consider G-P. Here, your knowledge and experience will be crucial to helping design and develop high-performing cloud-based software products using traditional Agile methodologies and modern frameworks.
Beyond a competitive compensation and benefits package, what we offer to all employees along the way is the clear and simple promise of Opportunity Made Possible. Come expand your skills in new ways and experience the thrill of your best innovations becoming reality
Data Engineer II
India  Work Remotely within India
The Team With our in-house engineering team spanning across US, Europe and India we are building the worlds top-rated global Employment Platform. We are strong advocates of DevOps culture where product managers, engineers, QA, IT operations and InfoSec work together towards a common goal, enabling the fast flow of planned work into production, while achieving world-class stability, reliability, availability and security.
What you will do

Design and develop our best-in-class cloud platform, working on all parts of the code stack from front-end, REST and asynchronous APIs, back-end application logic, SQLNoSQL databases and integrations with external systems
Develop solutions across the data and analytics stack
Design and develop reusable libraries
Write unit and integration tests

What were looking for

At least 5 years experience in developing data and analytics applications in geographically distributed teams
Hands-on experience in using modern architectures and frameworks, structured and unstructured data, and programming with Python
Hands-on SQL knowledge and experience with relational databases such as MySQL, PostgreSQL, and others
Hands-on ETL knowledge and experience
Knowledge of commercial data platforms Databricks, Snowflake or cloud data warehouses Redshift, BigQuery
Knowledge of data catalog and MDM tooling Atlan, Alation, Informatica, Collibra
Knowledge of how machine learning  A.I. workloads are implemented in batch and streaming, including the preparing of datasets, training models, and using pre-trained models
Excellent analytical and troubleshooting skills
Excellent communication skills
Excellent English both verbal and written
B.S. in Computer Science or equivalent


About Us
G-P helps growing companies unlock their full potential by making it possible to build highly skilled global teams in days instead of months. Through our SaaS-based platform, we help find, hire, onboard, pay, and manage team members, quickly and compliantly, to expand growth opportunities for everyone, everywhere  without the hassle of setting up local subsidiaries or branch offices. 
G-P. Global Made Possible.
G-P is a proud Equal Opportunity Employer, and we are committed to building and maintaining a diverse, equitable and inclusive culture that celebrates authenticity. We prohibit discrimination and harassment against employees or applicants on the basis of race, color, creed, religion, national origin, ancestry, citizenship status, age, sex or gender including pregnancy, childbirth, and pregnancy-related conditions, gender identity or expression including transgender status, sexual orientation, marital status, military service and veteran status, physical or mental disability, genetic information, or any other legally protected status.
G-P also is committed to providing reasonable accommodations to individuals with disabilities. If you need an accommodation due to a disability during the interview process, please contact us at careersg-p.com.


"
https://startup.jobs/data-engineer-join-our-team-3-4512437,Engineer,Data Engineer,Join our team ,"Kyiv, Ukraine",Full-Time,"

United Tech builds mobile and web products for the social networking domain, serving a vast user base in the US, UK, Canada, as well as leading European and Middle East countries.


We are looking for a Data Engineer to become a member of our team. You will have the opportunity to unlock your potential by creating, tuning, and maintaining data pipelines, while driving innovation with new feature implementation on a new Data Platform.

What youll help us do

Analyze and improve the performance, maintainability, and reliability of the Data Platform, related environments, and tools
Develop, maintain, and improve the existing Clickhouse based Data Warehouse solution and corresponding infrastructure
Develop a new GCP stack based Data Lakehouse solution for multiple products data and perform the migration from the existing Data Platform to the new one
Continuously learn and evaluate new approaches to perfect our data layer and adapt to the ever-growing scale and business needs
Collaborate with Product Managers, BusinessProduct analysts, Software and Data Engineers to deliver value to our products

Its all about you

5 years of total experience in RD as a Software  Data Engineer
23 years of experience in Data Engineering
Proficiency with coding well in at least one language PythonScalaRubyetc.
Proficiency with writing, tuning, and profiling complex SQL queries
Experience in Data Modeling and ETLELT
Experience with message queues e.g., RabbitMQ, Kafka, PubSub
Experience working with data-intensive products and analytics
Hands-on experience in building and developing Data Lakes, Data Warehouses, and Data Marts
Experience with AWS
Experience with the GCP stack BigQuery, Data Flow, Data Form, Composer, Data Fusion, etc.

Will be a plus

Experience with Clickhouse
Degree in Mathematics, Statistics, Computer Science, or a similar field

What we offer


Comfortable working environment. You can work at our comfortable hub in Kyiv or remotely anywhere with a flexible schedule. We reimburse expenses for renting workspaces in other cities. Also, you can get financial and legal support, modern equipment, and up-to-date tools.

Highly-skilled team. We have several independent products and cross-functional autonomous teams with Tech leads. The seniority level of the team is Middle  Senior, so you will get the option to share practical cases and knowledge with experts in the social networking niche.

Personal impact and ambitious goals. Our products are in top positions in their categories on App Store and Google Play. Were free to implement our ideas and directly influence products, cherishing our users. So you can make your direct impact on fast-growing products with a multi-million audience.

Investment in the future. The growth of our teammates is boosted with performance-oriented reviews, IDPs, free English classes, reimbursement of professional development courses, constructive feedback, international projects, a corporate library, and knowledge sharing.

Care and support. We maintain a human-to-human approach and care for our teammates safety, health, and rest with 20 paid vacation days, 15 days of paid sick leave, 100 coverage of medical insurance, reimbursement for sports and equipment, corporate events, stylish merch, and relocation options.

Hiring process

Intro call with a Recruiter
Tech interview
Bar-Raising Optional
Reference check
Offer

Take the next step in your career and apply now to be part of our team at United Tech!

P.S. Noticed a perfect match for your friend? We accept recommendations for all vacancies. If you are ready to refer your candidate, you can do it now and get a bonus up to 2000 lick Here!




"
https://startup.jobs/data-engineer-reinventing-geospatial-inc-rgi-4512261,Engineer,Data Engineer,"Reinventing Geospatial, Inc. (RGi) ","Springfield, United States",Full-Time,"

Job Highlights
As a Data Engineer, you will  work with a small talented team working with BIG DATA. You will implement complex data parsers for a variety of structured and unstructured data sets to render them in a format recognized by downstream analytical tools. You will work primarily in Python but knowledge of Java and Perl is valuable. You will automate certain bulk data processing using Apache NiFi and other technologies. As part of the programs process improvement program you will refactor legacy code to improve speed and efficiency of production processing. You will research new technology to develop innovative new methods for extracting and transforming the huge flows of data received and ingested daily. 

Core Technologies

Arc, SAS, R, Java, C, MATLAB, ScaLa, or Python 
SQL 
Pandas, Scikit, TensorFlow or Gensim 

In this role, you will...

Utilize programming languages such as SAS, R, Java, C, MATLAB, ScaLa, or Python to accelerate large data transactions across industry-leading GPU architectures to answer analytic questions 
Have experience with assessments, enterprise data integration, governance, and metrics, including the application of metadata management techniques and ability to interrogate databases efficiently using SQL 
Coordinate and support cross-community meetings and working groups; assimilate large volumes of information, and independently produce reports using data science focused libraries such as Pandas, Scikit, TensorFlow and Gensim to answer analytical questions 
Research, create, develop, and deliver professional briefings, multimedia presentations, and written reports

Qualifications

MA or MS in Data Science, Data Analytics, Informatics, Statistics, or related field AND 2 years CURRENT Intelligence Analysis experience; OR 
BA or BS in Data Science, Data Analytics, Informatics, Statistics, or related field AND 5 years CURRENT Intelligence Analysis experience; OR 
Undergraduate degree with graduateprofessional certificate in Data Science, Data Analytics, Informatics, Statistics, or related field AND at least 10 years of Intelligence Analysis experience 

Additional skills wed like to see

Knowledge of Army structure and defense level intelligence operations intelligence collection, fusion, analysis, production, and dissemination for intelligence databases and products 
Knowledge and experience with intelligence operations and in assisting with drafting expert assessments across operations priorities on behalf of the stakeholder 
Specialized training from any intelligence collection and analysis school or certification to include GEOINT Professional Certification GPC-F, GPCIA-II, GPCGA-II, GPCIS-II, etc 
Knowledge and understanding of the National System for GEOINT NSG and Intelligence Community; knowledge of private sector data scienceanalytics, machine learning, and data visualization communities 

Clearance

TSSCI level security clearance; ability to obtain TSSCI-CI Poly 
US Citizenship Required 


Company Overview 
Reinventing Geospatial RGi is a leading geospatial expert working with Defense, Intelligence, and Federal clients to achieve mission success and solutions for varied mission-critical programs. Projects at RGi span a wide range of software and analytical methodologies and lie at the intersection of software development and geospatial intelligence. We work with soldiers and geospatial analysts to produce solutions that allow them to develop better situational understanding of complex operational pictures. We do everything from data collection to UIUX development and advanced deep learning, using a broad toolset including Python, C, and ArcGIS. Our projects include cutting-edge RD efforts, as well as large mission programs incorporating data processingoptimization, data dissemination techniques, visualization, and collaboration across the commercial and government sectors.  

Grow to be our next leader ... 
At RGi, fostering a strong and organic corporate culture is paramount and serves as a compass on the decisions we make and how we operate the company. We believe our culture of camaraderie, innovation, and collaboration reflects the caliber of our employees and their dedication to the mission of providing quality software to our customers. As such, we want our employees to feel empowered to seek growth and leadership opportunities within the company and position us to maintain our culture as we grow. RGi provides opportunities, resources, training, and mentorship to all our employees to let them take control of their careers and become a leader or a crucial member of our company. If this is what you are looking for in a company, then you are what we are looking for in an employee. 

Who We Are

Reinventing Geospatial, Inc. RGi is a fast-paced small business that has the environment and culture of a start-up, with the stability and benefits of a well-established firm. We solve complex problems within geospatial software development and national defense to make an Immediate Impact for our nations soldiers and analysts.

We pride ourselves on giving employees an exceptional life experience, where creativity thrives and challenges are simply part of the fun. We provide truly excellent benefits, including

       100 paid employee healthcare  dental insurance
       Paid parental leave
       401k with matching
       Escalating vacation time
       Referral bonuses
       Tuition reimbursement
       Professional development training
       Free beverages and snacks
       Catered breakfast on Fridays

Reinventing Geospatial, Inc. is an Equal Opportunity Employer committed to hiring and retaining a diverse workforce. We are an Equal Opportunity Employer, making decisions without regard to race, color, religion, sex, national origin, age, veteran status, disability, or any other protected class. U.S. Citizenship is required for all positions.

"
https://startup.jobs/data-engineer-zennify-4512236,Engineer,Data Engineer,Zennify ,,,"

Data Engineer
Zennify is looking for a Data Engineer! In this hands-on role, you will be a team member who will participate in delivery and implementation support of the Zennify Data Strategy GTM. Zennify data engineers focus on data warehouses, ETLELT, data integration exposure to Zennifys DataHub strategy that is deployed on an ESB leveraging API led connectivity. , Klingon, and other compatible technologies. Okay, just kidding about the Klingon, but sometimes those technical terms you use can definitely sound like the language of Spock! You will focus on delivering next-gen data solutions to support our customers and maximize the use of their Enterprise Data, as well as explore new technologies to better meet future market needs. You will bring experience with transformations, canonical modeling, and associated data processing activities.
Primary Responsibilities 


Responsible for aligning skill sets to provide support and technical solutions for data verticals including ETLELT, WarehouseLake, ESBData Hub


Works collaboratively with each of the technology domains for ETLELT, WarehouseLake, and ESBDataHub to deliver enterprise Data Strategy solutions for clients


Collaborates with client stakeholders and data architects on technical requirements and specifications, data modeling, data processing, data integration and any other data management activities associated with a given solution. This will include technical documentation, requirements analysis, SQL query development, API integration development, API management, ETLELT development, testing, among other duties.


Champion and actively work in at least two of the data verticals ETLELT, WarehouseLake, ESBData Hub


Champion the enterprise technology vision big picture as well as delve into the specific details of a technical design


Mentors data analysts and aspiring engineers in industry and internal best practices and approaches


Follows unit, integration, and performance testing and participates with users for user acceptance testing to support a successful project outcome


Required Qualifications


5 years working experience delivering Enterprise Data solutions


B.S. in Computer ScienceSoftware Engineering


Experience with Mid Market Projects and above. Multiple data silos, data integrations


Experience with connectors, protocols, file types


Strong experience working with an ETLELT tool, including Talend, SSIS, Informatica


Strong experience with RDMS and exposure to NoSQL platforms. Examples MSSQL, Oracle, PostgreSQL, DynamoDB, MongoDB, Aurora, Redis, Amazon RDS, etc.


Experience with more than one database programming languages T-SQL, PLSQL, PLpgSQL, etc.


Experience with Enterprise Warehouse platforms including Snowflake, Synapse, Redshift, CosmosDB, BigQuery, etc.


Hands on experience with data modeling, including processing workflows


Experience with object oriented or scripting languages to access and transform data Java, C, Python, JavaScript, TypeScript, etc.


Experience recommending alternative approaches, analyzing impacts and facilitating sizing estimates 


Strong experience implementing and delivering designs to match solution approach


Experience working in structured release planning and deployment models


Strong technical writing skills. The ability to take the lead role in delivering comprehensive design documents and communication with clients


Experience designing and implementing data strategies for various technologies, protocols, data formats


Delivery experience with data migrations from third party systems to Salesforce or other external systems.


Is comfortable with the overuse of GIFs on Slack no, seriously, we send a lot of GIFs


 
Preferred Qualifications


Experience integrating with and troubleshooting enterprise level REST and SOAP APIs


Snowflake experience


Talend experience


Mulesoft experience


Experience modeling and delivering data to Machine LearningAI systems


Experience with Big DataSpark


Strong experience in the Banking and Financial Services industry


Strong Salesforce experience


 
Qualities of the Ideal Candidate


Thrives in a team-based, high energy and fast-paced environment


Service-oriented and innately driven to produce outstanding customer satisfaction and results


Passion for discovering, learning about and implementing new technologies


Analytical and able to logically and methodically work through problems


Strong aptitude for prioritization and multitasking in a deadline-driven environment


Possess a sense of urgency with strong organizational and follow-up skills 


Be a nerd. No, seriously, we are all a little nerdy here. If nerding out can both excite you and put you at ease, youre in good company.


Demonstrates the following leadership skills 



Sets deadlines for project team members 


Assigns responsibilities and directs the work of project team members 


Mentor, train and guide team members 


Monitors progress


Coaches and mentors 


Escalates when necessary 


AnalyzesIdentifies areas of risk and develops plans to minimize risk




The full compensation package is based on candidates experience and certifications. 
Zennify pay range 

105,000140,000 USD



Pay TransparencyWe believe that pay transparency helps to foster trust and open communication between employers and employees. It allows our team members to have a clear understanding of their earning potential and the expectations associated with their roles. We also believe that pay transparency promotes fairness, as everyone is compensated based on their experience and skills, regardless of factors such as gender, race, or ethnicity. We recognize that the skills and experience of our candidates may vary, and we are committed to compensating them accordingly. As such, we will evaluate each candidates qualifications, experience, and performance to determine the appropriate compensation within the stated salary range. We hope that this transparent approach to compensation will provide our potential candidates with the information they need to make informed decisions about their career paths.This is what we mean when we say were the Sum of Our Parts
We understand that everyone comes to the table with their own special histories, perspectives, opinions, backgrounds, and cultures. Here at Zennify, we believe that all of those distinctive individual aspects of our employees are precisely what makes Zennify a great place to work. We want to see you for who you are, so please bring your whole self to the interview process with us. In fact, here are some unique benefits that we offer as a direct result of our amazing employees
Pawreavement  Pawternity Leave Two days off of work if you bring a new furry friend home, and two days off of work if you have to say goodbye to one. We take our pets very seriously at Zennify, so its important to us to represent our entire family in the benefits we offer!
Unlimited PTO Because we want you to be a human being first and an employee second!
We cannot wait to meet you!
 
About Us
Founded in 2013, Zennify is a Platinum Salesforce consulting partner focused in Financial Services, Health and Life Sciences, and other key industries that prioritize the customer experience. With consistently high Customer Satisfaction scores, we live to provide epic solutions that solve our customers challenges, and exceed expectations! Our commitment to People Development and Customer Success are the driving forces behind our firm.Our purpose is to provide opportunities for professionals to develop amazing careers that drive compelling business impact. We promote and grow from within, building on strengths and investing in development.Balancing industry-leading Salesforce technical expertise with clear purpose and line of sight to our clients business objectives, we deliver innovative and contextual solutions that scale. Zennifys mission is to be problem solvers and relationship builders, striving to create solutions, opportunities, and sustained success for our people, customers, community, and future generations. We aspire to be the most trusted, impactful, and inspirational advisor in the consulting ecosystem. As professionals, we put in the work and are committed to milestones, but at Zennify having fun and giving back are part of the journey. We offer a culture that inspires, a place people love coming to work and believe in open communication.  Its all part of the Zenn!  
Our Values  Inspire Passion, Embrace Equality, Be Authentic, Integrity Matters.
 
EEO Posting Statement
It is the policy of Zennify to provide equal employment opportunities without regard to race, color, religion, sex, national origin, age, disability, marital status, veteran status, sexual orientation, genetic information, or any other protected characteristic under applicable law. This policy relates to all phases of employment, including, but not limited to, recruiting, employment, placement, promotion, transfer, demotion, reduction of workforce and termination, rates of pay or other forms of compensation, selection for training, the use of all facilities, and participation in all company-sponsored employee activities. Provisions in applicable laws providing for bona fide occupational qualifications, business necessity, or age limitations will be adhered to by the company where appropriate.


"
https://startup.jobs/data-engineer-complyadvantage-2-4511559,Engineer,Data Engineer,ComplyAdvantage ,"Lisbon, Portugal",,"

Description
We are looking for talented Data Engineers to help us build our internal BI solutions as well as provide insights to our customers. By giving our customers actionable insights, we enable them to be agile with their compliance operations, iterating on how they tune their search and transaction monitoring parameters. You will join the Operational Insights team, be involved in data engineering at scale, and design and build systems that create insights and analytics from large volumes of data.
About us
Since launching in 2014, we have been on a mission to neutralize the risk of money laundering, terrorist financing, corruption, and other financial crime on a global scale. In that time, we have raised over 88m in funding, have four global hubs located in New York, London, Singapore, and Cluj-Napoca, and are backed by Ontario Teachers, Index Ventures, and Balderton Capital.
We aim to grow to over 350 employees in the next 12 months, as we continue to fight the good fight against financial crime and help make compliance less painful for our client base of over 500 enterprises across 75 different countries. Were leveraging game-changing tech to help us on our mission as the financial industrys leading source of AI-driven financial crime risk data and detection technology.
No fight against crime is complete without the right values, and we take ours very seriously!
We are looking for people to join our team who share our core values...
Focus on the Team - Were Collaborative, Human, and Humble
Kaizen - Were Curious, Proactive and Agile
Deliver Results - Were Tenacious, Accountable and Focused 
We can only defeat financial crime if we have the right people with the right values in place to do so, and were committed to investing in passionate people who are experts in their field. Our culture and working environment are second to none - Dont believe us? See what our employees have to say on Glassdoor.
The Role
You will join as a Data Engineer within the Platform and Core Services group. We are looking for someone with demonstrable experience working with Data Warehouses and understands the design patterns and subtleties behind the different flavours of databases, computation engines, message queues and workflow orchestrators used in the Data Engineering space. When building solutions, they should be able to bring real-world experience to implementation and problem solving to help drive and implement a cohesive vision.
You will join one of our empowered teams and help contribute to the right technical and design decisions as will being an important part of implementing a solution which enables us to continue to evolve and scale our systems. You will be accountable for delivering parts of the system, and to discuss ideas collaboratively and agree solutions with your team.
We pride ourselves on having an open and collaborative environment within the teams and as well as working on your code, you will help others with their work through review and paired working. You will contribute to the overall success of the team, with a focus on the teams successful delivery rather than solely at an individual level.
Within the team, we look to engineers to take responsibility for the quality of their code.  Engineers are expected to write tests, add observability and take ownership of the systems to ensure overall quality. You will not only take part in this but also support others in the team to ensure the overall system conforms to a high quality.
What does success look like in your first 6 months?


You contributed to delivering your teams objectives to a high quality. Youve helped achieve this by both delivering your own tasks and also collaborating with other team members to support the overall objectives and milestones. Youve also utilised other engineers outside the team where relevant, and taken steps to include them.


You can explain to others the main problems were solving for our stakeholders with regards to the product capabilities the team owns, including where it integrates with other ComplyAdvantage systems. You know some of our customers and understand their biggest pain points. You know our future direction and participate in sprint reviews, design meetings and keep up to date with the product research.


You will have contributed your ideas to the architectural evolution of the platform and seen your ideas adopted.


You are continuously keeping up to date with best practices and evolutions of technology relevant to the systems you work on, as well as building up a knowledge of the technologies and systems used across the Engineering group. 


You will have delivered key components forming part of the Data Warehouse used to generate and deliver Insights to our customers.


Who we are looking for
We value those who take initiative and pride in their work and contribute to a positive working environment. You will be able to deliver on the points covered above, taking into account the following
Needs


Experienced developer in a range of languages and paradigms 



Ideally Kotlin. Python is a plus



Experience with high throughput data streaming architectures and technologies


Experience working with Data warehousing technology and Databases used for analytical workflows


Ownership of software from idea inception to production.


Experience working with tooling for testing, build and deployment pipelines 


Working understanding of observability - logging, monitoring, and alerting tooling


Experience implementing and designing cloud-native solutions 


Good communication and writing skills including experience producing technical documentation.


Desirable


Experience working in a multi-disciplinary team Software Engineers, SREs, Product


Experience managing data models and versioning.


Working with cloud based and containerised infrastructure.


Experience with ETL  ELT pipelines


Experience with Data visualisation tools or libraries


Benefits of working at ComplyAdvantage include


Competitive salary


Stock options scheme


Unlimited annual leave


Flexible working hours, and remote working opportunity


ComplyAdvantage celebrates diversity in our teams and welcomes applications from all backgrounds. Additionally, we value potential and growth, so if you dont feel that you fulfill all of the criteria, then you should still feel comfortable to apply and your application will be considered fairly.
 

At ComplyAdvantage diversity fuels our rocket ship and our commitment to inclusion across race, gender, age, religion, identity and experience drives us forward every day. We encourage everyone to apply and aspire to consider every application fairly.
We will handle your information in accordance with our Privacy Policy. For further information, please click here.


"
https://startup.jobs/data-engineer-valor-equity-partners-4509651,Engineer,Data Engineer,Valor Equity Partners ,"Chicago, United States",,"

Who We Are
Valor Equity Partners is a different kind of private investment firm. We invest in technology and technology-enabled companies that innovate and disrupt existing industries  from biosciences to transportation to food to health and wellness.
Our mission is to invest in and work side by side with companies that make the world a better place. These companies include SpaceX, Anduril, GoPuff, HackerOne, Cloud9, and others. Weve had the honor of serving some of the worlds greatest entrepreneurs and companies.
Our values are core to all we do. These values are excellence, humility, integrity, and responsibility.
Valor means that we

Strive for excellence in everything we do;
Maintain our humility and mutual respect no matter what circumstances we encounter;
Insist upon the highest level of integrity in our interactions and in the logic of our investment process; and
Demonstrate responsibility and dedication to all of our constituents.

About the Team
Labs is an internal team at Valor that builds software to support the Firms investment process. It comprises software, data, and machine learning engineers as well as data scientists with diverse backgrounds and levels of experience. The teams mission is to build cutting edge software applications and data models that generate proprietary investment insights and provide the investment team with tools that augment the investment decision making process. 
About the Role

Create, design, and deploy ETL and ELT flows for consuming from disparate sources of 3rd party data 
Build tooling to monitor data pipelines and perform troubleshooting and debugging to identify and resolve data quality and performance issues
Collaborate with data scientists, analysts, and other stakeholders to understand their data requirements and translate them into technical specifications
Collaborate with software and data engineers to help build a highly valuable software product
You will have autonomy to help shape the future of software engineering at Valor by bringing your ideas on improving and automating what we do and how we do it

Were excited about candidates that have

3 years of software development experience, with significant contributions that you can talk to
Exceptional coding skills in Python

Additional skills in RESTFUL API design, especially Flask, Django, or FastAPI are good to have

Experience with modern cloud platforms AWS, Azure, or GCP
SQL skills and working knowledge of multiple database types
Experience in DevOps for CICD, IaC Infrastructure as Code
Modern ETL tools Apache Beam, Kafka, Spark or similar
Passion for software engineering while being mission-driven, hard-working, humble, intellectually curious, and most importantly, great team players
Bias for execution and delivery. You know that what matters is delivering software that works every time
Ability to assist in system design and the generation of key technical assumptions while encouraging solutions that respect existing infrastructure
Willingness to be resourceful, flexible, and adaptable; no task is too big or too small

Our Tech Stack

Frontend React with Hooks, Material UI
Backend Python, Fast API
Tooling Google Cloud Platform
Data PostgreSQL, Firestore, BigQuery, Elastic Search, Prefect, Kafka, Scala, Spark, dbt


"
https://startup.jobs/data-engineer-intercom-4508935,Engineer,Data Engineer,Intercom ,"Dublin, Ireland",,"


Intercom is redefining how businesses support their customers using powerful messaging and automation.
Customer service teams from more than 25,000 global organizations, including Atlassian, Amazon and Lyft Business, rely on Intercom to deliver efficient and personal customer experiences at scale. Intercom is used to send over 500 million messages per month and enables interactions with over 600 million monthly active end users. 
Join the company helping businesses grow revenue through in-product messaging, and so much more!

Whats the opportunity? 
The Data team builds distributed systems and tools supporting Intercom by empowering people with information. As the company grows, so does the volume and velocity of our data along with the appetite for increasingly sophisticated and specialized data solutions.
Our team builds, maintains, evolves, and extends the data platform, enabling our partners to self-serve by creating their own end-to-end data workflows, from ingestion through transforming data and evaluating experiments to analyzing usage and running predictive models. We provide the data foundation to support many highly impactful business and product-focused projects.
Were looking for a Data Engineer to join us and collaborate on data-related initiatives, who is passionate about making quality data available for our stakeholders. 
What will I be doing? 

Develop, run and support our batch and real-time data pipelines processing petabyte-scale data on a daily basis using tools like Airflow, Kinesis, Snowflake, Tableau and Superset, all in AWS.
Get to work in an environment where multiple different programming languages are used. Python and SQL are our main languages, but we also regularly use shell scripts and Terraform to maintain our own CICD pipeline and the underlying infra.
Collaborate with product managers, go-to-market teams as well as analysts and data scientists to build automation and tooling to support their needs in an environment where dozens of changes can be shipped daily. This includes a lot of quality-of-life tooling to automate away daily toil to help everyone focus on high value added tasks. Custom integrations between various services Github, DataDog, Slack.. are a big part of this.
Implement systems to monitor what we have built, to detect and surface both bottlenecks and problems with the infra and data quality issues.
Help evolving the Data Platform by contributing to the design and implementation of the next generation of the stack, both from infrastructure and from data modeling point of view.

Recent projects the team has delivered

Redshift to Snowflake migration
New analytics stacks in AU and EU, including cross-regional pipelines
Switch to DBT from in-house transformations framework

What skills do I need? 

You have 3 years of full-time, professional work experience using a modern programming language on a daily basis. Strong preference for Python
You have a very strong understanding of SQL, and experience or interest in data modeling, warehouse design or backend systems.
You have some experience with or are interested in building and running data pipelines for large and complex datasets, including handling dependencies.
Worked with Apache Airflow - we use Airflow extensively to orchestrate and schedule all of our data workflows. A good understanding of the quirks of operating Airflow at scale would be helpful.
Experience or understanding of tools and technologies that are in our stack, such as Snowflake, DBT
You have some hands-on cloud provider experience preferably AWS.
You are aware of the importance of data security and are passionate about privacy.

Benefits 
We are a well treated bunch, with awesome benefits! If theres something important to you thats not on this list, talk to us! 

Competitive salary and equity in a fast-growing start-up
We serve lunch every weekday, plus a variety of snack foods and a fully stocked kitchen
Regular compensation reviews - we reward great work!
Peace of mind with life assurance, as well as comprehensive health and dental insurance for you and your dependents
Pension contribution
Open vacation policy and flexible holidays so you can take time off when you need it
Paid maternity leave, as well as 6 weeks paternity leave for fathers, to let you spend valuable time with your loved ones
If youre cycling, weve got you covered on the Cycle-to-Work Scheme. With secure bike storage too
MacBooks are our standard, but were happy to get you whatever equipment helps you get your job done



Intercom values diversity and is committed to a policy of Equal Employment Opportunity. Intercom will not discriminate against an applicant or employee on the basis of race, color, religion, creed, national origin, ancestry, sex, gender, age, physical or mental disability, veteran or military status, genetic information, sexual orientation, gender identity, gender expression, marital status, or any other legally recognized protected basis under federal, state, or local law. Intercom is currently able to hire if an employee has a permanent residence in the following locations; Australia, Ireland, England and applicable US states. California, Colorado, Florida, Illinois, Massachusetts, New York, North Carolina, Texas, and Washington.
Is this role not quite what youre looking for? Join our Talent Community to stay connected with us. 


"
https://startup.jobs/data-engineer-system1co-4508665,Engineer,Data Engineer,System1 ,"Los Angeles, United States",Full-Time,"


System1 is one of the largest customer acquisition companies in the world whose growth depends heavily on a very talented data engineering team. 

The Data Engineering team at System1 is focused on building frameworks, processes, and automation to ensure smooth running data pipelines and infrastructure. We process billions of records per day, for the benefit of multiple business functions like business intelligence, data science  machine learning, traffic quality and analytics.

You would be working in a fast-paced environment where system scalability, reliability, usability, efficiency are the goals. Come join us!

 The Role You Will Have

Designing and developing data processing infrastructure.
Developing new and improving existing data pipelines, extracting from external API sources or internal events.
Developing self-serve data solutions, self-correcting robust ETL pipelines.
Continuously improving monitoring and alerting coverage.
Self-driving proof of concept for new technologies, new patterns and writing technical specifications for data architecture projects.
Identifying scaling bottlenecks and how to prevent them.
Performing maintenance of existing infrastructure, investigating issues and failures.
Conducting SQL data investigations, and optimizations..
Participate in peer code reviews and produce high quality documentation


What You Will Bring

Bachelors or Masters degree in Computer ScienceEngineering.
Programming proficiency in Python is required. 
Experience in Cloud ecosystems like AWS is required. GCP, Azure are preferred.
SQL expertise, and preferably SQL query optimization experience.
Database design skills, both relational and non-relational, SQL and NoSQL.
Experience with Cloud data warehouses like BigQuery, Snowflake, Redshift preferred.
Modern orchestration platforms such as Airflow.
Good data engineering fundamentals, ETL experience and analytics skills required.
Knowledge of data engineering mechanics, flow, distribution, optimization.
Data organization, distribution, latency, observability.
Distributed big data processing and storage systems.
Kubernetes, docker, containerization strategies, LinuxUNIX would be good.
Experience with Kafka would be a plus. 

What We Have to Offer

Competitive salary  bonus  equity
Generous PTO  11 company holidays
Open sick time
100 covered Medical, Dental, Vision for employees
401k with match
Health  Dependent Care Flex Spending Account 
Paid professional development
Leadership  growth opportunities
Virtual company and team building events 
LI-Remote
LI-Hybrid
BI-Hybrid
BI-Remote
LI-AW1



The U.S. base salary range for this full-time position is 116,500 - 186,600   bonus  equity  benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. Please note that the compensation details listed in U.S. role postings reflect the base salary only, and do not include bonus, equity, or benefits.

System1 offers flexible work arrangements for most employees unless they hold positions which are identified as having to be 100 onsite in Marina del Rey, CA, Bellevue, WA or Guelph, ON Canada. Most System1 full-time employees choose to work in a hybrid environment, splitting their time between working in our offices and working remotely.  System1 allows fully-remote work in the following approved locations Arizona, Colorado, Georgia, Hawaii, Minnesota, Missouri, New Jersey, New York, North Carolina, Oklahoma, Oregon, Pennsylvania, Tennessee, Texas and Virginia. Prospective U.S. employees who live outside of any of these states will need to establish residency in one of the approved states prior to employment.

"
https://startup.jobs/data-engineer-dayshape-4507075,Engineer,Data Engineer,Dayshape ,"Edinburgh, United Kingdom",Full-Time,"

Job Title Data Engineer 
Salary Range 35,000 - 50,000, dependent on experience

About us
Were an award-winning enterprise software scale-up with high ambitions for growth. Being recognised as Scotlands fastest growing tech company in the Deloitte Technology Fast 50 awards three years in a row shows that we are on the right track! Dayshape is an advanced resource management solution, incorporating AI and intelligent automation to help professional services firms optimise their workforce like never before. Its built to handle large, complex, and ever-changing requirements with ease. Our customers include accountancy firms from the Big Four and global top 10, and Dayshape is used in more than 15 countries across four continents. Our target customers are global firms, international networks, and large nationwide or regional organisations, if theyre big enough to have the challenges that Dayshape can solve. This year were making strides into other areas of Professional Services, focusing on IT service providers and consultancies and management consultants. As a company, we live our values every day and were committed to making sure our friendly and inclusive environment grows with us.

About the role
Over the last year we have grown and gained many new customers. We are adapting our processes as we scale, and this includes growing our new, specialised team for developing customer integrations.
This is a highly collaborative role where you will have the opportunity to work directly with clients on requirements gathering and the implementation of new integrations. As the demand for integration work increases, you will be heavily involved in setting the standards for our integrations going forward. 
 What youll do
 Work with our software implementation consultants SICs to define and verify specification documents for ETL process Work with customer IT to test customer data source endpoints to ensure they meet specification Implement, test and deploy Azure Data Factory ADF pipeline definitions within version control to customer environments Work with our Site Reliability Engineering team to ensure your solutions are observable, reliable and performant Work with our Engineering teams to ensure end-to-end capability for integrated data Support cutover to production systems can be outside normal working hours Identify improvements to existing Azure Data Factory processes to ensure they are more maintainable across a growing set of customers 
 About you
 Experience building and maintaining data integrations with a variety of external systems You have at least a years experience of Azure Data Factory and are comfortable building transparent, easy to support pipelines. Good understanding of the ETL process Comfortable being in a client-facing role Excellent communication skills you are able to clearly explain technical matters to any audience Confident working with complex referential data Knowledge of Rest APIs, SQL databases and other data sources A team player, with experience collaborating with other departments You demonstrate good attention to detail and enjoy breaking complex problems down into simple steps.  
 Bonus points if you have
 Previous experience leading calls with clients Experience in other Azure data technologies such as Azure Databricks Experience using PowerBI  Data Warehouse Integrated with a variety of downstream data sources, including but not limited to Cloud services Custom APIs Database on-prem   

What youll get
 Salary 35,000 - 50,000, dependent on experience At least 1,000 per year to spend on professional and personal development 33 days holiday per year including bank holidays, increasing by 1 day each year to a maximum of 40 days Paid four week sabbatical in your fifth anniversary year on top of your holiday entitlement Private healthcare and rewards through Vitality Income protection and death in service cover Matched 5 auto-enrolment workplace pension scheme EMI options as part of our employee share options scheme Access to wellbeing offerings, such as our Employee Assistance Programme and a dedicated counselling service Innovation Week twice a year - a chance to experiment and work off-project Weekly All Hands meeting for inspiration and over-communication Time out of the working week for team socials each month, with a mix of in-person and virtual options past events include hiking, family BBQs, online games, DD, and at-home cocktail classes! Genuinely nice, smart people to work with, who are excited about growing our company  

Working Details
This is a full-time role 37.5 hours per week. We typically work from 0900 - 1730 from Monday to Friday, though we can be flexible around this, just let us know. Were ideally looking for someone inaround Edinburgh, though were open to the possibility of this being a remote role as long as youre in the UK. Were making the most of flexible hybrid working, so you wont need to come into the office everyday unless you want to!. We dont mandate required office time, but we find that most of the team enjoy working from home 2-3 days a week, and come into our office to connect with each other, make use of space, and for meetings. 
Join the team!
Equality of opportunity is more than just a responsibility we believe its a huge advantage to welcome a variety of experiences and perspectives into the team. Diversity is a great asset and, as such, we strongly encourage applications from any background. This is your opportunity to really influence how we get things done, and take our customer integrations to the next level. Were doing well, but theres lots more to do in order to maintain the high bar and pace that weve set. Everyone here is growing personally as the company grows, so if that sounds like something youd like to be part of, wed love to see your application.
Initial video calls will be conducted from the 26th June, with follow-up interviews taking place from the 10th July. 

"
https://startup.jobs/data-engineer-python-engineer-nahc-limited-4506398,"Engineer,Python",Data Engineer/ Python Engineer,NAHC Limited ,,Full-Time,"

Our client is a global technology group with wide data access in over 18 countries, with particularly strong expansion plans in Asia and European markets. Their data driven business creates a need for data professional to join them and help construct a world-class data infrastructure. They are now looking for a senior data engineer with following responsibilities.
Responsibilities

Manage data lakehouse architecture and self-service tools
Streamlining the internal process improvements such as the automation of manual processes, infrastructure design for greater scalability etc
Performing ETL pipeline, Data lake and Data warehouse building to meet business requirements with optimal extraction, transformation, and loading of data from a wide variety of data sources
Ensuring data reliability and analyzing data to isolate and explain data irregularities
Ensure that data is highly available and reusable for enabling data science initiatives, analytics and end users to drive business growth
Working with stakeholders including the Data Analysts, business and technology stakeholders to assist with data-related technical issues and support their data infrastructure needs

Requirements

Sound technical experience in data analytics, strong in ETLELT, data warehouse performance tuning and data structure design
Solid experience on AWS EC2, S3, KinesisKafka, Athena, Redshift or other public clouds, SQL, PythonScala.
Experience leading a small team of data engineers is a plus.
Working experience in the digital, e-commerce industry, mobile app and web environment is highly desirable.
Detail-oriented and self-starter mentality with the ability to manage multiple projects in a fast-paced working environment
Problem solver with positive attitude
Excellent communication  interpersonal skills
Candidates with less experience are welcomed and will be considered as a Data Engineer Associate




"
https://startup.jobs/data-engineer-uk-or-europe-remote-honuai-4506170,Engineer,Data Engineer - UK or Europe (Remote),Honu.ai ,"London, United Kingdom",Full-Time,"

We are honu.ai, a remote-first team of 10 creating a novel technology that empowers business owners with superhuman decision making capabilities. Our platform transforms the digital ecosystem into a frictionless smart connective tissue, creating the foundations for the future of the commerce. Our investors have backed TransferWise, Tide, Coinbase, CityMapper, Cazoo, Nested, Habito, Cleo, King, and Snyk, and we are recipients of a Smart Grant from Innovate UK fund. Our highly experienced senior team has built cutting edge innovations across multiple sectors, including robotics and neuroscience.
You Will
 Collaborate with the team in designing, constructing, and maintaining data pipelines. Ensure data quality and integrity through monitoring and testing. Monitoring and supporting production, staging, research, and demo environments Develop and optimise ETL workflows using Airflow, Airbyte, and DBT. Build and maintain data models using Python in Postgres and BigQuery. Manage OAuth user credentials and work with various data feeds. Develop services using FastAPI and Auth0. Manage data infrastructure using Terraform for deployment. Integrate data sources and systems into our pipelines by working with third-party providers. Continuously improve data infrastructure and processes for enhanced efficiency and scalability 
Requirements
About You
 Proficiency in Airflow, Airbyte, Python, DBT, Postgres, and BigQuery on GCP. Knowledge of OAuthAPI user credential handling and feed acquisition from sources such as Shopify, QuickBooks, Google Analytics, Facebook, Klaviyo, and many others Experience in service development with FastAPI and Auth0. Familiarity with Poetry, GitHub, and Terraform tools for development and deployment. Ability to investigate and assess new data products and services pragmatically. You believe in easy-to-maintain code and identify with modern concepts such as clean coding, automated testing and continuous deployment. You know start-up life, ideally within a SAAS business. Youve shipped high-quality products from start to finish and are equally as comfortable building from scratch as you are improving an existing product. You understand how a product funnels  retrieves data and are comfortable working closely with data scientists and data engineers to ensure optimal efficiency. You will be comfortable with RemoteAsync ways of working. Youre considerate, humble, and a strong believer in teamwork. Youre comfortably organised amongst the chaos. Youre comfortable taking ownership, accountability and responsibility. Based in the UK or Europe. Youre available to meet in London for a 3-day team working session every 4-6 weeks. 
Nice to haves
 Experience with 3rd-party API integrations DevOps experience  incl. Terraform  React and Typescript experience Fintech experience is a big plus, eCommerce experience is nice to have, but not a requirement. 
Tech Stack
Our initial stack is Python-centric with some ancillary GoLang services for the backend, React  Typescript on the front end, and we are running on GCP. We have a pragmatic, non-dogmatic approach and will refine the tech stack, finding the right tools for the job, as the product evolves. Scientific computing will be mostly implemented in Python, for now.
Our Process
 Technical Screening with our Founder and our CTO 30 - 45min Pair programming Technical Assessment with our CTO 45min - 1hr Final chat with Team members and Founder 1hr 
Benefits
We are a young business with a huge amount of evolution to come. Alongside a salary, we are able to offer
 Competitive stock options package Remote-first working policy Regular company offsites 
We are growing fast and will be sure to respect your time and expectations each step of the way. We embrace diversity in all forms and are dedicated to building an inclusive, equitable environment for all people to do their best work with us.
Were an equal-opportunity employer. All applicants will be considered for employment without attention to ethnicity, religion, sexual orientation, gender identity, family or parental status, national origin, veteran, neurodiversity status or disability status. If there are any suitable adjustments that can be made to ensure a smooth interview process, please do let us know.

"
https://startup.jobs/data-engineer-q-centrix-2-4506076,Engineer,Data Engineer,Q-Centrix ,"Chicago, United States",,"

Were super into the work we do and the community weve built and think you might be, too.
Q-Centrix is the largest exclusive provider of clinical data management solutions to acute care hospitals. A market disruptor and innovator, Q-Centrix believes that there is nothing more valuable than clinical data as it is critical in delivering safe, consistent, quality healthcare for all. Bringing together deep expertise across providers, clinical knowledge, data and software, Q-Centrix provides an integrated approach that can redefine and streamline the data management and real-world application process for the healthcare industry, thereby enabling increased efficiency and exciting new solution opportunities.
Providing the industrys first Enterprise Clinical Data Management eCDM platform, Q-Centrix utilizes its market-leading software, the largest and broadest team of clinical data experts, a modern-stack software and reporting data structure, and best practices from its 1,200 hospital partners to securely extract, curate, structure, and enhance clinical data at the highest quality level. The resulting high quality structured clinical data is then utilized to support reporting demands, drive improved care delivery, meet financial and operational needs, enable population health workflows and power broad research use cases. Its solutions cover a breadth of clinical segments, including cardiovascular, oncology, infection prevention, trauma and real-world data applications. Q-Centrixs platform enables its partners to access valuable clinical information that may otherwise be trapped across multiple workflow systems and clinical information platforms. Q-Centrix is positioned for continued growth as they integrate new capabilities and business lines.
Backed by a leading global private equity growth firm, TPG, Q-Centrix will continue to invest heavily in technology data, software, automation, people, and processes that can accelerate access to high quality structured clinical data at scale and facilitate greater real world data applications.
Job Summary
Q-Centrix is implementing robust, next-generation data solutions to provide advanced capabilities for our partners in quality reporting, analysis, and improvement. Were seeking an ambitious, curious and inventive team member to join our small and mighty team.
Essential Functions

Partner closely with the Clinical Data Architecture Team to optimize and load clinical data models.
Design, develop and implement data models, schemas, and distribution strategies for Amazon Redshift. Optimize existing data workloads using dbt, Apache Airflow, and AWS Glue for performance and scalability
Work with the Product Development, Operations and Finance Teams in creating and analyzing reports both for our partners and internally.
Work with other engineers to build and maintain data pipelines.
Envision and create reports that are translated from internal ideas, customer feedback and competitive analysis
Partner with other disciplines within Q-Centrix to ensure project requirements are comprehensive and thoroughly planned and documented
Other Duties Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice.

Required SkillsAbilities

Bachelors Degree in Computer Science, Information Technology, MIS, Mathematics, or a related field.
4 years of experience working on production systems within healthcare or a related industry.
3 years developing data models for data visualization software, preferably Tableau.
Experience with data warehousing and data lakes.
Understanding of data architecture, deployment, and optimization best practices.
Experience with ETLdata integration tools such as dbt, Informatica, Apache Airflow.
Proficient in Database Management Systems.
Experience with common Project Management methodologies including waterfall, agile, project planning, scope control, and customer relationship management.
Strong problem-solving and analytical skills.
Excellent communication and teamwork skills.

Preferred Education and Experience

AWS Certified Data Analytics - Specialty or 3 years working on Redshift.
3 years of experience building solutions with core AWS services.
3 years of experience designing, developing and implementing solutions with Amazon Redshift.
Experience working with Database Management Systems, preferably with PostgreSQL.

Supervisory Responsibilities None
Work environmentPhysical Demands Continuous sitting and fine manipulation.
Travel Requirements None
Work Authorization Legally able to work in the United States without sponsorship
Total Rewards
At Q-Centrix, our purposesafer, consistent, quality healthcare for alldrives everything we do. To accomplish this important work, we need to attract, engage, and retain a talented team by providing a compelling, equitable rewards package comprised of an inclusive culture, flexible work environment, learning and development opportunities, competitive pay that rewards high performance, and robust benefits that support health and financial wellness. Add to this package a supportive community of people who help each other not only do meaningful work, but learn, grow, and have fun while doing so, and you get an organization that has earned the Great Place to Work distinction multiple years in a row!
The target salary range for this role is 110,000 to 130,000 per year, plus an annual bonus targeted at 5 of the team members annual salary An individuals salary within this range is based on multiple factors including but not limited to skills, experiences, licensure, certifications, and other business and organizational considerations. Salary ranges are reviewed, at minimum, annually and all team members are eligible for performance-based salary increases during our organizations annual review period. All commissions are considered variable pay and are paid per the Q-Centrix commission plan, which is shared with team members annually. The actual commission payout may be higher or lower, dependent on individual performance.
In addition to our inclusive and innovative working environment and competitive pay, team members enjoy

Remotehybrid flexibility depending on location and a generous Flexible Time Off program with additional paid time for volunteering.
Robust benefits package including medical, vision, dental, health savings accounts, company paid short- and long-term disability, employee assistance
program, paid parental leave, life insurance, accident insurance, and other voluntary benefit programs for employees and their eligible dependents.
401k retirement plan with a company match.
Opportunities for professional development.


Commitment to Diversity, Equity, Inclusion and Belonging 
At Q-Centrix, we hire people who love learning, value innovation, and believe in our purpose of safer, consistent, quality health care for all. We applaud qualified applicants who are accountable and committed to producing quality work. As an Equal Opportunity Employer, we support and value diversity, dignity, and respect in our work environment, and are committed to creating an inclusive environment in which everyone can thrive.
We employ people based on the needs of the business and the job, and their individual professional qualifications. Heres what does not impact our employment decisions race, religious creed, religion, color, sex, sexual orientation, pregnancy, parental status, genetic information, gender, gender identity, gender expression, age, national origin, ancestry, citizenship, protected veteran or disability status, health, marital, civil union or domestic partnership status, or any status or characteristic protected by the laws or regulations in locations where we operate. If you are an individual with a qualified disability and you need an accommodation during the interview process, please reach out to your recruiter.
 
California Privacy Rights Notice
During the application and hiring processes, Q-Centrix, LLC Q-Centrix may collect personal information from applicants.  For residents of California, the collection of this personal information is subject to the California Consumer Privacy Act of 2018 the CCPA and the California Privacy Rights Act the CPRA which impose a duty on Q-Centrix to provide this notice at the point of collection, explaining what may be collected and for what purpose.  Information collected by Q-Centrix in this way is stored and handled in accordance with data security industry standards best practices including technological, administrative, procedural, and physical safety measures.  Q-Centrix does not store this information longer than is reasonably necessary to fulfill its purposes listed herein and securely disposes of the data at the time and in the manner dictated by its internal document retention policies.
Personal Information we Collect  Q-Centrix may collect certain personal information during the application process. Over the preceding 12-month period, Q-Centrix may have collected the following categories of personal information as defined by the CCPA

Identifiers This may include name, username, alias, postal address, telephone number, email, IP address, social security number, drivers license number, passport number, or other personal identifiers, including identifiers created by Q-Centrix to recognize an applicant or employee.
Characteristics of Protected Classifications Under California or Federal Law This may include race, skin color, national origin, religion, sex, gender identity, sexual orientation, marital status, medical conditions, disability, militaryveteran status, age, or any requests for protected leave.
Professional or Employment-related information This may include application, resume, curriculum vitae, professional licenses, designations, credentials, certifications, or any information collected as part of the application or on-boarding process.
Education Information This may include education records, transcripts, report cards, diplomas, degrees, or certificates.
Criminal, Financial, and Recreational Drug-Use Information This may include information obtained from a background check, credit check, or employment-related drug test.

Purpose and Use of the Collected Information  Q-Centrix may use the above data for a variety of purposes related to the recruitment process and the administration of our business.

Recruitment Process This may include identifying candidates, processing applications, creating and making employment offer packages, reference checks, education verification, visawork-permit filings, and on-boarding applicants as new employees.
Improve Internal Processes This may include data about responsiveness to recruitment postings, effectiveness of recruiting processes and tools, and the collection and analysis of data as it pertains to employment markets, hiring practices, compensation, and recruitment processes or tools.
Recordkeeping This may include keeping records for administrative purposes, maintaining contact information of candidates, complying with legal, regulatory, or law enforcement requirements, to comply with any reporting obligations, to protect or exercise our legal rights or remedies, or for litigation purposes.

Disclosure of Information  Q-Centrix may disclose your personal information to third parties within the context of our recruitment process or your employment.

Vendors This may include individuals or companies that provide recruitment-related services, administrative services, human resource services, employment screening services, onboarding and training services, process refinement services, data processing, data storage, or other services related to the application and hiring processes.
Employees, Parents, and Affiliates This may include individuals, parent companies, or affiliate companies of Q-Centrix related to the recruiting process or onboarding process. Additionally, Q-Centrix may disclose information to employees, parents, affiliates, or other companies or individuals as necessary as part of a reorganization, merger, or acquisition.
Mandatory Disclosure Q-Centrix may disclose your personal information as it believes necessary to comply with any applicable laws, regulations, court orders, or governmental requests.
Protection of Interests Q-Centrix may disclose your personal information in order to protect its interests, which may include protecting itself from illegal activities, regulatory violations, security issues, technical issues, rights violations, harm to property, or harm to individuals.

Sources of Information  Q-Centrix may obtain personal information from

You This may include information you provide on an application or any other information you provide us directly.
Vendors This may include any vendors we disclose information to in accordance with Section 3.1 above.
Verification Sources This may include professional organizations, licensing bodies, academic institutions, or any references.
Publicly Available Sources This may include public governmental filings, social media profiles, personal websites, news articles, or any other publicly available information.

Your Rights  You retain some rights to your personal information and may submit requests in accordance with those rights by reaching out via the methods below.

Access and Updates You may submit a request to access, correct, update, or delete any of your personal information Q-Centrix possesses.
Use You may submit a request to limit the allowed purposes for which Q-Centrix may use or process your personal information.

MODIFICATIONS AND REVISIONS
This notice may be updated to reflect changes to our policies in accordance with our business, legal, or regulatory obligations, at which time Q-Centrix will provide any legally required notices of such changes.
CONTACT INFORMATION
If you have questions about the Q-Centrix California Privacy Rights Notice or a request regarding any personal information which may have been collected during the application andor hiring process, please contact 1-877-284-8681 or send an email to peopleteamq-centrix.com.
 


"
https://startup.jobs/data-engineer-data-pipeline-focused-mid-senior-deptagency-4505843,"Engineer,Senior",Data Engineer - Data pipeline focused (mid/senior),Dept ,"Zagreb, Croatia",,"

We are seeking a highly skilled and experienced Business IntelligenceData Engineer to join our team. 
As a Business Intelligence Data Engineer, you will play a key role in designing and implementing robust data analytics solutions. You will collaborate with cross-functional teams to gather requirements, design data models, and develop visualizations that enable data-driven decision-making across our clients organizations. 
 
Note Visa or relocation costs are not supported. This position is located in Croatia and requires Croatian address and citizenship.
 
Our DEPT culture is very important to us. That is why we are eager to find not only the most talented expert in the game, but also a perfect DEPT-fit. 
We are looking for someone who is eager to learn and teach, is honest and direct. Our ideal candidate communicates clearly and brings out the best in situations and people around them. 
Together with your team, you will create and deliver top-quality solutions for our clients. Some of which are brands such as smart, Nikon, Douglas, Deichmann, Nivea, UNICEF, Samsung, Reckitt,...
Along with being a DEPT-fit some technical knowledge is required.
 
WHAT DOES A BI ENGINEER AT DEPT DO


Lead the design, development, and implementation of end-to-end business intelligence solutions.


Develop scalable data models and data pipelines to extract, transform, and load ETL data from various sources into a centralized data repository.


Assist with the management and maintenance of the overall Business Intelligence environment, including configuration, releases, metadata, user administration and standards as requested. 


Gathering and analyzing business requirements to design data models and structures that support effective data analysis and reporting.


Transforming raw data into meaningful metrics and accessible formats for end-users.


Participate in data modeling, business process analysis, development of reporting functional and technical specifications and most importantly business community review of BI deployments. 


Ingesting, accessing and integrating disparate big data sources. Analyze data to ensure consistency and integrity across systems. 


Ensuring data accuracy, consistency, and relevance in reporting.


Design and optimize data queries and procedures for performance, reliability and efficiency.


 
WHAT WE ARE LOOKING FOR


A minimum of 2-4 years of experience in analytics engineering roles must


Proficiency in designing and developing data models, visualizations, and dashboards using Power BI


Experience with self-service analytics tools and enabling self-service capabilities for business users


Experience in SQL, Databases, Data Modeling


Experience in workflow management and orchestration dbt andor airflow


Experience with ETLELT tools AWS Glue, Azure Data Factory, FiveTran


Familiarity with modern data warehouse architectures.


Familiarity with cloud-based data tool stack Data Lake - Databricks, S3


Plus if you have experience working with PowerBI DAX, M, data modelling, etc.


Plus if you have experience with DevOps CICD concepts, GIT, etc.


Fluency in English


 
WHAT WE OFFER 


Transparent career development and pay


Flexible working hours


Hybridremote work


A rewarding, supportive and healthy company culture


Meal allowance


Travel allowance


Apple MacBook Pro or a PC of equivalent performances


Additional desk budget


Christmas and yearly bonuses


Referral bonus


Baby bonus 10 000 HRK


Fully paid sick leave


Supplemental and additional health insurance


Third pillar pension fund


Paid days off for special events weddings, etc.


A minimum of 25 vacation days


Global team-building in the Netherlands at DEPT fest


Multisport programme


Other bonuses, rewards and benefits; some benefits we have, some we are working on  and we are always up for recommendations 



About DEPT 
We are a technology and marketing services company that creates end-to-end digital experiences for brands such as Google, KFC, Philips, Audi, Twitch, Patagonia, eBay and more. Our team of 4000 digital specialists across 30 locations on 5 continents delivers pioneering work. 
Our culture is built on diversity, knowledge and individuality and this culture ensures that our experts do what they excel in and gives them freedom to become even better. 
We love individuals who take charge, we develop talent, share knowledge and support each other in our goal to become the best version of ourselves.
We are committed to making a positive impact on the planet and since 2021 have been Climate Neutral and B Corporation certified.
To apply, use our application form.If you are still in doubt about choosing us, dont hesitate to reach out and get a clearer picture of what DEPT actually is. 
 
Note Visa or relocation costs are not supported. This position is located in Croatia and requires Croatian address and citizenship.
 

"
https://startup.jobs/data-engineer-chainalysis-4505682,Engineer,Data Engineer,Chainalysis ,,,"

Blockchain technology is powering a growing wave of innovation. Businesses and governments around the world are using blockchains to make banking more efficient, connect with their customers, and investigate criminal cases. As adoption of blockchain technology grows, more and more organizations seek access to all this ecosystem has to offer. Thats where Chainalysis comes in. We provide complete knowledge of whats happening on blockchains through our data, services, and solutions. With Chainalysis, organizations can navigate blockchains safely and with confidence.
The Data Engineering and Architecture DEA team is responsible for unlocking the value in Chainalysis corporate data. We work with internal stakeholders in HR, Finance, Sales and Marketing as well as partners in Business Technology and Core Engineering to ship data solutions that are trusted, reliable, mature and transformative. Our goal is to accelerate Chainalysis growth and empower our employees to do their best work. 
The Data Engineer will design, build and maintain data integrations and automations between enterprise SaaS platforms and data warehouses. This work will be done in close partnership with colleagues in DEA to spin up and maintain data pipelines that support critical data-driven decisions across the company.
The ideal candidate will have experience working closely with Analytics and Business Systems stakeholders to empower business leaders and users by enabling solutions that will help them make decisions. They have held past roles developing robust data integrations and automations between enterprise platforms using a mixture of no-code and low-code tools as well as custom builds using a general purpose scripting language. They will also have worked with DevOps engineers to stand up and update cloud services and integration and deployment pipelines.
This role is open to remote candidates in Canada. 
In one year youll know you were successful if

Youve developed and maintained data integrations and automations encompassing both ETL and Reverse ETL use cases 
Youve created and maintained documentation for those data integrations and automations
Youve have built a data pipeline end to end using Chainalysis data stack
Youve become comfortable with building custom solutions using cloud services
Youre working through a backlog of fixes and enhancements

A background like this helps 

Hands-on experience using no-code and low-code platforms like Fivetran and Workato to build data pipelines using off-the-shelf connectors to platforms like Redshift, Snowflake, Workday, Tableau, Salesforce and NetSuite
Hands-on experience using dbt to build models and transformations via source control tools like Git and GitHub
Ability to write and execute intermediate-level SQL queries
Intermediate knowledge of RDBMS database structures and data formats like JSON
Obtaining authorization to and utilizing endpoint and resources via REST APIs
Experience in an Agile-like environment that includes user stories, sprints, grooming and retrospectives
Performing root cause analysis on integration and automation errors while identifying opportunities to improve instrumentation and observability
Basic skills with writing procedural code using Python
Familiarity with orchestration tools like Airflow 

 
LI-BD1 LI-Remote 
 

At Chainalysis, we help government agencies, cryptocurrency businesses, and financial institutions track and investigate illicit activity on the blockchain, allowing them to engage confidently with cryptocurrency. We take care of our people with great benefits, professional development opportunities, and fun.
You belong here. 
At Chainalysis, we believe that diversity of experience and thought makes us stronger. With both customers and employees around the world, we are committed to ensuring our team reflects the unique communities around us. Some of the ways were ensuring we keep learning are an internal Diversity Committee, Days of Reflection throughout the year including International Womens Day, Harvey Milk Day, World Humanitarian Day, and UN International Migrants Day, and a commitment to continue revisiting and reevaluating our diversity culture. 
We encourage applicants across any race, ethnicity, gendergender expression, age, spirituality, ability, experience and more. Additionally, if you need any accommodations to make our interview process more accessible to you due to a disability, dont hesitate to let us know. You can learn more here. We cant wait to meet you.  
Applying from the EU? Please review our Chainalysis Applicant Privacy Policy.
  
By submitting this application, I consent to and authorize Chainalysis to contact my former employers, and any and all other persons and organizations for information bearing upon my qualifications for employment.  I further authorize the listed employers, schools and personal references to give Chainalysis without further notice to me any and all information about my previous employment and education, along with other pertinent information they may have, and hereby waive any actions which I may have against either partyies for providing a reference.  I understand any future employment will be contingent on the Company receiving satisfactory employment references.


"
https://startup.jobs/data-engineer-starlink-spacex-4505670,Engineer,Data Engineer (Starlink),SpaceX ,"Redmond, United States",,"

SpaceX was founded under the belief that a future where humanity is out exploring the stars is fundamentally more exciting than one where we are not. Today SpaceX is actively developing the technologies to make this possible, with the ultimate goal of enabling human life on Mars.
DATA ENGINEER STARLINK
At SpaceX were leveraging our experience in building rockets and spacecraft to deploy Starlink, the worlds most advanced broadband internet system. Starlink is the worlds largest satellite constellation and is providing fast, reliable internet to 1M users worldwide. We design, build, test, and operate all parts of the system  thousands of satellites, consumer receivers that allow users to connect within minutes of unboxing, and the software that brings it all together. Weve only begun to scratch the surface of Starlinks potential global impact and are looking for best-in-class engineers to help maximize Starlinks utility for communities and businesses around the globe.
As a Data Engineer, you will be responsible for developing the strategy, key metrics, tools, software services and processes for assessing how well key aspects of Starlink are scaling and how effective the Starlink Network is itself, in serving millions of users around the globe. You will work with operators, subsystem responsible engineers, software engineers, and network engineers inside the Starlink organization as well as key contacts with various major external partners to help ensure the growth of this program.
RESPONSIBILITIES

Build and maintain mission-critical infrastructure, tools, processes, and custom software to objectively assess growth areas for the Starlink program
Automate the aggregation of metrics and detection of widespread application issues across Starlink
Establish and maintain relationship with key third party applicationcontent owners
Lead technical investigations about chronic application-level issues
Build ground-based software systems that ingest, transform, and store data
Apply data analytics, models, and techniques to data products created by space vehicles 
Create catalogs of data and tools that can be used by you and other teams to perform analytics 
Fuse data from multiple sources to create usable information 

BASIC QUALIFICATIONS

Bachelors degree in computer science, data science, physics, mathematics, or a STEM discipline; OR 2 years of professional experience in data engineering in lieu of a degree
Development experience in an object-oriented programming language i.e. C, C, Python

PREFERRED SKILLS AND EXPERIENCE

Professional experience in analytics, data science, or machine learning 
Experience using Spark, Presto, Flink, or Snowflake 
Experience building solutions with Parquet, or similar storage formats 
Knowledge of Kubernetes 
Experience building solutions with in-stream data processing of structured and semi-structured data   
Experience building predictive models and machine learning pipelines clustering analysis, prediction, anomaly detection  
Experience in custom ETL design, implementation and maintenance 
Experience handling large TB datasets 
Experience with developing and deploying tools used for data analysis
Ability to work effectively in a dynamic environment that includes working with changing needs and requirements
Ability to take on projects that require taking initiative and developing new expertise

ADDITIONAL REQUIREMENTS

Must be willing to work extended hours and weekends as needed

COMPENSATION AND BENEFITS        Pay range    Data EngineerLevel I 120,000.00 - 145,000.00per year    Data EngineerLevel II 140,000.00 - 170,000.00per year    
Your actual level and base salary will be determined on a case-by-case basis and may vary based on the following considerations job-related knowledge and skills, education, and experience.
Base salary is just one part of your total rewards package at SpaceX. You may also be eligible for long-term incentives, in the form of company stock, stock options, or long-term cash awards, as well as potential discretionary bonuses and the ability to purchase additional stock at a discount through an Employee Stock Purchase Plan. You will also receive access to comprehensive medical, vision, and dental coverage, access to a 401k-retirement plan, short  long-term disability insurance, life insurance, paid parental leave, and various other discounts and perks. You may also accrue 3 weeks of paid vacation  will be eligible for 10 or more paid holidays per year. Exempt employees are eligible for 5 days of sick leave per year.
ITAR REQUIREMENTS

To conform to U.S. Government export regulations, applicant must be a i U.S. citizen or national, ii U.S. lawful, permanent resident aka green card holder, iii Refugee under 8 U.S.C.  1157, or iv Asylee under 8 U.S.C.  1158, or be eligible to obtain the required authorizations from the U.S. Department of State. Learn more about the ITAR here. 

SpaceX is an Equal Opportunity Employer; employment with SpaceX is governed on the basis of merit, competence and qualifications and will not be influenced in any manner by race, color, religion, gender, national originethnicity, veteran status, disability status, age, sexual orientation, gender identity, marital status, mental or physical disability or any other legally protected status.
Applicants wishing to view a copy of SpaceXs Affirmative Action Plan for veterans and individuals with disabilities, or applicants requiring reasonable accommodation to the applicationinterview process should notify the Human Resources Department at 310 363-6000. 

"
https://startup.jobs/data-engineer-gemini-4505030,Engineer,Data Engineer,Gemini ,"Gurugram, India",,"

About the Company
Gemini is a global crypto and Web3 platform founded by Tyler Winklevoss and Cameron Winklevoss in 2014. Gemini offers a wide range of crypto products and services for individuals and institutions in over 70 countries.
Our flagship product, the Gemini Exchange, was built to be a compliant and secure platform to buy, sell, and store crypto. Our suite of retail products includes ActiveTrader, a high-performance platform for advanced traders. Gemini also offers the Gemini Credit Card providing real-time crypto rewards, the Gemini dollar GUSD, a U.S. dollar-backed stablecoin, and Gemini Staking, allowing users to securely stake their tokens on-chain and receive rewards. Nifty Gateway, Geminis NFT platform, is the worlds premier marketplace for NFTs and digital art.
Gemini customers also have access to a wide range of institutional products tailor-made for high-net-worth individuals, asset and wealth managers, and hedge funds and liquidity providers seeking exposure to crypto. Customers looking to place large orders can use Gemini eOTC, a fully-electronic over-the-counter trading platform built for high-value bulk orders. For wealth management professionals, we offer a unique destination for their clients crypto portfolios from a single platform, and we enable fully electronic clearing and settlement of off-exchange crypto trades.
The Department Analytics
Data and analytics are central to all of our business functions and drive many of our most important decisions at Gemini. The Analytics team is responsible for data architecture, data engineering, business intelligence, machine learning, and data governance functions that shape the way data is stored and leveraged across Gemini. Data engineers and Machine Learning engineers make up the Analytics team are responsible for building the primary decision support system that derives continuous value by enabling individuals and various functional groups to make data driven informed decisions via our reliable data processes, data products and advanced analytics ability. The projects executed by the team cover a wide-range of topics including user acquisition and customer journey, cryptocurrency performance, product analytics, order book analytics, risk analytics, enabling automated and scalable blockchain based reconciliation systems, building predictive models all the way to enabling anomaly and fraud detection.
The Role Data Engineer
As a member of our data engineering team, youll deliver high quality work while solving challenges that impact the whole or part of the teams data architecture. Youll update yourself with recent advances in Big data space and provide solutions for large-scale applications aligning with teams long term goals. Your work will help resolve complex problems with identifying root causes, documenting the solutions, and implementing Operations excellence Data auditing, validation, automation, maintainability in mind. Communicating your insights with leaders across the organization is paramount to success.
Responsibilities

Design, architect and implement best-in-class Data Warehousing and reporting solutions
Lead and participate in design discussions and meetings
Mentor data engineers and analysts
Design, automate, build, and launch scalable, efficient and reliable data pipelines into production using Python
Build real-time data and reporting solutions
Design, build and enhance dimensional models for Data Warehouse and BI solutions
Research new tools and technologies to improve existing processes
Develop new systems and tools to enable the teams to consume and understand data more intuitively
Partner with engineers, project managers, and analysts to deliver insights to the business
Perform root cause analysis and resolve production and data issues
Create test plans, test scripts and perform data validation
Tune SQL queries, reports and ETL pipelines
Build and maintain data dictionary and process documentation

Minimum Qualifications

5 years experience in data engineering with data warehouse technologies
5 years experience in custom ETL design, implementation and maintenance
5 years experience with schema design and dimensional data modeling
Experience building real-time data solutions and processes
Advanced skills with Python and SQL are a must
Experience with one or more MPP databasesRedshift, Bigquery, Snowflake, etc
Experience with one or more ETL frameworks Custom, DBT, Databricks, etc
Strong computer science fundamentals including data structures and algorithms
Strong software engineering skills in any server side language, preferable Python
Experienced in working collaboratively across different teams and departments
Strong technical and business communication

Preferred Qualifications

Spark, HDFSS3, Messaging, Cloud computing especially AWS experience is a plus
Experience with orchestration frameworks like Airflow and continuous integration and deployment
Knowledge and experience of blockchain, financial markets, banking or exchanges
Knowledge of working with BI applications TableauLookerPower BI, etc
Web development skills with HTML, CSS, or JavaScript

It Pays to Work Here
 
The compensation  benefits package for this role includes

Competitive base salary
Benefits
Discretionary annual bonus
Discretionary equity

At Gemini, we strive to build diverse teams that reflect the people we want to empower through our products, and we are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, or Veteran status. Equal Opportunity is the Law, and Gemini is proud to be an equal opportunity workplace. If you have a specific need that requires accommodation, please let a member of the People Team know.
LI-AH1

"
https://startup.jobs/data-engineer-mama-money-4504780,Engineer,Data Engineer,Mama Money ,"Cape Town, South Africa",Full-Time,"

Who we are
Mama Money is a growth stage fintech startup working to help migrant workers in South Africa send money home. Since 2015 we have been providing reliable remittances, at a socially fair price, to those that need it most. 


Mama HQ is in beautiful Cape Town, South Africa. We are 122 people strong and counting from 19 countries across Africa, Asia, the UK, and Europe. Our culture is built around the well-being of our team members and making a difference in our communities. Just be lekker! personifies our approach to work we trust the wisdom of our talented and diverse team to do what is best for our customers and get the job done responsibly, without pedantic policies.  

Weve been around since 2015 and have earned a few accolades along the way, made some new friends, and expanded our reach. Weve had some great times and some tough times, but we continue to focus on helping people rather than maximizing profit. 

We are looking for a Data Engineer to join our dynamic team. This role entails the development, construction, testing, and maintenance of our data storage architecture, creating data systems and pipelines, and preparing data for prescriptive and predictive modeling.
As our Data Engineer you will 

Assist in the design, development, deployment, and management of Mama Moneys Data Lake and Data Warehouse solution on AWS.
Collaborate closely with the DevOps, Development and Data teams, providing data solutions for our migration to microservices.
Develop, construct, test, and maintain architectures.
Build infrastructure to automate high volumes of data delivery and creatively solving data volume and scaling challenges.
Contribute to the design and architecture of innovative solutions.
Align architecture with business requirements.
Engage in data acquisition and pipelining, including data extraction and transformation.
Identify opportunities to improve data reliability, efficiency, and quality.
Prepare data for predictive and prescriptive modelling.
Use data to discover tasks that can be automated

In your first 90 days at Mama Money, you will

Familiarize yourself with our data architecture and understand the scope and impact of your role.
Develop an understanding of our business model and operational strategies.
Engage with key stakeholders and the data team to get a grasp on ongoing data projects.
Begin contributing to the design and architecture of solutions.

About you

You have exposure to AWS infrastructure with Lambda, Glue, EC2  EMR.
You have 2 years of experience in a Data Engineering role, building and optimizing data pipelines, architectures and datasets.
You have constructed data acquisition, warehousing and reporting solutions.
You have extensive SQL knowledge for building optimal data extracts, query optimization.
You have Python experience, using it as the main programming language in our data infrastructure and buildingmaintainingimproving cloud solutions.
You have exposure to Terraform as an Infrastructure as Code IaC tool.
Experience in a Linux environment would be ideal.

It would be a bonus if you have experience with 

Experience with Tableau.
Exposure to version control systems.
A degree in Computer Science or related field.
Experience with dbt and AWS Athena.


Why Mama 
At Mama Money, the customers  communities we serve are at the heart of all that we do. 

We bring our authentic selves to work where we are free to express our diverse identities while staying true to what we believe in and how we feel. Part of that means being able to have difficult conversations when necessary but always remembering to do so with trust, respect, and a positive mindset. Being open to receiving feedback and understanding how this helps us evolve and grow is essential to succeeding in our environment. 

We are responsible for ourselves, our behaviour, our delivery, and our output. Although we always aim to move things forward and get hit done, we start small - taking it step by step and always looking for ways in which we can improve. We know from experience that when we work together towards a common goal, we can achieve anything. 

Mama Money is a special place because we care. We care about our customers, we care about the people who work for us and we care about the impact our work has on the world. There are things that we dont care about too. Things like bureaucracy, discrimination, ego, titles, and silos. Were a friendly bunch who dont take ourselves too seriously but we do take our work, and our customers hard-earned money, very seriously. 

Mamas values 
1. Customer  Community 
2. Authentic
3. Positive Mindset 
4. Responsible 
5. Togetherness 

"
https://startup.jobs/data-engineer-scribe-therapeutics-4504426,Engineer,Data Engineer,Scribe Therapeutics ,"Alameda, United States",,"

Scribe Therapeutics is a molecular engineering company focused on creating best-in-class in vivo therapies that permanently treat the underlying cause of disease. Founded by CRISPR inventors and leading molecular engineers Benjamin Oakes, Brett Staahl, David Savage, and Jennifer Doudna, Scribe is overcoming the limitations of current genome editing technologies by developing custom engineered enzymes and delivery modalities as part of a proprietary, evergreen CRISPR by Design platform for CRISPR-based genetic medicine.
 
We are seeking a highly creative, passionate, and motivated individual to join us in our quest to develop the next generation of CRISPR-based therapeutics. The current role is for a Data Engineer to join our team and advance our platform. The candidate should have a passion for working collaboratively with biologists and bioengineers to enable building the necessary tools for meeting the new frontier of CRISPR-based therapeutics. Additionally, the candidate would bring expertise in data engineering to work with Scribes fast-growing teams, contributing to the development of best practices of data solutions and playing a key role in data-driven gene editing innovation.  
 
The candidate will have numerous opportunities for professional growth in a rapidly growing biotechnology start-up, which includes growing into a leadership role of increasing responsibilities and the ability to publish highly impactful work in peer reviewed journals. 
 
Key Responsibilities


Design, construct, and maintain data systems and pipelines for efficient and reliable data ingestion, processing, and storage.


Develop and implement data integration and ETL Extract, Transform, Load processes to ensure data quality, accuracy, and consistency.


Build scalable and optimized databases, data warehouses, and data lakes to support the organizations data needs.


Implement and manage data governance practices, including data security, privacy, and compliance.


Automate data workflows, data validation, and data quality checks to ensure data accuracy and reliability.


Collaborate with data scientists and bioinformatics scientists to maintain code librariespipelines that implement and support Machine Learning workflows while integrating them with internal storage and metadata-management systems


Effectively collaborate with members within a fully integrated team to facilitate execution on projects within established timelines


Foster a driven, fast-paced, dynamic, and fun environment in which to do rigorous science


Required Skills and Background


Experience with big data and data science tools and development, as well as database and data management best practices


Strong proficiency in programming languages such as Python, Java, or Scala, as well as SQL for data manipulation and query optimization.


Familiarity with cloud platforms and services, such as AWS, Azure, or GCP, and their data-related offerings e.g., Amazon Redshift, Google BigQuery.


Experience with data integration and ETL tools such as Apache Spark, Apache Airflow, or Glue in AWS


Solid understanding of database systems, both relational e.g., PostgreSQL, MySQL and NoSQL e.g., MongoDB, Cassandra.


Knowledge of data modeling and database design principles to ensure efficient storage and retrieval of data.


Knowledge of data governance, data security, and compliance frameworks to ensure data integrity and privacy.


Familiarity with machine learning concepts and frameworks to support data science initiatives.


Ability to communicate and work effectively with internal software and data science teams, external IT contractors and members of other functional teams


At least 2 years of experience working in an academic or industry lab, with a proven track record of hands-on experience developing data engineering projects, databases, or data science tools.  


Demonstrated quantitative and scientific thinker as evidenced by a strong publication record


Ability to work both independently and collaboratively in a fast-paced, interdisciplinary research team


Preferred Skills and Background


B.S or M.S in Computer Science, Data Science, or related engineering fields


Familiarity with CRISPR technologies and therapeutic approaches


Familiarity with protein or RNA structure and engineering approaches


 
Salary will be commensurate with experience. We will provide an intellectually stimulating, collegial and fast-paced environment. If you are ready to engineer the future of therapeutics, then we are excited to hear from you! Visit us at www.scribetx.com.  
 
We are committed to creating a diverse environment and are proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status.
At the time of posting, the base pay wage range for this role is 80,000-130,000 per year.  The offered pay range will depend on internal equity and the candidates relevant skills, experience, qualifications, training, and market data.  Additional incentives are provided as part of the complete package in addition to comprehensive medical and other benefits.  


"
https://startup.jobs/data-engineer-london-mmob-4501407,Engineer,Data Engineer - London,mmob ,"Canary Wharf, United Kingdom",Full-Time,"

At mmob, were on a mission to make embedded finance technology accessible. mmob is driving customer acquisition and engagement in the Fintech space, an achievement recognised by the winning of a handful of industry awards in 2022, including Best Fintech Startup.
mmob integrates the products of service providers seamlessly in the channels of digital brands, enabling our clients to unlock new revenue streams and improve user retention.
mmob is looking for a Data Engineer to drive impact within our purpose-driven start-up. The Data Engineer will have the chance to revamp mmobs data pipeline, create the mmob data lake and work closely with our product team to surface insightful statistics on mmobs customers. The Data Engineer will gain a real insight into how a platform like mmob scales into a global platform, including cross datacentre operations. 

What you will do
 Refactor, iterate and improve the mmob data pipeline Work with big data to build infrastructure, pipelines, systems and tools Create accurate and accessible data sets for stakeholders to interpret Maintain and test data architecture and systems Detect potential issues and suggest improvements to the way data is collected Develop algorithms and data analysis tools to detect patterns and trends Query large datasets to answer specific questions for the different business units Bring knowledge of data to a variety of stakeholders, some less technical, and offer guidance and support Keep databases safe from threats and eliminate vulnerabilities in the data system Understand best practice for data maskingdata PII deletion 
Requirements
 Strong background in Computer Science or Information Systems 3 years in a Data Engineer role or Software Engineer role working with data Experience gained with in a data driven start-up or a business analytics environment Ability to build and optimise data pipelines, architectures and data sets Strong knowledge of SQL databases Experience with AWS solutions RDS, Redshift, S3 Understanding of ETL pipelines Familiarity of other data technologies such as Big Data, Cassandra, Python, R, Google Big Query Knowledge of industry best practice for data storage and security Ability to work and collaborate with cross functional teams in a dynamic environment Knowledge of ISO 27001 would be very useful 
Benefits
 A competitive salary, package and the ability to make your mark at an ambitious Fintech An amazing team, founders, and investors, backed by a supportive board committed to our vision of disrupting open finance Clear company direction, transparency and share of voice for all employees, championing those that drive value and push the business forward Commitment to culture, perks and more Commitment to personal development Flexibility with working hours, were focused on outputs, not hours worked Fun, modern and informal office environment Brand new technology, that works and hasnt been around the houses Access to impressive business leaders and external consultants 
About mmob
mmob is driving customer acquisition and engagement in the Fintech space. We specialise in empowering consumer-facing FinTechs to maximise their growth potential through partnerships.
Founded by experts in digital transformation from Barclays, HSBC, Goldman Sachs and Credit Suisse, we are on a mission to spur new growth and revenue verticals for fast growing Fintechs through a technology-first, complimentary partnerships infrastructure. By doing this we will introduce customers to new services that put them in control of their finance and enhance their financial wellbeing.
mmob is a people first company. We are ONE team with ambition, clarity and a thirst for learning.
LinkedIn and Twitter

"
https://startup.jobs/data-engineer-ii-rocket-lab-usa-4500174,Engineer,Data Engineer II,Rocket Lab USA ,"Long Beach, United States",,"


Rocket Lab
Rocket Lab is a vertically integrated provider of small launch services, satellites, and spacecraft components. Our mission is to open access to space to improve life on Earth. Our team is over 1,500 people strong and were adding to it every week. Collaboration is at our core  every idea is heard, and everyone makes a difference. Teams are nimble, decisions are made quickly, and we are action oriented.
Life at Rocket Lab


Data Engineer II
Based on site at Rocket Labs headquarters in Long Beach, CA. The Data Engineer, as part of the Business Intelligence team is responsible for solutioning and delivering robust data pipelines, ultimately streamline analytics and advanced data science. You will support the Business Intelligence Manager and Business IntelligenceIT organization, with deliverables required for internal business departments and analytics.
WHAT YOULL GET TO DO

You will work alongside an awesome team of Data Analysts, Software Engineers, Team Leads, Principal Data Engineers and other business functions to curate and shape datasets, develop data pipelines, to support Rocket and Satellite manufacturing.
Contributing to the development of Business Intelligence standards and our technology platform is an important part of the Business Intelligence Data Engineers responsibilities as is delivering streamlined data pipelines and assets.
Look after the data platforms to ensure analytics and pipelines are delivered on time.
Design and solution of re-usable data pipelines batch and stream using SQL and Python and other data programming languages.
Add value by technical knowledge in data and data science to propel the business intelligence team to the next level.

 
YOULL BRING THESE QUALIFICATIONS Must meet every point

3  years of hands-on work experience in data engineering and a bachelors degree in computer science  data and analytics  statistics or a related field.
3  years of MS SQL Server and ETL experience.
Experience querying and ingesting data from APIs and building APIs.
At least 1 year of experience with Python.
Knowledge in building and deploying pipelines alongside any other cloud ETL tools.

 
THESE QUALIFICATIONS WOULD BE NICE TO HAVE

Excellent communication skills, both verbal and written.
Excellent problem solving and analysis skills.
Cloud computing and deployment experience.
Experience with Data Science tools.
Experience implementing Machine Learning models.
Agile experience.

 

ADDITIONAL REQUIREMENTS

Ability to travel and communicate outside of work hours.
Must be able to work extended hours andor weekends as needed.



The expected salary range for the position is displayed in accordance with the California Equal Pay for Equal Work Act. Final agreed upon compensation is based upon individual qualifications and experience. In addition to base salary, Rocket Lab offers a comprehensive total compensation package based on individual qualifications and experience, which may include cash or equity incentives. Employees will also be eligible for medical, dental, vision coverage, 401k retirement plan options, and to purchase discounted stock through Rocket Labs Employee Stock Purchase Program.
Base Pay Range CA Only

95,000107,700 USD



Important information
FOR CANDIDATES SEEKING TO WORK IN US OFFICES ONLY
To conform to U.S. Government space technology export regulations, applicant must be a U.S. citizen as defined by ITAR 22 CFR 120.15 or eligible to obtain the required authorizations fromthe U.S Department of State.
Rocket Lab provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment at Rocket Lab, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
Applicants requiring a reasonable accommodation for the applicationinterview process for a job in the United States should contact Giulia Biow at g.biowrocketlabusa.com.This dedicated resource is intended solely to assist job seekers with disabilities whose disability prevents them from being able to applyinterview. Only messages left for this purpose will be considered. A response to your request may take up to two business days.
 
FOR CANDIDATES SEEKING TO WORK IN NEW ZEALAND OFFICES ONLY
For security reasons background checks will be undertaken prior to any employment offers being made to an applicant.  These checks will include nationality checks as it is a requirement of this position that you be eligible to access equipment and data regulated by the United States International Traffic in Arms Regulations. 
Under these Regulations, you may be ineligible for this role if you do not hold citizenship of Australia, Japan, New Zealand, Switzerland, the European Union or a country that is part of NATO, or if you hold ineligible dual citizenship or nationality.  For more information on these Regulations, click here ITAR Regulations.
 


"
https://startup.jobs/data-engineer-veevasystems-4500151,Engineer,Data Engineer,Veeva Systems ,"Lisbon, Portugal",Full-Time,"

Veeva Systems is a mission-driven organization and pioneer in industry cloud, helping life sciences companies bring therapies to patients faster. As one of the fastest-growing SaaS companies in history, we surpassed 2B in revenue in our last fiscal year with extensive growth potential ahead.

At the heart of Veeva are our values Do the Right Thing, Customer Success, Employee Success, and Speed. Were not just any public company  we made history in 2021 by becoming a public benefit corporation PBC, legally bound to balancing the interests of customers, employees, society, and investors.

As a Work Anywhere company, we support your flexibility to work from home or in the office, so you can thrive in your ideal environment.

Join us in transforming the life sciences industry, committed to making a positive impact on its customers, employees, and communities.

The Role

Veeva OpenData supports the industry by providing real-time reference data across the complete healthcare ecosystem to support commercial sales execution, compliance, and business analytics. We drive value to our customers through constant innovation, using cloud-based solutions and state-of-the-art technologies to deliver product excellence and customer success. The OpenData Global Data Tools team delivers the tools and data processing pipelines to build the global data core for life sciences in 100 countries.
 
As a Data Engineer of the Global Data Tools team, you will take responsibility for the OpenData processing workflows. You will be developing complex algorithms, building and maintaining data relationships through data processing pipelines, ensuring data quality in our reference data, and working closely with Product and Data Operations teams to adapt our reference data to changing demands in the market.
What Youll Do

Build and maintain data processing pipelines using state-of-the-art technologies
Develop algorithms to build complex data relationships
Work with Python on Spark-based data pipelines
Build analytical data structures to support reporting tools
Automate quality control processes

Requirements

Fluent in Python programming language and PySpark 3 years of experience
3 years of experience developing data pipelines using cloud-managed Spark clusters e.g., AWS EMR, Databricks
Proficient with SQL  SparkSQL
Experienced algorithm developer
Hands-on experience working with a Data Lakehouse

Nice to Have

Experience running data workflows through DevOps pipelines
Develop data pipelines with cloud orchestration tools e.g., Airflow, AWS Steps, or similar from other cloud vendors
Valuable experience working with Scala or Kotlin programming languages
Previous experience in the Life Sciences sector

Perks  Benefits

Benefits package including Restricted Stock Units RSUs, family health insurance, and contributions to private pension plans
Annual allocations for continuous learning, development  charitable contributions
Fitness reimbursement
Work from anywhere


RemotePortugal

Veevas headquarters is located in the San Francisco Bay Area with offices in more than 15 countries around the world.

Veeva is committed to fostering a culture of inclusion and growing a diverse workforce. Diversity makes us stronger. It comes in many forms. Gender, race, ethnicity, religion, politics, sexual orientation, age, disability and life experience shape us all into unique individuals. We value people for the individuals they are and the contributions they can bring to our teams.

"
https://startup.jobs/data-engineer-remote-fender-4499746,Engineer,Data Engineer - Remote,Fender ,,,"

Fender Musical Instrument Corporation ""FMIC"" is looking for a Data Engineer for our Ensenada, Mexico location. 
An American icon, Fender was born in Southern California and has built a worldwide influence extending beyond the studio and the stage. A Fender is more than an instrument; its a cultural symbol that resonates globally.
The Data  Analytics team is focused on transforming our operations to be a data-first and data-driven organization. Our mission is to evolve, grow and equip the enterprise by keeping the end user in mind while making data products reusable, scalable, and sustainable.  
We are looking for a Data Engineer to join our growing Data and Analytics team. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent and scalable throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our companys data architecture to support our next generation of products and data initiatives.
What youll do
      Collaborate with partners and product managers to design requirements and track plans.
      Develop data tools for the analytics team, facilitating product optimization.
      Define data storage, integration, and management for different systems.
      Document, cleanse and transform unstructured data.
      Design and implement data transformation processes.
      Build data pipelines and process data in distributed data stores.
      Apply best practices for development and performance.
      Monitor system performance and troubleshoot issues.
      Integrate various data sources for centralized maintenance.
      Create conceptual, logical, and physical data models.
      Translate product specifications into scalable, reusable, and maintainable solutions.
      Ensure real-time analytics availability and proactively address issues.
What You Need
        3 years of experience in a Data Engineer role
        BABS degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field MSPhD or equivalent a plus. 
        Experience in root cause analysis of dataprocesses to address business questions and identify opportunities for improvement. 
        Be able to work independently as a Data SME for a line-of-business or area, being responsible for the data processes and data quality of that area, while still contributing and working cooperatively within Data Engineering and the data platform.
        Strong communication skills to convey requirements, address questions, and explain technical issues to non-technical stakeholders are essential.
        Experience in operational support and monitoring of data and data pipelines from collection to the final data product.
        Excellent understanding and working knowledge of SQL and Python 
        Experience building and optimizing big data pipelines, architectures, and data sets.
        Experience with both data lakes and data warehouses, as well as underlying file formats for data lakes like Parquet and file partitioning in S3.
        Experience with relational SQL and NoSQL databases, including Postgres, DynamoDB, and Redshift.
        Knowledge of data pipeline and workflow management tools such as Airflow, AWS Step Functions, UC4 Data automation, and others.
        Experience with AWS cloud services EC2, EMR, RDS, Redshift, Glue, Kinesis and Lambdas.
        Experience with streaming data processing systems a plus Kafka, Kinesis, Storm, Spark-Streaming, etc.
        Experience with defining and implementing naming standards, abbreviations, guidelines, and best practices.
        Experience in 3NF, star, and snowflake schema design and development
        Knowledge of relational and dimensional models, keys and constraints, and normalization and indexes
Nice to Have
      Experience
      documenting source to target mappings, data dictionaries, and general modeling documentation.
      carrying out projects in an Agile environment
      working with Alteryx and Tableau
      Functional knowledge of financial systems and processes
      Proficient with relational data modeling
      Understanding of Data Governance, data quality, metadata management, middleware, or privacy programs concepts
About Fender Musical Instruments
Fender Musical Instruments Corporation FMIC is one of the worlds leading musical instrument manufacturers, marketers and distributors, whose portfolio of brands includes Fender, Squier, Gretsch, Jackson, EVH, Charvel and Groove Tubes, among others. For more information, visit www.fender.com. Fender Musical Instruments Corporation is an equal opportunity employer and considers qualified applicants for employment without regard to race, gender, age, color, religion, disability, veterans status, sexual orientation, or any other protected factor.
 

"
https://startup.jobs/data-engineer-i-pagerduty-2-4499234,Engineer,Data Engineer I,PagerDuty ,,,"

PagerDuty is growing and we are looking for an experienced Data Engineer for our Data team in IT to manage and contribute to the software and services that we provide to our users. As a Data Engineer at PagerDuty, you will help lead the team responsible for designing, building, deploying, and supporting solutions for teams across PagerDutys growing global user base. You are scrappy, independent, and excited about having a big impact on a small but growing team.
 
Together with the other members of the Data Platform team, you will have the opportunity to re-define how PagerDuty, designs, builds, integrates, and maintains a growing set of software and SaaS solutions. In this role, you will be working cross-functionally with business domain experts, analytics, and engineering teams to re-design and re-implement our Data Warehouse models.
 
How You Contribute to Our Vision Key Responsibilities 


Translate business requirements into data models that are easy to understand and used by different disciplines across the company. Design, implement and build pipelines that deliver data with measurable quality under the SLA 


Partner with business domain experts, data analysts and engineering teams to build foundational data sets that are trusted, well understood, aligned with business strategy and enable self-service 


Be a champion of the overall strategy for data governance, security, privacy, quality and retention that will satisfy business policies and requirements 


Own and document foundational company metrics with a clear definition and data lineage 


Identify, document and promote best practices


 
About You Skills and Attributes 


You will design, implement and scale data pipelines that transform billions of records into actionable data models that enable data insights.


You will help lead initiatives to formalize data governance and management practices, rationalize our information lifecycle and key company metrics. 


You will provide mentorship and hands-on technical support to build trusted and reliable domain-specific datasets and metrics. 


You will have deep technical skills, be comfortable contributing to a nascent data ecosystem, and building a strong data foundation for the company. 


You will be a self-starter, detail and quality oriented, and passionate about having a huge impact at PagerDuty.


 
Minimum Requirements 


Bachelors degree in Computer Science, Engineering or related field, or equivalent training, fellowship, or work experience 


1 years of experience working in data integration, pipelines, data modeling. 


Experience designing, deploying code in Data platforms in a cloud-based and Agile environment


Fluent English is required


 

Not sure if you qualify?
Apply anyway! We extend opportunities to a broad array of candidates, including those with diverse workplace experiences and backgrounds. Whether youre new to the corporate world, returning to work after a gap in employment, or simply looking to take the next step in your career path, we are excited to connect with you.
Where we work
PagerDuty currently has offices in Atlanta, Lisbon, London, San Francisco, Sydney, Tokyo, and Toronto, with remote opportunities in those cities and Chile. We offer a hybrid, flexible workplace, while also providing ample opportunities for in-person and virtual connection with your fellow Dutonians. 
How we work
Our values are deeply embedded in how we operate and the people we bring on board. You will see our values ingrained in how we support our customers, collaborate with our colleagues, develop our products and foster an inclusive and empathetic work culture.

Champion the Customer  Put users first to design great products and experiences.
Run Together  Build strong teams that amplify our impact on users.
Take the Lead  Disrupt and invent to be the first choice for users.
Ack  Own  Take ownership and action to deliver more efficiently to users. 
Bring Your Self  Bring your best self to build empathy and trust with users.

What we offer
One way we ensure our employees are inspired to do their best is through a comprehensive total rewards approach that supports them and their loved ones. As a global organization, our programs are competitive with industry standards and aligned with local laws and regulations. 
Your package may include

Competitive salary and company equity
Comprehensive benefits package from day one
Flexible work arrangements
ESPP Employee Stock Purchase Program
Retirement or pension plan
Paid parental leave - up to 22 weeks for pregnant parent, up to 12 weeks for non-pregnant parent some countries have longer leave standards and we comply with local laws
Generous paid vacation time
Paid holidays and sick leave
Paid volunteer time off - 20 hours per year
Bi-annual company-wide hack weeks
Mental wellness programs
Dutonian Wellness Days  Midyear Wellness Week - scheduled company-wide paid days off in addition to PTO and scheduled holidaysHibernationDuty - a week each year when everyone at PagerDuty, with the exception of a small, coverage crew, is asked to take a much needed break to truly disconnect and recharge

About PagerDuty
PagerDuty, Inc. NYSEPD is a leader in digital operations management. In an always-on world, organizations of all sizes trust PagerDuty to help them deliver a better digital experience to their customers, every time. Teams use PagerDuty to identify issues and opportunities in real time and bring together the right people to fix problems faster and prevent them in the future. Notable customers include Cisco, Cox Automotive, DoorDash, Electronic Arts, Genentech, Shopify, Zoom and more. 
Led by CEO Jennifer Tejada, two-thirds of the PagerDuty board is classified as non-white, with women making up nearly half of all board members. We strive to build a more equitable world by investing 1 each of company equity, product, and employee volunteer time.
PagerDuty is Great Place to Work-certified, a Fortune Best Place to Work for Women, and a top rated product on TrustRadius and G2. 
Go behind-the-scenes pagerdutylife on Instagram.
Additional Information
PagerDuty is committed to creating a diverse environment and is an equal opportunity employer. PagerDuty does not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, parental status, veteran status, or disability status.
PagerDuty is committed to providing reasonable accommodations for qualified individuals with disabilities in our job application process. Should you require accommodation, please email accommodationpagerduty.com and we will work with you to meet your accessibility needs.
PagerDuty uses the E-Verify employment verification program.


"
https://startup.jobs/data-engineer-ii-pagerduty-2-4499232,Engineer,Data Engineer II,PagerDuty ,,,"

PagerDuty is growing and we are looking for an experienced Data Engineer for our Data team in IT to manage and contribute to the software and services that we provide to our users. As a Data Engineer at PagerDuty, you will help lead the team responsible for designing, building, deploying, and supporting solutions for teams across PagerDutys growing global user base. You are scrappy, independent, and excited about having a big impact on a small but growing team.
 
Together with the other members of the Data Platform team, you will have the opportunity to re-define how PagerDuty, designs, builds, integrates, and maintains a growing set of software and SaaS solutions. In this role, you will be working cross-functionally with business domain experts, analytics, and engineering teams to re-design and re-implement our Data Warehouse models.
 
How You Contribute to Our Vision Key Responsibilities 


Translate business requirements into data models that are easy to understand and used by different disciplines across the company. Design, implement and build pipelines that deliver data with measurable quality under the SLA 


Partner with business domain experts, data analysts and engineering teams to build foundational data sets that are trusted, well understood, aligned with business strategy and enable self-service 


Be a champion of the overall strategy for data governance, security, privacy, quality and retention that will satisfy business policies and requirements 


Own and document foundational company metrics with a clear definition and data lineage 


Identify, document and promote best practices


 
About You Skills and Attributes 


You will design, implement and scale data pipelines that transform billions of records into actionable data models that enable data insights.


You will help lead initiatives to formalize data governance and management practices, rationalize our information lifecycle and key company metrics. 


You will provide mentorship and hands-on technical support to build trusted and reliable domain-specific datasets and metrics. 


You will have deep technical skills, be comfortable contributing to a nascent data ecosystem, and building a strong data foundation for the company. 


You will be a self-starter, detail and quality oriented, and passionate about having a huge impact at PagerDuty.


 
Minimum Requirements 


Bachelors degree in Computer Science, Engineering or related field, or equivalent training, fellowship, or work experience 


2  years of experience working in data integration, pipelines, data modeling, 


Experience designing, deploying code in Data platforms in a cloud-based and Agile environment


Fluent English is required


 

Not sure if you qualify?
Apply anyway! We extend opportunities to a broad array of candidates, including those with diverse workplace experiences and backgrounds. Whether youre new to the corporate world, returning to work after a gap in employment, or simply looking to take the next step in your career path, we are excited to connect with you.
Where we work
PagerDuty currently has offices in Atlanta, Lisbon, London, San Francisco, Sydney, Tokyo, and Toronto, with remote opportunities in those cities and Chile. We offer a hybrid, flexible workplace, while also providing ample opportunities for in-person and virtual connection with your fellow Dutonians. 
How we work
Our values are deeply embedded in how we operate and the people we bring on board. You will see our values ingrained in how we support our customers, collaborate with our colleagues, develop our products and foster an inclusive and empathetic work culture.

Champion the Customer  Put users first to design great products and experiences.
Run Together  Build strong teams that amplify our impact on users.
Take the Lead  Disrupt and invent to be the first choice for users.
Ack  Own  Take ownership and action to deliver more efficiently to users. 
Bring Your Self  Bring your best self to build empathy and trust with users.

What we offer
One way we ensure our employees are inspired to do their best is through a comprehensive total rewards approach that supports them and their loved ones. As a global organization, our programs are competitive with industry standards and aligned with local laws and regulations. 
Your package may include

Competitive salary and company equity
Comprehensive benefits package from day one
Flexible work arrangements
ESPP Employee Stock Purchase Program
Retirement or pension plan
Paid parental leave - up to 22 weeks for pregnant parent, up to 12 weeks for non-pregnant parent some countries have longer leave standards and we comply with local laws
Generous paid vacation time
Paid holidays and sick leave
Paid volunteer time off - 20 hours per year
Bi-annual company-wide hack weeks
Mental wellness programs
Dutonian Wellness Days  Midyear Wellness Week - scheduled company-wide paid days off in addition to PTO and scheduled holidaysHibernationDuty - a week each year when everyone at PagerDuty, with the exception of a small, coverage crew, is asked to take a much needed break to truly disconnect and recharge

About PagerDuty
PagerDuty, Inc. NYSEPD is a leader in digital operations management. In an always-on world, organizations of all sizes trust PagerDuty to help them deliver a better digital experience to their customers, every time. Teams use PagerDuty to identify issues and opportunities in real time and bring together the right people to fix problems faster and prevent them in the future. Notable customers include Cisco, Cox Automotive, DoorDash, Electronic Arts, Genentech, Shopify, Zoom and more. 
Led by CEO Jennifer Tejada, two-thirds of the PagerDuty board is classified as non-white, with women making up nearly half of all board members. We strive to build a more equitable world by investing 1 each of company equity, product, and employee volunteer time.
PagerDuty is Great Place to Work-certified, a Fortune Best Place to Work for Women, and a top rated product on TrustRadius and G2. 
Go behind-the-scenes pagerdutylife on Instagram.
Additional Information
PagerDuty is committed to creating a diverse environment and is an equal opportunity employer. PagerDuty does not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, parental status, veteran status, or disability status.
PagerDuty is committed to providing reasonable accommodations for qualified individuals with disabilities in our job application process. Should you require accommodation, please email accommodationpagerduty.com and we will work with you to meet your accessibility needs.
PagerDuty uses the E-Verify employment verification program.


"
https://startup.jobs/data-engineer-agile-six-applications-4499086,Engineer,Data Engineer,Agile Six Applications ,,,"

Agile Six is a people-first, remote-work company that serves shoulder-to-shoulder with federal agencies to find innovative, human-centered solutions. We build better by putting people first. We are animated by our core values of Purpose, Wholeness, Trust, Self-Management and Inclusion. We deliver our solutions in autonomous teams of self-managed professionals no managers here! who genuinely care about each other and the work. We know thats our companys purpose  and that we can only achieve it by supporting a culture where talented people feel valued, self-managed, and love to come to work.
The role
Agile Six is searching for a Data Engineer to join a team partnering with the Contention Classification Team at the Department of Veterans Affairs VA. The successful candidate will bring their experience in data engineering to help us expand our biomedical classification service. As part of the team, you will primarily be responsible for data cleaning and data management tasks, building data pipelines, and data modeling designing the schemastructure of datasets and relationships between data sets. In the future, youll likely have the opportunity to leverage your expertise in statistical and ML modeling to create a model that can be trained and scaled to classify contentions on more complex claims. We are looking for someone who enjoys working on solutions to highly complex problems and someone who is patient enough to deal with the complexities of navigating the Civic Tech space. The successful candidate for this role is an excellent communicator, as well as someone who is curious about where data analysis, backend development, data engineering, and data science intersect.
The project
This role is for the Contention Classification Team within the Automated Benefits Delivery Team at the Department of Veterans Affairs. Put simply, the better and more reliable our classification service works, the faster veterans can get the benefits they need. We are developing a service that classifies claims from veterans based on contentions ailments entered, to map to correct diagnostic codes and facilitate fewer errors in veteran claim processing.
Responsibilities

Contribute as a member of a cross functional Agile team using your expertise in data engineering, critical thinking, and collaboration to solve problems related to the project

Experience with Python, command line, and git are must-have skillsets for this project
Working with RubyRuby on Rails or within a Ruby environment is a nice-to-have skillset


Extract, transform, and load data. Pull together data sets, build data pipelines, and turn semi-structured and unstructured data into data sets that can be used for machine learning models
Eventually, help select, build, and train a highly complex machine learning model for classifying unstructured and semi-structured text
We expect the responsibilities of this position to shift and grow organically over time, in response to considerations such as the unique strengths and interests of the selected candidate, other team members, and an evolving understanding of the delivery environment

Basic qualifications

2 years of hands-on data engineering experience in a production environment
Experience with Python, command line, and Git RubyRuby on Rails is a plus
Demonstrated experience with extract, transform, load ETL and data cleaning, data manipulation, and data management
Demonstrated experience building and orchestrating automated data pipelines in Python
Experience with data modeling defining the schemastructure of data sets and the relationships between data sets
Ability to create usable data sets from semi-structured and unstructured data
Ability to prepare data for statistical modeling and an interest in building statistical and machine learning models including NLP
Solution-oriented mindset and proactive approach to solving complex problems
Ability to work independently on high complexity tasks
Experience working on an Agile team and demonstrating an Agile mindset
Have lived and worked in the United States for 3 out of the last 5 years

Additional desired qualifications

Familiarity with various machine learning ML algorithms and their application to common ML problems e.g. regression, classification, clustering
Experience with statistical modeling building and training different types of statistical models, evaluating and selecting models for a given problem
Experience with Ruby
Statistical experience or degree
Experience working with government agencies
You are a U.S. Veteran

Salary and Sixer Benefits
To promote equal pay for equal work, we publish salary ranges for each position.
The salary for this position is 107,558-113,074
Our benefits are designed to reinforce our core values of Wholeness, Self Management and Inclusion. The following benefits are available to all employees. We respect that only you know what balance means for your life and season. While we offer support from coaches, we expect you to own your wholeness, show up for work whole, and go home to your family the same. You will be seen, heard and valued. We expect you to offer the same for your colleagues, be kind not controlling, be caring not directive and ready to participate in a state of flow. We mean it when we say We build better by putting people first.
All Sixers Enjoy

Self-managed worklife balance and flexibility
Competitive and equitable salary equal pay for equal work
Tenure-based profit sharing
Employee Stock Ownership ESOP for all employees!
401K matching
Medical, dental, and vision insurance
Employer paid short and long term disability insurance
Employer paid life insurance
Self-managed and generous paid time off
Paid federal holidays and Election day off
Paid parental leave
Self-managed professional development spending
Self-managed wellness days

Hiring practices
Agile Six Applications, Inc. is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, national origin, ancestry, sex, sexual orientation, gender identity or expression, religion, age, pregnancy, disability, work-related injury, covered veteran status, political ideology, marital status, or any other factor that the law protects from employment discrimination.
Note We participate in E-Verify. Upon hire, we will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. This role is required to work from the contiguous United States. Unfortunately, we are unable to sponsor visas at this time.
If you need assistance or reasonable accommodation in applying for any of these positions, please reach out to careersagile6.com. We want to ensure you have the ability to apply for any position at Agile Six.
Please read and respond to the application questions carefully. Interviews are conducted on a rolling basis until the position has been filled.

"
https://startup.jobs/data-engineer-kyruus-4498863,Engineer,Data Engineer,Kyruus ,"Portland, United States",Full-Time,"

Kyruus mission is to connect people to the right care, in pursuit of our vision a better healthcare system- one thats transparent and accessible- where everyone gets the care they need. At Kyruus, our values are at the heart of everything we do 


We care deeply  We do the right thing even if its the harder thing. 


We are fiercely driven  We harness our curiosity to pursue continuous improvement and create simple solutions to complex problems.


We lead with respect  We celebrate the individual traits that make each of us unique and seek out diverse voices to listen and learn.


We are accountable  We do what we promise for each other and our customers.


Heres what that would mean for you in the Data Engineer role.
Care  You will strive for quality in everything you do and always try to deliver the best product possible.

Driven You will encounter new problems and approach them with curiosity and excitement, always eager to learn new things.

Respect You will give constructive feedback when needed, and accept it when given. Review and feedback is an integral part of the software development  process.

Accountable You understand we all make mistakes occasionally, and as long as we own them and fix them we can all move forward together.
What you will do in a Data Engineer role at Kyruus 

Collaborate with other internal teams to design data pipelines to ingest information from various systems to our BigQuery data warehouse.
Create data products that can be consumed by individuals in the form of reports, but also by applications in the form of curated data sets or data contracts.
Use the data warehouse to provide guidance and decision making assistance to internal and external stakeholders.
Provide feedback and participate in pair programming exercises.
Create dashboards and visualizations to monitor both the internal systems we own and our various customer facing products.

How You Can Grow
Kyruus will bring you through an onboarding process that is both structured and self-guided,  designed to enable connection and productivity as you learn more about our company, functions and products.  Additionally, we have a culture of feedback, inclusive of our performance review process that provides you with the coaching, resources and opportunities to help you learn and grow with us.

What you will bring

Two to four years experience developing data platforms using Python.
Experience creating and maintaining Extract, Transform, and Load ETL Pipelines.
Experience working with database and data warehouse technologies such as Postgres, Bigquery, and Redshift.
Experience creating data visualizations in tools like Datadog, Looker, Tableau, or other applicable data visualization and business intelligence tools.
Experience with cloud services such as AWS, Azure, GCP.
Experience in developing large scale data structures and streaming pipelines such as Apache Beam.
Plus Working experience in the US healthcare system, in particular with data systems and EHRs such as Epic, Cerner or Athenahealth.


Equal Opportunity Employer 

Kyruus is dedicated to providing equal employment opportunities EEO to all employees and applicants for employment without regard to race, color, religion, sex, gender, national origin, citizenship, age, disability, sexual orientation, genetic information. We will not discriminate, in any employment decision, against any individual or group on the basis of race, color, religion, sex, gender, national origin, citizenship, age, disability, sexual orientation, genetic information, or veteransnational guardmilitary reserve status. This shall be done in compliance with all applicable federal, state, and local laws in every location in which Kyruus has facilities. 

"
https://startup.jobs/data-engineer-pilytix-4497519,Engineer,Data Engineer,PILYTIX ,"Austin, United States",Full-Time,"

PILYTIX brings Explainable Artificial Intelligence XAI to sales and fundraising teams with software-as-a-service products that enable them to be more effective throughout their entire sales funnel. Data Engineers will assist in the development of PILYTIXs cutting-edge zeroG platform by creating and maintaining the data pipelines that ingest, transform and archive client data. They will also build internal tools to support our team of Data Scientists and Software Engineers, powering the PILYTIX products to help our clients win more revenue, faster.

Responsibilities
 Author and monitor directed acyclic graphs DAGs in Apache Airflow to ingest and transform data  Build and maintain internal Python packages to streamline ingest processes and add connections for new types of data Manage Kubernetes infrastructure as well as PostgreSQL and BigQuery databases on Google Cloud Platform Work collaboratively with the app development team to add new data-driven features to our software-as-a-service product Work collaboratively with the data science team to enhance our AI and machine learning capabilities Participate in Agile  Kanban processes on a daily basis Comply with change management policies and code reviews to ensure data integrity and system stability 
Requirements
 BSMS in a STEM field and 2 years of software industry experience programming and working with data Exceptional understanding of data architecture and software engineering best practices 2 years experience with Python Python 3 preferred 2 years experience with SQL PostgreSQL  BigQuery preferred 2 years of experience with Docker  1 years experience with cloud infrastructure GCP preferred 1 years of experience with server orchestration Kubernetes preferred Experience using Apache Airflow or similar data pipeline systems Experience using Git or other DVCS Knowledge of Agile  Kanban processes Entrepreneurial spirit and highly self-motivated 
Job is based in Austin TX, but extraordinarily qualified remote candidates willing to travel to Austin semi-regularly may apply.
Benefits
 Competitive base salary with ability to earn bonuses Professional development and entrepreneurial opportunities Paid time off 401k Medical and dental plans 

"
https://startup.jobs/data-engineer-aws-developer-kyivstar-4497455,"Developer,Engineer",Data Engineer (AWS developer),Kyivstar ,"Kyiv, Ukraine",Full-Time,"

  Data Engineer AWS Developer       .                    AWS
 
Requirements

 3 years of technical expertise in Database development, designing and building database solutions tables  stored procedures  forms  queries  etc.
Business intelligence knowledge with a deep understanding of data structure  data models to design and tune BI solutions 
3 years experience AWS services like EC2, S3, RedshiftSpectrum, Glue, Athena, RDS, Lambda, and API gateway;
Familiar with software DevOps CICD tools, such Git, Jenkins, Linux, and Shell Script;
Understanding of Spark, Hive, Kafka, Spark Streaming, and Airflow;
Experience in Software Engineering and Development;
Understanding of database schema design;
Advanced data analytics  designing and building solutions using AWS technologies 
Data formats knowledge and the differences between them
Experience with Hadoop stack
Experience with RDBMS andor NoSQL
Experience with Kafka
Experience with Java andor Scala andor python
Knowledge of version control system git or bitbucket
BI Tools experience PowerBI
Background in test driven development, automated testing and other software engineering best practices e.g., performance, security, BDD, etc.
DockerKubernetes paradigm understanding

English upper intermediate
Related AWS Certifications are a plus

Responsibility

Developing ETL flows based on AWS stack technology such as EC2, S3, edshiftSpectrum, Glue, Athena, RDS, Lambda, and API gateway 
Participate in efforts to design, build, and develop rapid Proof-of-Concept POC solutions and services;
Be a key team member in design and development of the product team;
Participate in sprint planning meetings and provide estimations on technical implementation;
Collaborate with the other engineering team members to ensure all services are reliable, maintainable, and well-integrated into existing platforms;
Review functional and technical designs to identify areas of risk andor missing requirements;
Troubleshooting and performance optimization for data processing flows, data models
Build and maintain reporting, models, dashboards 

 We offer

A unique experience of working the most customers beloved and largest mobile operator in Ukraine.
Real opportunity to ship digital products to millions of customers
To contribute into building the biggest analytical cloud environment in Ukraine
To create Big DataAI products, changing the whole industry and influencing Ukraine
To be involved in real Big Data projects with Petabytes of data and Billions of events daily processed in Real-time.  
A competitive salary.
Great possibilities for professional development and career growth.
Medical insurance.
Life insurance
Friendly  Collaborative Environment


  
   -- httpsmedium.comkyivstar-careers

      LinkedIn -- httpswww.linkedin.comcompanykyivstar



    ,     ,  c.           ! 

  ,      VEON,        ,  ,       VEON,    .      ,      ,   .               .     ,  ,   ,        .

 
 
More about us
Discover moments of awesomeness and our careers blog -- httpsmedium.comkyivstar-careers

For the latest updates follow us on LinkedIn -- httpswww.linkedin.comcompanykyivstar

 
Working in Kyivstar VEON Group demands a high standard of business ethics, adherence to our legal obligations, our values and our Code of Conduct of VEON Group and supporting compliance policies and procedures. We strive to be truthful and transparent, which requires us to act ethically, honestly, and with integrity. We understand the importance of the protection of your Personal Data and are committed to using good practices in how we handle such Data. By clicking on ""Apply for this job"", you confirm that you have read, understood and explicitly agree to our Applicants Privacy Policy.

"
https://startup.jobs/data-engineer-foodpanda-philippines-4496683,Engineer,Data Engineer,foodpanda Philippines ,"Taguig, Philippines",,"

""To be the most loved everyday food and groceries destination!"" - thats our mission at foodpanda.
foodpanda is the largest food and grocery delivery platform in Asia, outside of China. Operating in more than 400 cities across 11 markets, we continue to expand and grow in our core food delivery business, as well as in new verticals like grocery deliveries, with a strong tech infrastructure at our core. From our restaurants-partners, cloud kitchens and cloud grocery stores  foodpanda is just one tap away, getting everything you need into your hands quickly and conveniently!
We are looking for enthusiastic problem solvers to join us in scaling our platform, to digitalise businesses in Asia, uplift rider livelihoods and build a hyper-convenient platform for our customers. If you love working with technology to create solutions and are not afraid to roll up your sleeves to get things done, you will find your tribe here at foodpanda. Our diverse and high-performing team comprises people from more than 60 nationalities, and we welcome all experiences, backgrounds, and perspectives.
We are looking for a Data Engineer to join our growing business!
Responsibilities


Create and maintain optimal data pipeline architecture


Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery for timely availability, re-designing infrastructure for greater scalability


Communicating with different teams regarding data consistency and data availability.


Research new approach, solutions, and architecture that can contribute to the projects improvement and evolution


Requirements


1-2 years experience in data engineering.


Ability to write clean, structured, and high performance SQL and Python code.


Experience in using GIT


Strong experience with big data, Data Warehouse technologies such as Airflow, DBT, and Google Cloud Platform BigQuery, etc.


Strong oral and written communication skills.


Strong business mindset and ability to grasp business requirements from stakeholders.


Able to work and deliver with minimal supervision


Excellent time management skills


What we offer


foodpanda Philippines is a Great Place to Work Certified! You are assured to be joining a dynamic, fun, and an amazing work environment!


A company committed to developing you personally and professionally.


A great working atmosphere with regular company and team events.


A vibrant and international team committed to diversity and inclusion.


Responsibility from day one in a fast growing and global company.


We recognize top performers, welcome our newbies, and share good food!


Competitive package, allowances, food perks, Insurances, learning and development opportunities and more!


Our parent company, Delivery Hero, is a global leader in the food delivery industry processing over 3 million orders every day and operating in 40 markets in the world, with 18,000 employees and approximately 500,000 restaurant partners.

"
https://startup.jobs/data-engineer-edge-node-4495210,Engineer,Data Engineer,Edge & Node ,,,"

Edge  Node is a creative software development company working to build a vibrant, decentralized future. Founded by the initial team behind The Graph, Edge  Node is dedicated to the advancement of web3, a decentralized and fair internet where public data is available to allan internet that enables its users to increase agency over their creations and their lives.
Edge  Nodes initial product is The Graph, an indexing protocol for querying networks like Ethereum and IPFS, which ensures open data is always available and easy to access. The Graph is used by thousands of protocols and dapps including Uniswap, Livepeer, Aave, Decentraland, and more. Edge  Node also launched Everest, a decentralized registry with the mission to catalyze the shift to web3, facilitating community-driven curation of projects providing ongoing utility to the crypto space.
The Engineering Operations  Customer Success team works closely with all other Engineering teams across Edge  Node to ensure the services we operate are reliable, performant, secure, and predictable. We focus on a mix of software development, operational automation, cyber security, and collaboration with other teams to help take our service delivery to the next level.
We are looking for an early-career Data Engineer to be focused on developing and maintaining data science pipelines. Ideally, the team would like to bring on someone who has experience with the current tools being used by the team which include, but are not limited to, Redpanda, Materialize, and GCP. In this role, you will monitor and maintain reliability of the Redpanda cluster, streaming database, DBT jobs, QoS oracle, and other data engineering systems. Youll be expected to learn Materialize and help migrate BigQuery models to reduce costs. In addition, you will help establish and maintain good standards around documentation and internal educational tools and respond to data engineeringdevops requests in our incident management process.
What Youll Be Doing


Learning our infrastructure and data engineering toolset


Partnering closely with our Data Science team to perform various data warehouse jobs and periodic RedPandastreaming database devops tasks


Manage historical data models in BigQueryDBT


Develop pipelines to support dashboards and perform devops tasks to support Superset dashboards 


What We Expect


Experience with one or more of the following BigQuery, ETL automationworkflow tools DBT, BIdashboarding tools Apache Superset, streaming data platforms Apache Kafka, Redpanda, or Confluent, or other data engineering and data warehouse toolsetsenvironments 


Some experience or knowledge of container orchestration tools such as Kubernetes and Kustomize preferred


Some experience or knowledge of monitoring and alerting Grafana dashboards preferred


Some experience or knowledge of SQLable to create and manage tables within a SQL database


Proficiency in one or more programming languages, such as Python, R, or Rust 


Must be able to to serve on-call shifts and support devops needs


Ability to create documentation and communicate with a a variety of audiences


Clear communication skills written and verbal to document processes and architectures


Ability to work well within a multinational team environment


Preference to be physically located in The Americas, however the team is open to candidates in European time zones or other locations


About the Graph

The Graph is the indexing and query layer of web3. The Graph Networks self service experience for developers launched in July 2021. Developers build and publish open APIs, called subgraphs, that applications can query using GraphQL. The Graph supports indexing data from multiple different networks including Ethereum, NEAR, Arbitrium, Optimism, Polygon, Avalanche, Celo, Fantom, Moonbeam, IPFS, and PoA with more networks coming soon. To date, tens-of-thousands of subgraphs have been deployed on the hosted service, and now subgraphs can be deployed directly on the network. Over 28,000 developers have built subgraphs for applications such as Uniswap, Synthetix, KnownOrigin, Art Blocks, Balancer, Livepeer, DAOstack, Audius, Decentraland, and many others.
If you are a developer building an application or web3 application, you can use subgraphs for indexing and querying data from blockchains. The Graph allows applications to efficiently and performantly present data in a UI and allows other developers to use your subgraph too! You can deploy a subgraph to the network using the newly launched Subgraph Studio or query existing subgraphs that are in the Graph Explorer. The Graph would love to welcome you to be Indexers, Curators andor Delegators on The Graphs mainnet. Join The Graph community by introducing yourself in The Graph Discord for technical discussions, join The Graphs Telegram chat, and follow The Graph on Twitter, LinkedIn, Instagram, Facebook, Reddit, and Medium! The Graphs developers and members of the community are always eager to chat with you, and The Graph ecosystem has a growing community of developers who support each other.
The Graph Foundation oversees The Graph Network. The Graph Foundation is overseen by the Technical Council. Edge  Node, StreamingFast, Messari, Semiotic  and The Guild are five of the many organizations within The Graph ecosystem.


"
https://startup.jobs/data-engineer-detroit-lions-4492811,Engineer,Data Engineer,Detroit Lions ,"Detroit, United States",,"


ABOUT US 
The Lions, much like the city and community they represent, are built on a blue-collar work ethic that values hustle and conviction.  
Ford Field, the home of the Lions since 2002 in the heart of Detroit, is part of a vibrant sports and entertainment district featuring all four major professional sports franchises. Ford Field is also home to a dedicated and passionate multi-generational fan base reflective of the innovation, creativity and drive synonymous with Detroit.
Under the leadership of Sheila Hamp, the great granddaughter of Henry Ford, the Lions have ushered in an era of rebirth focused on creating an inclusive and equitable experience for employees, partners and fans. Building off the rich and diverse history of the city, the Lions embrace transparency and value contribution from all areas of the organization. We believe in the power of a pride, and acknowledge winning together takes understanding, acceptance, and teamwork. We believe in our we culture and are committed to building a new tradition that Detroit and Michigan can be proud of. 
One Pride. For All. 

The Data Engineer is responsible for developing and maintaining the Lions data infrastructure and architecture. This role is primarily responsible for the acquisition, storage, transformation and extraction of data for analysis.
 
ESSENTIAL FUNCTIONS
The Data Engineer position will have daily responsibilities including, without limitation, the following 

Build and maintain a data infrastructure to manage the companys analytic requirements
Develop, implement, and continuously troubleshoot ELT processes
Implement CICD best practices for software solutions utilized by the team
Engineer and develop cloud-based deployments of data pipelines, including automated deployments and job orchestration
Design data models and architecture to support analytical processes, working with Strategy  Analytics team and administrators of business systems to identify efficiencies
Develop processes ensuring data governance standards, security, stewardship, lineage, and metadata management
Create strategies to resolve, clean, enrich, and unify data of various types from disparate data sources and subsequently surface unified data to business applications
Support analytics work done by the Advanced Analytics team by translating requirements into scalable technical solutions
Oversee all aspects of data management operations, including defining and resolving database problems, tuning for performance, and managing storage
Identify, obtain, and integrate new data sources 
Collaborate with IT department and other system administrators to implement vendor sourced software, configuring that software, customizing, and integrating it with internal systems, including the data warehouse
Ensure data compliance as dictated by organizational Privacy and Security Policies
Responsible for appropriate documentation development, including requirements, design, code documentation, test plans, and other related documents

QUALIFICATIONS  REQUIREMENTS 

Bachelors degree or higher in technical discipline computer science, engineering, etc.
5 years of experience with data engineering, ETL processes and database architecture
Demonstrated experience and understanding of AWS cloud data environments 
Experience with Snowflake or another data cloud solution
Experience with Confluence or another team workspace solution
Experience with administering a dbt Core solution

 Advanced Python programming knowledge and background

Proficiency in terraform is preferred
Experience implementing CICD via GitHub actions is strongly preferred
Experience with Docker and containerized application development
Experience or knowledge of ECS TasksServices
Experience writing and maintaining ETL processes which operate on a variety of structured and unstructured data sources
Experience in large scale distributed data processing and orchestration e.g. Airflow, Prefect, Step Functions
Previous experience using APIs, web services, and automating tasks
Experience working with Salesforce and Ticketmaster is a plus
Knowledge of BI, analytics, AI, and machine learning
Strong analytical and problem-solving skills
Strong interpersonal and communication skills written and verbal



TO APPLY

To apply, please submit a copy of your resume along with a cover letter detailing your interest and related experience to the position.
Due to the high volume or resumes received, we regret that we are unable to update candidates on the status of their application. Those selected for further consideration will be contacted. Please no phone calls or emails. 


"
https://startup.jobs/data-engineer-r01525523-brillio-4492240,Engineer,Data Engineer - R01525523,Brillio ,"Bengaluru, India",Full-Time,"

About Brillio 
Brillio is the partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Backed by Bain Capital, Brillio is one of the fastest growing digital technology service providers. We help clients harness the transformative potential of the four superpowers of technology - cloud computing, internet of things IoT, artificial intelligence AI, and mobility. Born digital in 2014, we apply Customer Experience Solutions, Data Analytics and AI, Digital Infrastructure and Security, and Platform and Product Engineering expertise to help clients quickly innovate for growth, create digital products, build service platforms, and drive smarter, data-driven performance. With delivery locations across United States, Romania, Canada, Mexico, and India, our growing global workforce of over 6,000 Brillians blends the latest technology and design thinking with digital fluency to solve complex business problems and drive competitive differentiation for our clients. Brillio was awarded Great Place to Work in 2021 and 2022

Data Engineer
Primary Skills
SQS, Macie, Amazon API Gateway, Athena, CloudFormation, CloudWatch, DMS, DynamoDB, EMR, Kinesis, Oozie, Open Search, Redshift, SCT, Spark - Scala 
Secondary Skills
AWS CodeBuild, AWS Development, CICD Pipeline, Databricks, PythonPySpark 
Specialization
AWS Data EngineerIng Advanced Lead Data Engineer 
Job requirements
Has 10 yrs of experience in Data Engineering Must be having strong working experience in AWS cloud and Databricks Should be able to drive the design and data architecture along with Emerald Architect to improve the design Should be very independent and Proactive in bringing in new ideas Must be responsible for offshore data team delivery and quality Should act as a point of contact for all of Brillios tasks 

 

Know what its like to work and grow at Brillio Click here


"
https://startup.jobs/data-engineer-earthlinktele-4491739,Engineer,Data Engineer,Earthlinktele ,"Baghdad, Iraq",Full-Time,"

This position is responsible for conducting data importing, data cleaning and manipulating, reporting information related to activities of business development role.

Job Duties
 Design, construct, install, test and maintain data management systems. Build high-performance algorithms, predictive models, and prototypes. Ensure that all systems meet the businesscompany requirements as well as industry practices. Integrate up-and-coming data management and software engineering technologies into existing data structures. Develop set processes for data mining, data modeling, and data production. Create custom software components and analytics applications. Research new uses for existing data. Employ an array of technological languages and tools to connect systems together. Installupdate disaster recovery procedures. Collect market Data.  Cleans and organizes companys data.  Ensure all business data are updated to provide when necessary.  present clear databases to the key stakeholders to support insights to business-critical decisions. 
Requirements
 BSc. degree in Software Engineering. A minimum 2 Years experience in data engineering. Technical Expertise Basic Software Engineering Lifecycle Problem Solving  System Thinking  Time Management  Meeting Deadlines Analytical  Critical Thinking Excellent Command of English and Arabic- Written and Spoken. Excellent computer skills and experience using spreadsheet and Software tools, such as Microsoft, SQL python. Solid understanding of business products and services in the ISP industry. 

"
https://startup.jobs/data-engineer-veeva-link-veevasystems-4491008,Engineer,Data Engineer - Veeva Link,Veeva Systems ,"Lisbon, Portugal",Full-Time,"

Veeva Systems is a mission-driven organization and pioneer in industry cloud, helping life sciences companies bring therapies to patients faster. As one of the fastest-growing SaaS companies in history, we surpassed 2B in revenue in our last fiscal year with extensive growth potential ahead.

At the heart of Veeva are our values Do the Right Thing, Customer Success, Employee Success, and Speed. Were not just any public company  we made history in 2021 by becoming a public benefit corporation PBC, legally bound to balancing the interests of customers, employees, society, and investors.

As a Work Anywhere company, we support your flexibility to work from home or in the office, so you can thrive in your ideal environment.

Join us in transforming the life sciences industry, committed to making a positive impact on its customers, employees, and communities.

The Role

Veeva Link supports the life sciences industry to connect with key people to improve research and care. It helps professionals to find the right people for, e.g., clinical trials, education programs, or advisory boards. This streamlined access helps to reduce the time-to-market of important drugs, conduct trials with the most relevant experts in the respective field, and spread information about new treatments to key people in the life science community. You can read more about Veeva Link on our product pages at httpswww.veeva.comproductsveeva-link.
 
As a data engineer, you focus on our data pipelines and take responsibility for a major part of the Link data processing platform. We value end-to-end ownership, which puts you into the sweet spot of finding, designing, and implementing improvements to the products data pipelines and adjusting them to changing demands of the market. 
 
You take responsibility for features and innovation using SOLID and clean software principles, take part in the architectural enhancement process and care for the quality of the outcome. Monitoring, metrics, and general observability are part of the feature design process. 
 
We hire the same role for different engineering domains Sourcing, Tagging, Matching, and Provisioning.
 
Depending on the engineering domain, you will focus on different aspects. We decide together which domains fit best for you.
What Youll Do

Work on Veeva Links next-gen Data Platform
Improve our current environment with features, refactoring, and innovation
Work with JVM-based languages or Python on Spark-based data pipelines
Operate ML models in close cooperation with our data science team
Experiment in your domain to improve precision, recall, or cost savings

Requirements

Expert skills in Java or Python
Experience with Apache Spark or PySpark 
Experience writing software for the cloud AWS or GCP
Speaking and writing in English enables you to take part in day-to-day conversations in the team and contribute to deep technical discussions 

Nice to Have

Experience with operating machine learning models e.g., MLFlow
Experience with Data Lakes, Lakehouses, and Warehouses e.g., DeltaLake, Redshift
DevOps skills, including terraform and general CICD experience
Previously worked in agile environments
Experience with expert systems

Perks  Benefits

Comprehensive benefits package 
Annual allocations for continuous learning, development  charitable contributions
Fitness reimbursement 
Veeva Work-Anywhere


RemotePortugal

Veevas headquarters is located in the San Francisco Bay Area with offices in more than 15 countries around the world.

Veeva is committed to fostering a culture of inclusion and growing a diverse workforce. Diversity makes us stronger. It comes in many forms. Gender, race, ethnicity, religion, politics, sexual orientation, age, disability and life experience shape us all into unique individuals. We value people for the individuals they are and the contributions they can bring to our teams.

"
https://startup.jobs/data-engineer-tempcover-rvu-4490960,Engineer,Data Engineer - Tempcover,RVU ,"Fleet, United Kingdom",Full-Time,"

Location Fleet - Hybrid 2 days per week in-office
At RVU we combine the close-knit and agile environment of a startup, with the know-how, technology and backing of a well-established company.
Our mission is to empower people to make confident decisions. With our unique set of brands, including Uswitch, Confused.com, Tempcover, Money.co.uk and Mojo, we have the power to reach millions of consumers and the technology to deliver a world class online experience for them.
Tempcover is at the forefront of the fast-growing world of short term insurance. Our mission is to make car insurance flexible, quick, and easy for drivers. So far, weve sold over 4.5 million policies that have helped drivers get where they need to go, but were not finished there. Were growing our team to help us continue in that mission.If you are extraordinary at what you do and want to be a part of a rapidly growing business at the cutting edge of the InsureTech industry, we would love to hear from you 
The role 
The Tempcover Data and Analytics team plays a vital role in bringing data, MI and Insight to life for internal  external stakeholders, partners and third-party agencies. As a Data Engineer youll focus on supporting the integration and maintenance of our cloud based data platform and iteratively developing BI data products and features that are key to the continued growth of Tempcover. Youll be hands-on making a real difference to our brand while following the right standards and processes. In the last 12 months the engineers have worked on a range of interesting projects to help us increased our understanding of customer touch points, product purchase propensities and channel investment strategies, including; integrating our data with a new CRM tool driving a step change in marketing performance, migrating Google Analytics platforms and building aggregations of customer data providing insight on our most valuable customers. Key role responsibilities include
 Integrating Tempcovers single customer view with third parties, including our CRM Braze and other marketing channels such as paid search and social. Working with the marketing team to help drive consumer personalisation and wider Tempcover initiatives. Proactively suggesting backlog items that add value to the Data teams effectiveness and quality of data Creating and maintaining optimal data pipeline architecture and evaluatingreviewingimplementing the services required for optimal ETL of data from various data sources. Identifying, designing, and implementing internal process improvements automating manual processes, optimising data delivery, planning for greater scalability Writing code that follows our patterns and practices, is fit for purpose and that is tested Verifying and validating data models, including reconciliation to operational systems  

What we look for 
 Demonstrated experience developing applications using most of the following ELT experience Data modelling MS SQL ServerAzure SQL Azure experience of Azure Data Factory, Logic Apps, Azure Functions, Azure DevOps REST APIs Programming experience .NETPython HTML, CSS, JavaScript   Experience working within a SCRUM agile development environment. A good understanding of development methodologies and design patterns Ability to investigate issues, and to define and follow through on their resolution Experience in building data integrations with CRM platforms would be advantageous 

Our commitment to you
At RVU we believe that we can be the change we wish to see in the world. We hold ourselves accountable to being open and inclusive teammates and community members. We embrace our differences and are committed to creating an inclusive environment that reflects the world we live in.
Benefits
We want to give you a great work environment. We want to contribute to both your personal and professional development and give you great benefits to make your time at Tempcover even more enjoyable. Some of these benefits include  Discretionary bonus based on personal and business performance Annual leave of 25 days plus holiday purchase scheme Group pension scheme with 3 employer pension contribution Private Healthcare Group Life Assurance Employee Benefit Scheme that offers perks and discounts with major retailers. 


LI-hybrid
LI-FF1

"
https://startup.jobs/data-engineer-fanatics-4487524,Engineer,Data Engineer,Fanatics ,"Los Angeles, United States",,"

Company Overview
Fanatics is building a leading global digital sports platform. The company ignites the passions of global sports fans and maximizes the presence and reach for hundreds of sports partners globally by offering innovative products and services across Fanatics Commerce, Fanatics Collectibles, and Fanatics Betting  Gaming, allowing sports fans to Buy, Collect and Bet. Through the Fanatics platform, sports fans can buy licensed fan gear, jerseys, lifestyle and streetwear products, headwear, and hardgoods; collect physical and digital trading cards, sports memorabilia, and other digital assets; and bet as the company builds its Sportsbook and iGaming platform. Fanatics has an established database of over 100 million global sports fans, a global partner network with over 900 sports properties, including major national and international professional sports leagues, teams, players associations, athletes, celebrities, colleges, and college conferences, and over 2,000 retail locations, including its Lids retail business stores. 
 
As a market leader with more than 18,000 employees, and hundreds of partners, suppliers, and vendors worldwide, we take responsibility for driving toward more ethical and sustainable practices. We are committed to building an inclusive Fanatics community, reflecting and representing society at every level of the business, including our employees, vendors, partners and fans. Fanatics is also dedicated to making a positive impact in the communities where we all live, work, and play through strategic philanthropic initiatives.


We are a startup building products to transform the trading card industry. However, we arent a startup in the traditional sense, though. While we are a small team building new products from the ground up, we also have the backing of Fanatics  the worlds largest sports merchandiser with over 900 sports relationships and more than 90 million reachable fans. We have exclusive licensing deals with the MLB, NFL and NBA and the products we build will be used by millions of trading cards fans from day 1. We are out to reinvent the trading card industry, which is a bold and ambitious mission.


We have big plans in this space and theres a ton to do - trading card data is multifaceted and pricing is largely driven by secondary market activities that are challenging to coalesce and get insights from. We want to transform the way decisions are made in our organization and across the industry, and data is one of the pillars that will enable us to do that.
 
Are you interested in trading cards and sports? Have you ever wondered what is it like to combine your passion for modern data technology  science and trading cards to revolutionize the trading cards industry? Fanatics Collectibles Data Engineering, Science, and Analytics Team is hiring experienced data engineers. You will be among the the founding members of our team. You will help build a world-class data engineering organization with a strong data and software engineering culture. You will also establish, advocate, and execute data strategy and technical capabilities that support the business and enable data-driven decisions. Ideally, you are passionate the trading card industry, sports, and importantly, data and technology. Our data engineers are required to partner very closely with other engineering and business teams and understand our business needs. You will own high impact technical decisions and lead hiring efforts to help build out a world class data engineering team.
Qualifications 

A computer science or computer engineering degree with 2 years of experience as a data engineer or software engineer focusing on building data infrastructure.
Strong computer science fundamentals and datasoftware engineering hygiene is a must.
Experience in developing and deploying a components of an end-to-end data infrastructure leveraging cloud technologies that ingests data from a range of data sources.
Experience in data modeling for relational databases and one of the following data technologies document, columnar, graph, and key-value data stores.
Proficiency in SQL, Python, and Java.
Familiar with SQL and NoSQL database technologies.
Familiar with modern cloud native data tech stacks
Excellent communication skills.
Experience with the following technologies is a plus 
Streaming data technologies, such as Kafka or Amazon Kinesis
CICD pipelines with cloud native tools
Container tech such as Docker and Kubernetes
Data and ML orchestration tools
JavaScript


Key Responsibilities include, but not limited to,

Working under the direction of senior-level data engineers, research, develop, deploy, monitor, and maintain an efficient, reliable, and secure cloud data infrastructure to enable data-driven decisions.
Help integrate 3rd-party vendor data solutions.
Collaborate with internal and external tech teams and stakeholdersbusiness partners.
Participate in on-call technical support.
Continuously learn data engineering tools and techniques and adopt best practices.
Follow the high standard for datasoftware engineering excellence set by the senior-level members of the team.
Develop a strong understanding of Fanatics Collectibles and the trading cards industry.
Help in the data engineering and science team recruitment process.

What does this mean as a member of the data engineering team?

We are fans-and-collectors-obsessed and product-focused. We work backward from the focus of fans and collectors to drive our technical work and the development of products physical and digital that will reach tens of million.
We are agile and move at warp speed. We work hard and are not afraid to try, experiment, and pivot as needed. We have a lot to do and are looking for folks who will drive projects to completion with a sense of urgency, but not at the expense of quality, scalability, and performance.
We think about scale. The trading card market is massive, and we will be the de facto entry point into the hobby.
Were pragmatic. We embrace new technologies if they add business value. 
We are passionate about trading cards, sports, and data. We take data seriously and work tirelessly to ensure their highest possible quality. We build data infrastructure to enable the seamless flow data from source to impactful actions.1 


Some indicators that this role may be a good fit for you

You feel comfortable speaking up and asking driving questions to get clarity on discussions with both technical and non-technical team members.
You are looking for high impact opportunities with lots of ownership - very little is defined here - youll need to work with stakeholders and a Product Manager counterpart to define objectives, drive technical direction to solve problems, breakdown work, and go after high value initiatives that will move the needle for the business.
You get excited about greenfield architecture and can make big decisions without getting bogged down in analysis paralysis.
You have experience mentoring developers and an interest in people management.

Additional indications that this is the right role for you

8 years of software engineering experience with at least 2 of those managing people.
Expertise in end-to-end implementations of modern data stacks.
Expertise in the current data landscape trends, patterns, technologies, tools, etc. and have delivered high impact projects using best practices.
You have experience leading high performing teams that solve challenging technical problems          



A computer science or computer engineering degree with 2 years of experience as a data engineer or software engineer focusing on building data infrastructure.
Strong computer science fundamentals and datasoftware engineering hygiene is a must.
Experience in developing and deploying a components of an end-to-end data infrastructure leveraging cloud technologies that ingests data from a range of data sources.
Experience in data modeling for relational databases and one of the following data technologies document, columnar, graph, and key-value data stores.
Proficiency in SQL, Python, and Java.
Familiar with SQL and NoSQL database technologies.
Familiar with modern cloud native data tech stacks
Excellent communication skills.
Experience with the following technologies is a plus 
Streaming data technologies, such as Kafka or Amazon Kinesis
CICD pipelines with cloud native tools
Container tech such as Docker and Kubernetes
Data and ML orchestration tools
JavaScript



Ensure your Fanatics job offer is legitimate and dont fall victim to fraud.  Fanatics never seeks payment from job applicants. Feel free to ask your recruiter for a phone call or other type of communication for interview, and ensure your communication is coming from a Fanatics or Fanatics Brand email address.  For added security, where possible, apply through our company website at www.fanaticsinc.comcareers


Tryouts are open at Fanatics! Our team is passionate, talented, unified, and charged with creating the fan experience of tomorrow. The ball is in your court now.

Fanatics is committed to responsible planning and purchasing RPP practices, working with its business partners across its global and multi-layered supply chain, to ensure that planning, sourcing, and purchasing decisions, along with other supporting processes, do not impede or conflict with the fulfillment of Fanatics fair labor practices.


NOTICE TO CALIFORNIA RESIDENTSAPPLICANTS In connection with your application, we collect information that identifies, reasonably relates to or describes you Personal Information. The categories of Personal Information that we collect include your name, government issued identification numbers, email address, mailing address, other contact information, emergency contact information, employment history, educational history, criminal record, and demographic information.  We collect and use those categories of Personal Information about you for human resources and other business management purposes, including identifying and evaluating you as a candidate for potential or future employment or other types of positions, recordkeeping in relation to recruiting and hiring, conducting criminal background checks as permitted by law, conducting analytics, and ensuring compliance with applicable legal requirements and Company policies. For additional information on how we collect and use personal information in connection with your job application, review our Candidate Privacy Policy-CA



"
https://startup.jobs/data-engineer-remote-in-spain-edpuzzle-2-4484897,Engineer,Data Engineer (Remote in Spain),Edpuzzle ,,Full-Time,"

About us

Have you always wanted to put your passion for education to use on the job? Would you like to work with an incredible team making an impact on learning around the world? If you answered yes, we cant wait to meet you!

And just who are we? Edpuzzle is a leading edtech company with offices in San Francisco and Barcelona and nearly 10 years of history helping teachers find and create exciting, interactive video lessons. Over 80 of U.S. schools and millions of teachers and students in 190 countries around the world are already using Edpuzzle to make education more equitable and engaging. 

About the role


Were looking for our next Data Engineer to join the Data  Analytics team in Spain. Youll be responsible for combining raw data from different sources to build, maintain, and monitor data systems, providing consistent and usable information for analytics and business decision-making.


Wondering what its like to work at Edpuzzle?

Picture a place where you can connect with your teammates, whether remotely or in person, whenever you need support. A place where one day youre helping shape one of the biggest edtech platforms in the world, and the next day youre doing a teambuilding activity with your coworkers. A place where everyone has been selected because theyre the best at what they do, and where your manager and team trust your decisions fully. A place where youre encouraged to learn and grow because education is the cornerstone of everything we do. Check out the job details below to see if Edpuzzle could be the right fit for you!


About the job

Identify valuable data sources and automate collection processes
Ensure data quality and integrity  continuously generate data-driven hypotheses and advocate for testing best practices, contributing to accelerating our testing cycles
Design, implement, and maintain a solid data lake and data warehouse that collects, stores, and processes data, focusing on scalability and reliability
Collaborate with the DevSecOps, engineering, and product teams
Build data systems and pipelinesBuild analytic systems
Productivize pipelines and predictive models 
Improve and maintain CICD pipelines



About you

You have 1-4 years of experience as a Data Engineer or in a similar role
Graduate degree in data or another quantitative field preferred; BScBA in Computer Science, Engineering, or a related field
You are able to work with big amounts of data and organize, store, and protect data
You have deep knowledge of Python and their most useful libraries
You can work with relational and non-relational databases
You are highly experienced working with AWS console
You have experience managing projects, handling multiple stakeholders, identifying constraints, and organizing processes to maximize efficiency



Bonus skills

Familiarity with Spark
Familiarity with Docker
Familiarity with JupyterHub
Familiarity with GitHub
Familiarity with JavaScript language
Knowledge of SalesforceKnowledge of Mixpanel
Knowledge of HubSpot 
or another amazing skill you bring to the table that we havent thought of yet!



 Whats it like to work remotely?

Work from the comfort of your own home
Use the Edpuzzle office as much or as little as you like
Meet with your manager and team via video calls on Google Meet or Slack 
Connect with coworkers via Slack with channels for work and for fun! 

What we offer

Competitive salary at  30,000 - 39,000 and a yearly salary review based on performance
Free private health insurance policy with AXA
2000 annually for meals
Flexible remuneration for childcare and transportation
24 paid holidays plus December 24th and 31st
Flexible working hours and reduced working time on Fridays
Remote-friendly Feel free to work 100 remotely or use the Barcelona office whenever you want, for the best of both worlds!
Free coffee, snacks, and drinks in the Barcelona office
Teambuilding events during working hours
Incredible opportunity to grow, learn, and build lifetime bonds with other passionate people




Edpuzzle is an equal opportunity employer that is committed to diversity and inclusion in the workplace. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws.

This policy applies to all employment practices within our organization, including hiring, recruiting, promotion, termination, layoff, recall, leave of absence, compensation, benefits, training, and apprenticeship. Edpuzzle makes hiring decisions based solely on qualifications, merit, and business needs at the time.

References from previous employers will be requested from candidates during the selection process. If youd like to be considered for this position, please apply below. We look forward to hearing from you!

"
https://startup.jobs/data-engineer-sustaincert-4483780,Engineer,Data Engineer,SustainCERT ,"Amsterdam, Netherlands",Full-Time,"


Are you ready to put your data skills to truly good use? Want to accelerate your career in an innovative and technology-focused organization? Join us at SustainCERT! 

 
 
Your role 

As a Data Engineer at SustainCERT, you will be responsible for designing, developing, and maintaining data pipelines, data products, and data infrastructure to support our organizations data mesh architecture, ensuring data quality and accessibility across the organization. 
 
You join the Data team at a thrilling time the team has already kicked off the work on a data mesh, and youll get to bring in your insights early on in this building process. You will collaborate with domain teams to identify, design, and develop data products and self-serve data capabilities within the data mesh framework.
 
At SustainCERT, we have a warm engineering culture that fosters psychological safety. Our team is collaborative and very curious, always trying to push the bar higher. We are driven by our mission of bringing scientific credibility to climate action.
 
Our team works largely in a remote fashion. For this role, you are ideally based in Luxembourg or the Netherlands. Applications from candidates based in other locations will be considered case by case. 
Your key tasks  

Design and develop scalable, high-performance data pipelines, and data integration solutions using Azure cloud services such as Azure Data Factory, Azure Databricks, Azure Synapse Analytics, and Azure Stream Analytics; 
Implement and maintain data mesh architecture, focusing on domain-oriented, self-serve data infrastructure that promotes data discoverability, access, and governance; 
Ensure data compliance with industry standards and data governance policies, including GDPR, and other relevant regulations;
Develop and maintain data models, metadata, and data catalog, while promoting a data-driven culture across the organization.  

You have these claims to fame  


3 years of experience in data engineering, data warehousing, with a focus on cloud-based technologies;
Proficiency in Azure cloud services, including Azure Data Factory, Azure Databricks, Azure Synapse Analytics, and Azure Stream Analytics; 
Solid understanding of data modeling, data management, data warehousing concepts, and data pipeline optimization techniques;
Proficiency in programming languages such as Python, C, R, or similar;
A Bachelors or Masters degree in Computer Science, Engineering, or a related field.

Bonus points for 

Experience with data mesh implementation, data platform development, or data product development;
Knowledge of containerization and orchestration technologies like Docker and Kubernetes;
Familiarity with data cataloging, data governance, and data lineage tools.


Who we are
At SustainCERT, we help quantify and report on the social and environmental impacts from a wide range of climate actions. Our role is to provide robust evidence of progress towards our collective sustainability goals and ensure climate pledges bring real, meaningful impact on the ground.   
We deploy technology to create the next generation of impact accounting and improve the way carbon emissions are measured, reported, and verified more simplicity, more affordability, more efficiency and always the best level of accuracy and credibility. We are on a mission to mainstream best-practice for the benefit of all  businesses, people, and the planet.  

Come join our team of mission-driven professionals with big ideas, tireless optimism, and the belief that our work can change the world! 

"
https://startup.jobs/data-engineer-remote-req-515-mindex-4480939,Engineer,Data Engineer - Remote (Req. #515),Mindex ,"Rochester, United States",Full-Time,"

Founded in 1994, Mindex is a software development company with a rich history of demonstrated software and product development success. Our three divisions  Cloud, SchoolTool, and Software Development  are all rapidly growing, and our employee base is close to 400. We are ranked the 1 Software Developer in the 2023 RBJs Book of Lists, the Best Software Developer in the RBJs 2022 Reader Rankings, and a 2022 Certified Great Place to Work.
Mindexs Software Development division is the go-to software developer for enterprise organizations looking to engage teams of skilled technical resources to help them plan, navigate, and execute through the full software product lifecycle.
We seek a Data Engineer to join our team and work with the Enterprise Reporting program. The Enterprise Reporting program writes and maintains the BI and Reporting framework and live reports. These reports are used by customers and staff to gain information and insight from the underlying enterprise and lines of business data. Many teams are working on reporting modernization initiatives that will rewrite the reporting platform in Micro Strategy. In addition to modernizing the platform, teams are migrating reports to the new platform while also maintaining the legacy reports that customers use today.
Duties and Responsibilities
 Creates design documentation and code independently for intermediate to complex modules. Follows documentation, architectural, and coding standards. Identifies code, process, andor standard inefficiencies and provides suggestions for improvement. Creates and executes automated unit tests, manual unit tests, and manual integration tests. Tracks and resolves intermediate to complex defects. Reviews and evaluates detailed business, functional and high level technical requirements. Acquires and utilizes business knowledge to support applications. Designs detailed technical components with high-level architecture, recognizes and understands use of design patterns for intermediate to moderately complex applications. Applies reusability and future state architectures. Participates in design and code reviews. Provides and confirms detailed tasks and estimates for assigned work items. Plans and conducts timely structured code reviews to enforce coding standards and effective systems interoperability. Identifies support issues, assists in troubleshooting activities, and provides technical assistance of medium to complex problems by determining the nature, impact, extent, and appropriate resolution of application issues identified in various environments. Merges code, builds and deploys application packages in the appropriate lower environments to facilitate testing by other departments. Effectively communicates and coordinates efforts with the release coordinator, developers, and various other personnel on the build and deployment process. Experience with Reporting Tools MicroStrategy, OBI, SSRS Ability to write and troubleshoot SQL Object Oriented experience with Java or Python is a plus 
Requirements
 Bachelors Degree in Business or Computer Science preferred Five years of experience in software development or equivalent combination of education and experience Microstrategy experience 
Benefits
 Health insurance Paid holidays Paid time off 401k retirement savings plan and company match with pre-tax and ROTH options Dental insurance Vision insurance Employer paid disability insurance Life insurance and ADD insurance Employee assistance program Flexible spending accounts Health savings account with employer contributions Accident, critical illness, hospital indemnity, and legal assistance Adoption assistance Domestic partner coverage 
Mindex Perks
 Tickets to local sporting events Teambuilding events Holiday and celebration parties 
Professional Development
 Leadership training License to Udemy online training courses 
Growth opportunities

"
https://startup.jobs/data-engineer-deptagency-4480778,Engineer,Data Engineer,Dept ,"Skopje, North Macedonia",,"

For our Skopje data team we are looking for a Data Engineer to help our clients create state-of-the-art and scalable data warehouse solutions. 
We consult, design and build big data solutions that fit our clients needs and business logic, all the way from tracking set up, analytics through to BI and machine learning use cases.
As part of the Data Management Team we consult clients of all sizes, from startups to enterprise, to help them get real business value out of their data working in project teams with awesome Project Managers, Data Consultants  and Data Analysts.
 
YOUR ROLE


Build data solutions according to cloud computing best-practices;


Consult on technical topics both internally and at clients;


Scope, sell and execute the implementation of elegant big data solutions;


Work closely with our business consultants and project managers to shape projects which are feasible in time and budget;


Embrace agile methodology, think iteratively and propose workable steps on the way to the final setup;


Besides client work, you support the development of internal projects and re-structure code for repeated and speedy deployment;


Share your knowledge within our team and exchange with other data engineering teams within DEPT-wide;


Explain the what and why of cloud-based solutions to both technical and non-technical audiences.


 
YOUR PROFILE 


Have a minimum of 3 years of experience in a data engineering role;


Solid, hands-on developing skill in Python and SQL;


Hands-on experience with cloud databases, such as R.B. Redshift, Snowflake, BigQuery, Synapse Analytics ;


Are proficient in setting up infrastructure with at least one major cloud computing provider, such as Amazon Web Services, Google Cloud, Microsoft Azure;


Knowledge of the open source data stack is appreciated;


BONUS Linux administration knowledge is a plus;

Fluent in written and spoken English.

You will work in a vibrant environment with clever minds that work for a variety of large global clients. Yes, we are eager to hire the most talented experts in the game but we are also looking for a perfect DEPT-fit. Someone who is eager to learn, inspires, strives towards a better world, takes the stage, a futurist at heart, and someone who will toast to a successful week with us. We strongly support diversity and are committed to creating an inclusive environment for all employees. Ultimately, we are looking for a unique individual who is able to strengthen our collective.
 
WE OFFER

An opportunity to work together with colleagues from all around the world Netherlands, UK, Germany, USA and many more;
Development of your skills even further via training, conferences and seminars;
A chance to grow and learn from day one;
A family-like, informal and rewarding company culture;
Friday drinks;
Legendary team outings, field trips and company weekends to Amsterdam - a chance to meet other people within DEPT!
Great fringe benefits, think laptop, additional private health insurance, paid parking, public transport, paid sporting activities like fitness, spa, swimming, bowling, and many more;
New and modern office near the city square.

 
 
About DEPT 
DEPT is a pioneering technology and marketing services company that creates integrated end-to-end digital experiences for brands such as Google, KFC, Philips, Audi, Twitch, Patagonia, eBay and more. Its team of 2,500 digital specialists across 30 locations on 5 continents delivers pioneering work on a global scale with a boutique culture. DEPT is committed to making a positive impact on the planet and since 2021 has been Climate Neutral and B Corporation certified. www.deptagency.com
 
At DEPT, we take pride in creating an inclusive workplace where everyone has an equal opportunity to thrive. We actively seek to recruit, develop, nurture, and retain talented individuals from diverse backgrounds, with varying skills and perspectives.
Not sure you meet all qualifications? Apply, and let us decide! Research shows that women and members of underrepresented groups tend not to apply for jobs when they think they may not meet every requirement, when in fact they do. We believe in giving everyone a fair chance to shine. 
We also encourage you  to reach out to us and discuss any reasonable adjustments we can make to support you throughout the recruitment process and your time with us.
Want to know more about our dedication to diversity, equity, and inclusion? Check out our efforts here.

"
https://startup.jobs/data-engineer-dashlane-4480716,Engineer,Data Engineer,Dashlane ,"Lisbon, Portugal",,"


About Dashlane
Dashlanes mission is to make security simple for millions of organizations and their people. We empower businesses of every size to protect company and employee data while helping everyone easily log in to the accounts they needanytime, anywhere. Over 17 million users and 20,000 businesses in 180 countries use Dashlane for a faster, simpler, and more secure internet. 
Our global team is united by a strong sense of community and passion for improving the digital experience of our users. Learn more about how we work, how we hire, and the benefits of being a Dashlaner in our Life at Dashlane page.

About the role
Dashlane is looking for a talented Data Engineer to optimize our data to enable our company to scale in the coming years. Inside our Data Engineering team, you will be designing pipelines and warehouses to model data from multiple sources that will allow analysts to derive business and product insights. Using AWS data tools and open-source technologies, such as Airflow and Airbyte, you will design and build our next-generation ETL pipelines and data products.
You will be based in Lisbon, with a flexible hybrid work schedule and English as your working language. We offer relocation support.
At Dashlane you will

Contribute to our real time clickstream stack, using AWS Lambda and Kinesis
Expand our third party acquisition capabilities through Airbyte, Hightouch or custom Airflow Dags 
Provide a dbt runtime for analysts and accompany them on building resilient transformations, keeping data governance in mind for our existing 620 models on our daily run
Contribute to our Airflow for orchestration environments 
Create or use data quality tools to ensure our stakeholders have an accurate picture of what is happening on the product and with our business
Handle the challenges that come with managing terabytes of data
Eventually, build the next data powered apps to keep the business secure and the insights accurate
Build a scalable infrastructure on AWS using Terraform 

Requirements

You have 3 years of experience as a data engineer, analytics engineer, or in a data-oriented software engineering role
You have 3 years of experience with a scripting language preferably Python 
You have 2 years experience designing and provisioning data systems from pipelines to databases

You have 2 years experience designing database andor datalake tables 
You are fluent in English

Were also looking for

You have a passion for sharing and communicating data insights to a broad audience with varying levels of technical expertise
You have experience administrating, ingesting, and monitoring data in data warehouses such as Amazon Redshift 
You are passionate about security and value users privacy 
You are eager to learn and be challenged continuously
You are a self-starter that can work in a fast-paced, distributed environment, as you will be collaborating with our New York and Lisbon teams


Diversity, Equity, Inclusion and Belonging at Dashlane
As a truly international companyfounded in France and distributed across France, US and PortugalDashlane thrives off diverse perspectives. We value all aspects of diversity gender identity, sexual orientation, ability, ethnic origin, social background, age, lifestyle, and more. We are committed to hiring a diverse community and fostering a culture where everyone is heard and belongs. See more about this here. 
Your interview experience 
To know what to expect once youve sent your application, read about how we interview and hire at Dashlane. Feel free to browse our blog to find more information about our product and how we work.


"
https://startup.jobs/data-engineer-tiger-analytics-4479568,Engineer,Data Engineer,Tiger Analytics ,"Toronto, Canada",Full-Time,"

Tiger Analytics is a fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning and AI. We are the trusted analytics partner for several Fortune 100 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent as we continue to build the best analytics global consulting team in the world.
We are seeking an experienced Data Engineer with expertise in Dataiku to join our data team. As a Data Engineer, you will be responsible for designing, building, and maintaining data pipelines, data integration processes, and data infrastructure using Dataiku. You will collaborate closely with data scientists, analysts, and other stakeholders to ensure efficient data flow and support data-driven decision making across the organization.
Requirements
 8 years of overall industry experience specifically in data engineering Proven experience as a Data Engineer, specifically working with Dataiku. Strong knowledge of data engineering principles, data integration, and data warehousing concepts. Proficiency in building and maintaining data pipelines using Dataiku. Solid understanding of ETL processes and tools. Strong programming skills in languages such as Python, SQL, or Scala. Experience with data modeling, data architecture, and database design. Familiarity with cloud platforms like AWS, Azure, or GCP. Excellent problem-solving and troubleshooting skills. Strong communication and collaboration abilities. Attention to detail and a focus on delivering high-quality work. 
Benefits
Significant career development opportunities exist as the company grows. The position offers a unique opportunity to be part of a small, challenging, and entrepreneurial environment, with a high degree of individual responsibility.

"
https://startup.jobs/data-engineer-citizengo-4476689,Engineer,Data Engineer,CitizenGO ,,,"

Responsibilities

Primarily responsible for developing and constructing data architectures, systems, and processing tools. This could involve creating and integrating APIs, developing data set processes, and dealing with Artificial intelligence AI and Machine Learning ML.
Tasked with maintaining systems to ensure their continual operation. They also work to improve the performance of these systems through optimization and troubleshooting.
Create, maintain, and improve databases. This could involve creating table schemas, improving data processing, and ensuring data solutions comply with advanced best practices, privacy and security regulations.
Maintain reports that effectively communicate trends, patterns, insights, recommendations, and predictions.
Create and manage ETL Extract, Transform, Load processes to pull data from various sources, transform it to a usable format, and load it into a more usable destination, such as a database or a data warehouse like Google Big Query.
Develop effective communication methods through asynchronous communication being able to work in an agile development environment.
Share knowledge through detailed documentation.

Requirements

2-3 years of experience as a Data Analyst, Data Scientist andor Data Engineer.
Knowledge of SQL tables, views, queries, stored procedures, triggers at the master level.
Understanding data modelling concepts, such as designing efficient database schemas, creating appropriate data structures, and optimizing table partitioning.
Knowledge of Extract, Transform, Load ETL tools and processes is essential. Master Google Cloud Run is a plus.
Proficient in Python.
Solid command of SQL; heshe can find the most efficient way to write a SELECT statement and possess experience with relational databases principles.
Experience using analytical tools like Google Analytics 4.
Adopt modern techniques to ensure data protection, privacy and security.
Proficiency with data visualization tools like Looker and data warehousing software like Google Big Query.
Experience constructing data system pipelines for Artificial intelligence AI and Machine Learning ML.
Familiarities with Cloud Services such as the Google Cloud Platform and Google Big Query in particular.
Excellent written and oral communication skills.
Passion and commitment to human dignity, life, family, liberty, citizen empowerment, and social change.
High professionalism, self-motivation, efficiency, and results-oriented delivery on short timelines.
Strong teamwork skills, comfortable in a highly collaborative team culture and a hierarchical team structure.
Strong project management skills.
Ability to adjust quickly to changing conditions and requirements.
Fluency in English is a requirement. Duolingo English Test is required for all non-natives with a minimum 100 B2 equivalent score. Spanish and other additional languages are an asset. Fluency in English and excellent communication skills, especially in writing.

Top 10 reasons to work for CitizenGO

Were winning. Our campaigning works and were changing politics and policies across the world. Check out some of our recent victories here. 
Our tactics are bold and effective. We do whats needed to win campaigns, whether its launching petitions, partnering with like-minded organizations, helping our members call the decision maker or rallying on the streets.
Large funders dont control our strategy. We work with members to do whats best for the world each and every time.
We thrive in risky situations. We dont let powerful forces like the Spanish Socialist Government or Soross empire bully us with legal, brand or financial threats. Were experts at navigating risk and meeting threats head on, in the media, the courts or on the streets.
Work with helpful, kind, motivated, and talented people.
Work remote so you have no commute and are free to travel and move.
Have flexible work hours so you are there for other people and free to plan the day how you like.
Everyone works remote, but you dont feel remote. We dont have a head office, so youre not in a satellite office.
Work on campaigns that make an impact so you can enjoy defending life, family and freedom and changing the world.
Focused on results, not on long hours, so that you can have a life and dont burn out.

See our culture page for more!
Work remotely from anywhere in the world. Curious to see what that looks like? Check out our remote manifesto.
Benefits
Vacation  Holidays Join CitizenGO and enjoy 24 vacation days per year and local public holidays. We also provide various types of leave, such as parental, sick, medical, and bereavement. Attractive Benefits Embrace the freedom of working remotely at CitizenGO, with flexible hours and a consistent 40-hour work week. Benefit from our tuition reimbursement program up to 500 per year, EnglishSpanish language courses USD120 per month on iTalki, and an annual Christmas gift. Each team member may receive one free book per month up to 30. Practical Assistance for Your Success We equip you with the necessary tools to excel in your role, including Chromebooks, mobile phone lines, and smartphones for select positions. Work comfortably with our home office setup allowance up to 500 every two years and coworking memberships up to 200 per month. Innovative Technology CitizenGO grants access to Copy.ai, an AI-powered copywriting platform, and Grammarly, designed to enhance the quality of your English writing. Engaging Meetups Connect and collaborate with your colleagues during our annual retreats, evaluation meetings, mini-retreats, conferences, summits, and monthly meetups in Madrid. 

Your Privacy
For information about our privacy practices in the recruitment process, please visit our Recruitment Privacy Policy page.

"
https://startup.jobs/data-engineer-jet-support-services-inc-4475510,Engineer,Data Engineer,"Jet Support Services, Inc. ","Chicago, United States",Full-Time,"

About JSSI   
For more than 30 years, Jet Support Services, Inc. JSSI, has been the leading independent provider of maintenance support and financial services to the business aviation industry. JSSI is responsible for maintaining in excess of 2,000 business jets, regional jets, and helicopters across the globe and serves customers through an infrastructure of certified technical advisors and providing maintenance tracking software Traxxall and SierraTrax. JSSI leverages this technical knowledge, experience, buying power, and data to provide support at every stage of the aircraft life cycle; from aircraft acquisition to aircraft teardown and part out.

For more information, visit jetsupport.com.

About this Position

We are looking for a Data Engineer who has experience in Data warehouses, Data Lakes, ETL tools, Database systems like NoSQL and SQL, Machine learning, Python, Scala, and Java programming languages, Data APIs and with an understanding of the basic concept of distributed systems and with strong knowledge on data structures and algorithm.

In this role, the Data Engineer will collaborate with stakeholders across the organization, including product management, operations, finance, marketing, and engineering, to identify, define and execute data engineering projects and initiatives, including but not limited to, designing and developing data warehouses, data lakes, ETL processes, data analysis scripts, models and visualizations. 

The ideal candidate will be naturally analytical and empathetic, able to naturally explain technical or complex concepts to stakeholders and committed to staying up to date with industry trends, emerging technologies, and best practices in data engineering, data science and aviation, and leverage this knowledge to drive innovation and improve existing processes.
Desired Credentials

8 Years of experience in Database technologies like OracleDB2MS SQL.
Knowledge and experience working with SQL Server databases, including advanced SQL and T-SQL
Strong SQL and database programming skills
Understanding of RDBMS databases, data analysis, database design, data warehousing concepts
Hands on experience with Azure Data Factory, Azure Data Lake, Azure Synapse and Power BI
Strong communication and collaboration skills 

Good to have
Knowledge of Salesforce
Basic knowledge of Shell scripting and Python.
Knowledge of any ETL and data warehousing tool or technology
Experience in Master Data Management MDM
Knowledge of Data Catalog and Data Governance tools, frameworks and processes
Prior experience in aviation industry

Certifications
Microsoft Certified Azure Data Fundamentals
Microsoft Certified Azure Data Engineer Associate
optional Microsoft Certified Power BI Data Analyst Associate
optional Microsoft Certified Azure AI Fundamentals


JSSI is an Equal Opportunity  Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or other characteristic protected by law.

"
https://startup.jobs/data-engineer-paymongo-4474238,Engineer,Data Engineer,PayMongo ,"Taguig, Philippines",Full-Time,"

PayMongo builds the most advanced online payments infrastructure in SE Asia, starting in the Philippines. Over the next few years, we will build a broad suite of products that aims to lower the barriers and remove all the complexities in payments. We are rapidly expanding to serve diverse needs of thousands of business. To continue on this exponential period of growth, we seek to build the best and the most diverse team of thinkers, doers and starters.

In this role, you will play a crucial role in shaping our data infrastructure and contributing to the success of our products. As a Data Engineer,
You will

Collaborate with the data engineering team to develop and maintain scalable data pipelines and ETL processes.
Design, implement, and optimize data models and databases to ensure data quality, integrity, and accessibility.
Work closely with software engineers and data analysts to identify and address data-related challenges and requirements. 
Assist in the development of data integration and transformation solutions to support business intelligence and analytics initiatives.
Develop and maintain data documentation, including data dictionaries, data lineage, and data flow diagrams.
Monitor and troubleshoot data pipelines to identify and resolve issues in a timely manner. 
Stay up-to-date with emerging technologies and industry trends related to data engineering and data management.

You may be a fit if

You possess a Bachelors degree in Computer Science, Engineering, or a related field.
You have strong understanding of relational databases and SQL.
You are proficient in at least one programming language such as Python, Java, or Scala. 
You are familiar with data modeling and database design concepts.
You have good experience with ETL Extract, Transform, Load processes and tools. 
You have good knowledge of data warehousing concepts and technologies.
You have good understanding of cloud platforms e.g., AWS, Azure, GCP and their data services. 
You possess excellent problem-solving skills and attention to detail. 
You possess strong communication and collaboration skills to work effectively in a team-oriented environment.
You are self-motivated and eager to learn new technologies and skills.
You have experience with Amazon Redshift, Metabase, and Airbyte. A plus but not a requirement.
You have good understanding of financial data and industry-specific requirements. A plus but not a requirement.


PayMongo is a financial technology company enabling businesses to receive payments online easily. We started the company in March 2019, and we are backed by Silicon Valleys most forward-thinking investors including Y Combinator, Peter Thiel, Founders Fund and Stripe. We are looking for people with passion, grit and the integrity to help us build the future of payments.

"
https://startup.jobs/data-engineer-connexity-4472722,Engineer,Data Engineer,Connexity ,"London, United Kingdom",Full-Time,"

Skimlinks, a Connexity and Taboola company, drives e-commerce success for 50 of the Internets largest online retailers. We deliver 2B in annual sales by connecting retailers to shoppers on the most desirable retail content channels. As a pioneer in online advertising and campaign technology, Connexity is constantly iterating on products, solving problems for retailers, and building interest in new solutions. We have recently been acquired by Taboola to make the first Open-Web Source for Publishers connecting editorial content to product recommendations, where readers can easily buy products related to stories they are reading.
Read more about how were changing the game here! Due to our explosive growth, we are seeking an experienced Data Engineer to help with a variety of large scale data challenges and build pipelines that can analyse the torrent of our data. Our ideal candidate enjoys a fast-paced environment, working with multiple, complex projects and teams with a positive, collaborative attitude. Projects we are working on currently include the building of a large scale product databases, a centralised system for reporting publishers revenue across their marketing channels to manage and develop data pipelines and ETL processes across Skimlinks.  About the role We are looking for a Data Engineer to join our team in London. We are creating a fundamentally new approach to digital marketing, combining big data with large-scale machine learning. Our data sets are on a truly massive scale - we collect data on over a billion users per month and analyse the content of hundreds of millions of documents a day. We use latest open source and Google Cloud technologies such as Apache Druid, Airflow, BigQuery, Apache Beam and are open to innovation. As a member of our Data Engineer team your responsibilities will include  Designing, building and implementing data engineering systems across all parts of our data platform; from high-volume data collection, scheduling, enrichment through to automated analysis of large datasets Working with our team of Product Managers and our commercial teams to understand the needs of the market and our customers; and how they can be translated into solutions Crafting innovative solutions to complex technical problems and making design decisions in line with our technical strategy and high engineering standards Defining our development environment, and communicate the best development practices within the organisation i.e. code reviews, testing, etc Sharing your knowledge across the business and mentor others in your areas of deep technical expertise 
Requirements
Here at Skimlinks we value dedication, enthusiasm, and a love of innovation. We are disrupting the online monetization industry, and welcome candidates who want to be a part of this ambitious journey. But it is not just hard work, we definitely appreciate a bit of quirkiness and fun along the way.
Were looking for a Data Engineer with the following
 Proven work experience with large data sets. An advanced degree BachelorMasters degree in computer science or a related field. Solid coding skills Python preferred. Youll have an interest in processing large datasets at scale we use BigQuery, Apache Druid, Dataflow and are a Python shop Ability to create, optimise and automate data pipelines SQL, Airflow, Star Schema. A commercial mindset, you are passionate about creating outstanding products Experience with DBT  Looker is a plus. 
Benefits
Voted Best Places to Work, our culture is driven by self-starters, team players, and visionaries. Headquartered in Los Angeles, California, the company operates sites and business services in the US, UK, and EU. We offer top benefits including Annual Leave Entitlement, paid holidays, competitive comp, team events and more!
 Healthcare insurance  cash plans Pension Parental Leave Policies Learning  Development Program educational tool Flexible work schedules Wellness Resources Equity 
We are committed to providing a culture at Connexity that supports the diversity, equity and inclusion of our most valuable asset, our people. We encourage individuality, and are driven to represent a workplace that celebrates our differences, and provides opportunities equally across gender, race, religion, sexual orientation, and all other demographics. Our actions across Education, Recruitment, Retention, and Volunteering reflect our core company values and remind us that were all in this together to drive positive change in our industry.

This position is hybrid 1dayweek or remote, and based in our London office. 
HP

"
https://startup.jobs/data-engineer-beyondtrust-4470637,Engineer,Data Engineer,BeyondTrust ,,,"

BeyondTrust is a place where you can bring your purpose to life through the work that you do, creating a safer world through our cyber security SaaS portfolio.
Our culture of flexibility, trust, and continual learning means you will be recognized for your growth, and for the impact you make on our success. You will be surrounded by people who challenge, support, and inspire you to be the best version of yourself.
The Role
As a Data Engineer at BeyondTrust, you will help us build massive-scale data systems from the ground up in order to process hundreds of millions of events per day. You will be a crucial part of launching our new Identity Threat Detection and Response solution as you work with internal and external customers to understand their use cases and find ways to make the data they need available to them.
What Will You Do

Define and improve the architecture of large-scale data systems
Create and maintain streaming and batch jobs to transform, load, and export data
Maintain a Databricks-powered data lakehouse
Maintain a real-time detection pipeline running against data aggregated in lakehouse
Process data and build out representations useful for particular purposes within lakehouse and other data stores, possible including graph databases

Skills You Need

Ingest large scale event streams and react to them in real time
Govern and engineer data systems to maintain regulatory compliance and respect customer preferences
Experience building data lakes or data lakehouses
Ability to work autonomously

Technologies You Should Know

Required Spark
Required Databricks, Snowflake, Presto, Redshift or similar
Required Proficient in either Python or Scala
Required Flink, Spark Streaming, Storm, Samza, or similar
Ideal Some experience with graph data stores

Who are you?

Smart, kind, flexible, determined
Passionate about data performance, quality and integrity
Enthusiastic about what you do and have a high interest in keeping up with current best practices in your areas of expertise
Write code that is understandable, simple, and clean
You have unwavering personal integrity and work ethic
You ask how we can rather than say we cant
You are a team player and excel individually as the situation dictates

Better Together
Diversity. Inclusion. Theyre more than just words for us. They are the guiding values of how we build our teams, cultivate leaders, and create a culture where people feel connected.
We take care of our employees so they can take care of our customers. Customers who come from all walks of life just like us. We hire incredible people from diverse backgrounds because when we are different together, we are stronger together.
About Us
BeyondTrust is the worldwide leader in intelligent identity and access security, empowering organizations to protect identities, stop threats, and deliver dynamic access to empower and secure a work-from-anywhere world. Our integrated products and platform offer the industrys most advanced privileged access management PAM solution, enabling organizations to quickly shrink their attack surface across traditional, cloud, and hybrid environments.
Learn more www.beyondtrust.com
 

"
https://startup.jobs/data-engineer-1482-meridianlink-4467745,Engineer,Data Engineer - 1482,MeridianLink ,,Full-Time,"

JOB SUMMARY
We are looking for an accomplished Data Engineer to join our quickly growing Analytics team. The hire will be responsible for expanding and improving our data and data pipeline architecture, as well as optimizing data flow and MDM for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing or even re-designing our companys data pipelines to support our next generation of products and data initiatives.
RESPONSIBILITIES
Design, develop, and operate large scale data pipelines to support internal and external consumers
Improve and automate internal processes
Integrate data sources to meet business requirements
Write robust, maintainable, well documented code
QUALIFICATIONS
2-4 years professional Data Engineering and Data warehousing experience
Extremely strong implementation experience in  
oPython, Spark, Azure Databricks, Delta Lake, and Databricks Data Warehouse.
oSQL development knowledge  Stored procedures, triggers, jobs, indexes, partitioning etc.
oBe able to writedebug complex SQL queries
oAzure Data factory or Azure Synapse Analytics
oETLELT and Data-warehousing techniques and best practices
Experience with MS-SQL server and Databricks Data warehouse.
Experience building, maintaining, and scaling ETLELT processes and infrastructure
Knowledge of being able to work with a variety of Ingestion patterns such as APISQL servers etc.
Experience with cloud infrastructure Azure strongly preferred
Knowledge of Master Data Management
Implementation experience with various data modelling techniques 
Implementation experience working with a BI visualization tool Sisense is a plus
Experience with CICD tools Preferred Gitlab, Jenkins
Pluses for experience with
oUI development frameworks such as java script, Django, REACT etc.
Experience working in a fast-paced product environment, with an attitude of getting the job done with the least amount of tech debt
Prior Financial industry experience a plus.
Be able to navigate ambiguity and pivot based on business priorities with ease.
Strong communication, negotiating and estimating skills.
Be a team player and should be able to collaborate well.
   
Enables entrepreneurs and consumers to achieve the American dream by creating technological solutions that fuel the engine for financial growth. Our top-notch solutions create the premier customer experience every time. We believe in the principles of empowerment, collaboration, individual achievement, and innovation.
At MeridianLink, we work together to design, implement, test, and deliver state-of-the-art web applications for the financial services industry, using the latest technologies including cloud computing, mobile development, responsive design, ASP.NET, JavaScript, C, VB.NET, and SQL Server. 
OUR CULTURE
Our low turnover is a testament to our wonderful culture where people value the work they do and appreciate each other for their contributions. MeridianLink develops our employees so they can grow professionally by preferring to promote from within. We have an open door policy with direct access to executives; we want to hear your ideas and what you think. Our company believes that to be productive in the long term, we must have a genuine work-life balance. We understand that employees have families and full lives outside of the office. To that end, we honor their personal commitments. 
MeridianLink is an Equal Opportunity Employer. We do not discriminate on the basis of race, religion, color, sex, age, national origin, disability or any other characteristic protected by applicable law.   
MeridianLink runs a comprehensive background check, credit check and drug test as part of our offer process.


MeridianLink has a wonderful culture where people value the work they do and appreciate each other for their contributions. We develop our employees so they can grow professionally by preferring to promote from within. We have an open-door policy with direct access to executives; we want to hear your ideas and what you think. Our company believes that to be productive in the long term, we must have a genuine work-life balance. We understand that employees have families and full lives outside of the office. To that end, we honor their personal commitments.
MeridianLink is an Equal Opportunity Employer. We do not discriminate based on race, religion, color, sex, age, national origin, disability, or any other characteristic protected by applicable law.  
MeridianLink runs a comprehensive background check, credit check, and drug test as part of our offer process.
Salary range of 94,500-133,400. It is not typical for offers to be made at or near the top of the range. The actual salary will be determined based on experience and other job-related factors permitted by law.
Meridianlink offers

Potential For Equity-Based Awards
Insurance coverage medical, dental, vision, life, and disability
Flexible paid time off
Paid holidays
401k plan with company match
Remote work
All compensation and benefits are subject to the terms and conditions of the underlying plans or programs, as applicable and as may be amended, terminated, or superseded from time to time.  
LI-REMOTE

"
https://startup.jobs/data-engineer-1483-meridianlink-4467744,Engineer,Data Engineer - 1483,MeridianLink ,,Full-Time,"

JOB SUMMARY
We are looking for an accomplished Data Engineer to join our quickly growing Analytics team. The hire will be responsible for expanding and improving our data and data pipeline architecture, as well as optimizing data flow and MDM for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing or even re-designing our companys data pipelines to support our next generation of products and data initiatives.
RESPONSIBILITIES
Design, develop, and operate large scale data pipelines to support internal and external consumers
Improve and automate internal processes
Integrate data sources to meet business requirements
Write robust, maintainable, well documented code
QUALIFICATIONS
2-4 years professional Data Engineering and Data warehousing experience
Extremely strong implementation experience in  
Python, Spark, Azure Databricks, Delta Lake, and Databricks Data Warehouse.
SQL development knowledge  Stored procedures, triggers, jobs, indexes, partitioning etc.
Be able to writedebug complex SQL queries
Azure Data factory or Azure Synapse Analytics
ETLELT and Data-warehousing techniques and best practices
Experience with MS-SQL server and Databricks Data warehouse.
Experience building, maintaining, and scaling ETLELT processes and infrastructure
Knowledge of being able to work with a variety of Ingestion patterns such as APISQL servers etc.
Experience with cloud infrastructure Azure strongly preferred
Knowledge of Master Data Management
Implementation experience with various data modelling techniques 
Implementation experience working with a BI visualization tool Sisense is a plus
Experience with CICD tools Preferred Gitlab, Jenkins
Pluses for experience with
oUI development frameworks such as java script, Django, REACT etc.
Experience working in a fast-paced product environment, with an attitude of getting the job done with the least amount of tech debt
Prior Financial industry experience a plus.
Be able to navigate ambiguity and pivot based on business priorities with ease.
Strong communication, negotiating and estimating skills.
Be a team player and should be able to collaborate well.
   
Enables entrepreneurs and consumers to achieve the American dream by creating technological solutions that fuel the engine for financial growth. Our top-notch solutions create the premier customer experience every time. We believe in the principles of empowerment, collaboration, individual achievement, and innovation.
At MeridianLink, we work together to design, implement, test, and deliver state-of-the-art web applications for the financial services industry, using the latest technologies including cloud computing, mobile development, responsive design, ASP.NET, JavaScript, C, VB.NET, and SQL Server. 
OUR CULTURE
Our low turnover is a testament to our wonderful culture where people value the work they do and appreciate each other for their contributions. MeridianLink develops our employees so they can grow professionally by preferring to promote from within. We have an open door policy with direct access to executives; we want to hear your ideas and what you think. Our company believes that to be productive in the long term, we must have a genuine work-life balance. We understand that employees have families and full lives outside of the office. To that end, we honor their personal commitments. 
MeridianLink is an Equal Opportunity Employer. We do not discriminate on the basis of race, religion, color, sex, age, national origin, disability or any other characteristic protected by applicable law.   
MeridianLink runs a comprehensive background check, credit check and drug test as part of our offer process.


MeridianLink has a wonderful culture where people value the work they do and appreciate each other for their contributions. We develop our employees so they can grow professionally by preferring to promote from within. We have an open-door policy with direct access to executives; we want to hear your ideas and what you think. Our company believes that to be productive in the long term, we must have a genuine work-life balance. We understand that employees have families and full lives outside of the office. To that end, we honor their personal commitments.
MeridianLink is an Equal Opportunity Employer. We do not discriminate based on race, religion, color, sex, age, national origin, disability, or any other characteristic protected by applicable law.  
MeridianLink runs a comprehensive background check, credit check, and drug test as part of our offer process.
Salary range of 94,500-133,400. It is not typical for offers to be made at or near the top of the range. The actual salary will be determined based on experience and other job-related factors permitted by law.
Meridianlink offers

Potential For Equity-Based Awards
Insurance coverage medical, dental, vision, life, and disability
Flexible paid time off
Paid holidays
401k plan with company match
Remote work
All compensation and benefits are subject to the terms and conditions of the underlying plans or programs, as applicable and as may be amended, terminated, or superseded from time to time.  
LI-REMOTE

"
https://startup.jobs/data-engineer-business-intelligence-getyourguide-2-4467158,Engineer,"Data Engineer, Business Intelligence",GetYourGuide ,"Berlin, Germany",,"

We are looking for a Data Engineer, Business Intelligence to help make the vast amount of data we produce organized and valuable so everyone in the company can make great decisions through insights. You will work on analytics tools, data quality, reporting solutions, data pipelines, data modeling and more.
About GetYourGuide
GetYourGuide is the globally leading marketplace for unforgettable travel experiences. Travelers use GetYourGuide to discover the best things to do in a destination. Since its founding in 2009, travelers from over 170 countries have booked more than 80 million tours, activities, and attraction tickets through GetYourGuide. Powered by a global team of over 700 travel experts and technologists, we are headquartered in Berlin and have 17 local offices around the world. Visit our careers website to learn more.
Team mission
The Data Platform team plays the central role in our data-driven strategy, designing and developing our data ecosystem, data process processes and analytics capabilities. We work on problems such as product events tracking, AB experiments framework and large-scale applied machine learning. The team is also responsible for making the vast amount of data accessible, reliable and efficient to query, so we are empowered to make the best data-driven decisions. We work with a wide variety of tools and technologies, from Spark to Looker, making this team a great place to learn new skills.
You will

Work closely with different teams and business partners in the organization, build and foster strong relationships
Develop new data pipelines and work with data analysts, data scientists, engineers and product managers to add new data sources or new views in our data
Implement and maintain data models using analytics tools in order to allow our business users access the data directly and drive decisions
Design and implement metrics, dashboards and reports
Ensure data quality, accuracy and reliability
Empower and train our data users to use data more efficiently, by extending our self serve data services and documentation 
Serve as a liaison between the product teams, analytics team and the data engineering team

Who you are

Excellent written and verbal communication skills in English
2-4 years of relevant experience in a business intelligence role, including data warehousing and analytics tools, techniques and technologies
Experience in Data Warehousing solutions Snowflake, PostgreSQL, Redshift, Big Query
Expert knowledge with SQL  OLAP
Experience with Python or Scala
Advanced knowledge of data warehousing concepts and schema optimization based on usage patterns
Knowledge and direct experience in data visualization concepts using business intelligence reporting tools Looker, Tableau, Power BI, Jupyter notebooks
Experience in developing cross-platform ETL processes, maintaining systems for tracking data quality and consistency and using databases in a business environment with large-scale, complex data sets


Nice to have 


Experience with big data technologies e.g. Spark, Kafka, Hive, Hadoop
Experience with data manipulation tools and libraries e.g. PySpark, Pandas, R
Prior experience in start-up environment

How we set you up for success

Invest in your development with an annual personal growth budget
Create a comfortable workspace at home with an annual home office budget
Become a part of our success with virtual stock options
Enjoy flexibility with a hybrid work-from-home and telecommuting policy
Save on transportation costs with discounted public transportation tickets
Support your loved ones with generous maternity and paternity leave policies

And more...
We look forward to hearing from you
Unlock your full potential and join our mission to create unforgettable experiences for millions around the world. If you have the skills and passion for joining our team, we invite you to apply by submitting your CVresume in English through the form below. Check out how we hire for tips and visibility into our process and check out life at GetYourGuide. If you have any further questions, please do not hesitate to contact us via jobsgetyourguide.com.
We are an equal opportunity employer
Our commitment is that every qualified person will be evaluated according to skills regardless of age, gender identity, ethnicity, sexual orientation, disability status, or religion. Please refrain from including your picture and age with the application. 
Important notice Protect yourself from recruitment scams
We want to ensure your safety during the application process. Please be aware of potential scammers impersonating GetYourGuide recruiters. Remember, GetYourGuide never requests payment or sensitive personal information during recruitment. All official job opportunities are exclusively posted on our Careers page. If you suspect fraudulent activity, report it to jobsgetyourguide.com. Stay vigilant and protect yourself from recruitment scams.
LI-Hybrid

"
https://startup.jobs/data-engineer-appzen-4466336,Engineer,Data Engineer,"AppZen, Inc. ","Pune, India",Full-Time,"

AppZen is the leader in autonomous spend-to-pay software. Its patented artificial intelligence accurately and efficiently processes information from thousands of data sources so that organizations can better understand enterprise spend at scale to make smarter business decisions. It seamlessly integrates with existing accounts payable, expense, and card workflows to read, understand, and make real-time decisions based on your unique spend profile, leading to faster processing times and fewer instances of fraud or wasteful spend. Global enterprises, including one-third of the Fortune 500, use AppZens invoice, expense, and card transaction solutions to replace manual finance processes and accelerate the speed and agility of their businesses. To learn more, visit us at www.appzen.com.


AppZen is the leader in autonomous spend-to-pay software. Its patented artificial intelligence accurately and efficiently processes information from thousands of data sources so that organizations can better understand enterprise spend at scale to make smarter business decisions. It seamlessly integrates with existing accounts payable, expense, and card workflows to read, understand, and make real-time decisions based on your unique spend profile, leading to faster processing times and fewer instances of fraud or wasteful spend. Global enterprises, including one-third of the Fortune 500, use AppZens invoice, expense, and card transaction solutions to replace manual finance processes and accelerate the speed and agility of their businesses. To learn more, visit us at www.appzen.com.


We are seeking a skilled and experienced Data Engineer to join our team. As a Data Engineer, you will play a crucial role in designing, developing, and maintaining our data infrastructure and pipelines. You will collaborate with cross-functional teams to gather requirements, build scalable solutions, and ensure the availability and reliability of our data systems.




Qualifications
Bachelors or masters degree in Computer Science, Engineering, or a related field.Strong proficiency in Python and SQL for data manipulation, analysis, and scripting.Extensive experience with cloud platforms, particularly AWS, and working knowledge of services like EMR, Redshift, and S3.Solid understanding of data warehousing concepts and experience with relational databases like PostgreSQL.Familiarity with data visualization and reporting tools such as Superset, Domo, or Tableau.Experience with building and maintaining data pipelines using tools like Airflow.Knowledge of Python web frameworks like Flask or Django for building data-driven applications.Strong problem-solving and analytical skills, with a keen attention to detail.Excellent communication and collaboration skills, with the ability to work effectively in a team environment.Proven ability to work in a fast-paced environment, prioritize tasks, and meet deadlines.If you are a talented Data Engineer with a passion for leveraging data to drive insights and impact, we would love to hear from you. Join our team and contribute to building robust data infrastructure and pipelines that power our organizations data-driven decision-making process
Responsibilities

Design, develop, and implement scalable and efficient data pipelines in the cloud using Python, SQL, and relevant technologies.Build and maintain data infrastructure on platforms such as AWS, leveraging services like EMR, Redshift, and others.Collaborate with data scientists, analysts, and other stakeholders to understand their requirements and provide the necessary data solutions.Develop and optimize ETL Extract, Transform, Load processes to ensure the accuracy, completeness, and timeliness of data.Create and maintain data models, schemas, and database structures using PostgreSQL and other relevant database technologies.Experience reporting tools such as Superset, good to have  Domo, or Tableau to develop visually appealing and insightful data visualizations and dashboards.Monitor and optimize the performance and scalability of data systems, ensuring high availability and reliability.Implement and maintain data security and privacy measures to protect sensitive information.Collaborate with the engineering team to integrate data solutions into existing applications or build new applications as required.Stay up-to-date with industry trends, emerging technologies, and best practices in data engineering.



We are equal opportunity employer and value diversity. All employment is decided on the basis of qualifications, merit and business need. 

"
https://startup.jobs/data-engineer-fanduel-4465981,Engineer,Data Engineer,FanDuel ,"Atlanta, United States",,"

ABOUT FANDUEL GROUP
There are more ways to win, here at FanDuel. Were willing to bet on it.
THE ROSTER
At FanDuel Group, we give fans a new and innovative way to interact with their favorite games, sports and teams. Were dedicated to building a winning team and we pride ourselves on being able to make every moment mean more, especially when it comes to your career. So, what does winning look like at FanDuel? Its recognition for your hard-earned results, a culture that brings out your best workand a roster full of talented coworkers. Make no mistake, we are here to win, but we believe in winning right. That means well never compromise when it comes to looking out for our teammates. From creatives professionals to cutting edge technology innovators, FanDuel offers a wide range of career opportunities, best in class benefits, and the tools to explore and grow into your best selves. At FanDuel, our principle of We Are One Team runs through all our offices across the globe, and you can expect to be a part of an exciting company with many opportunities to grow and be successful.
WHO WE ARE
FanDuel Group is an innovative sports-tech entertainment company that is changing the way consumers engage with their favorite sports, teams, and leagues. The premier gaming destination in the United States, FanDuel Group consists of a portfolio of leading brands across gaming, sports betting, daily fantasy sports, advance-deposit wagering, and TVmedia.
FanDuel Group has a presence across all 50 states with approximately 17 million customers and nearly 30 retail locations. The company is based in New York with offices in California, New Jersey, Florida, Oregon, Georgia, Portugal, Romania and Scotland.
Its network FanDuel TV and FanDuel are broadly distributed on linear cable television and through its relationships with leading direct-to-consumer OTT platforms.
FanDuel Group is a subsidiary of Flutter Entertainment plc, the worlds largest sports betting and gaming operator with a portfolio of globally recognized brands and a constituent of the FTSE 100 index of the London Stock Exchange.
THE POSITION Our roster has an opening with your name on it
FanDuel Group is looking for an experienced Data Engineer with deep understanding of large-scale data handling and processing best practices in a cloud environment to help us build scalable systems. As our data is a key component of the business used by almost every facet of the company, including product development, marketing, operations, and finance. It is vital that we deliver robust solutions that ensure reliable access to data with a focus on quality and availability. 
Our competitive edge comes from making decisions based on accurate and timely data and your work will provide access to that data across the whole company. Looking ahead to the next phase of our data platform we are keen to do more with real time data processing and working with our data scientists to create machine learning pipelines 
THE GAME PLANEveryone on our team has a part to play

Creating and maintain optimal data pipeline architecture 
Designing and implementing data pipelines required in the data warehouse and data lake in batch or real-time using data transformation technologies. 
Identifying, designing, and implementing internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability 
Designing and deploying data models and views with large datasets that meet functional  non-functional business requirements 
Delivering timely after-action reporting to state regulatory groups 
Delivering quality production-ready code in an agile environment 
Delivering test plans, monitoring, debugging and technical documents as a part of development cycle 
Creating data tools for analytics and working with stakeholders across all departments to assist with data-related technical issues and supporting their data infrastructure needs 

THE STATSWhat were looking for in our next teammate

Experience writing Python scripts 
Working SQL knowledge and experience working with relational databases 
Build processes supporting data transformation, data structures, metadata, dependency, and workload management,
Show proficiency understanding complex ETL processes 
Demonstrate the ability to optimize processes  
Knowledge of data integrity and relational rules 
Understanding of AWS and Google Cloud knowledge of DMS tasks and processes are nice to have.  
Ability to quickly learn new technologies is critical 
Proficiency with agile or lean development practices 
Understanding of regulated systems and sensitive data 

PLAYER CONTRACTWe treat our team right
From our many opportunities for professional development to our generous insurance and paid leave policies, were committed to making sure our employees get as much out of FanDuel as we ask them to give. Competitive compensation is just the beginning. As part of our team, you can expect

An exciting and fun environment committed to driving real growth
Opportunities to build really cool products that fans love
Mentorship and professional development resources to help you refine your game
Flexible vacation allowance to let you refuel
Hall of Fame benefit programs and platforms

FanDuel Group is an equal opportunities employer and we believe, as one of our principal states, We Are One Team!  We are committed to equal employment opportunity regardless of race, color, ethnicity, ancestry, religion, creed, sex, national origin, sexual orientation, age, citizenship status, marital status, disability, gender identity, gender expression, and Veteran status. We believe FanDuel is strongest and best able to compete if all employees feel valued, respected, and included.  We want our team to include diverse individuals because diversity of thought, diversity of perspectives, and diversity of experiences leads to better performance.  Having a diverse and inclusive workforce is a core value that we believe makes our company stronger and more competitive as One Team! 
LI-Hybrid

"
https://startup.jobs/data-engineer-tomorrow-4463755,Engineer,Data Engineer,Tomorrow ,"Cluj-Napoca, Romania",Contractor,"

Seeking a Data Engineer based in Cluj, Romania for 140,000 - 190,000 RON Gross Annually
Serpr, a dynamic, forward-thinking organisation championed by a team of inspiring women, is looking for an enthusiastic mid-level Data Engineer. Were dedicated to surpassing our customers expectations while maintaining an exhilarating pace that keeps us on the cutting edge of technology. 
We take pride in fostering an inclusive environment that values diverse perspectives. To this end, we extend a warm invitation to candidates of all genders, including those who identify as women, individuals with disabilities, and members of the Black, Asian, and Minority Ethnic BAME communities to apply, as these groups are traditionally underrepresented in the technology sector. Our commitment to diversity and inclusion is not just a corporate policy; it is a fundamental component of our DNA that drives innovation, creativity, and success.
We are looking for a motivated Data Engineer to join our talented team of scientists and engineers in Cluj-Napoca, Romania. As a Data Engineer for an exciting, Google-backed, AI-driven new marketing intelligence platform, you will be an integral member of the Innovation  Engineering team, making fundamental contributions to a true data-driven SaaS platform.
Reporting to the Head of Innovation  Engineering you will work closely with scientists, analysts and engineers to deliver and maintain data-driven solutions that help us achieve our technological vision. You will be at the forefront of the collaborative and solution-focused culture here at Serpr, where we prioritise teamwork, innovation and creativity.
You will achieve Certified Google Data Engineer status if this still needs to be attained and maintain this qualification. Training will be provided where necessary.
 Responsibilities
 Plan, design and implement scalable and reliable data management systems, ensuring data accuracy, completeness and availability Develop and maintain ETL processes and jobs for moving data efficiently across systems in near real-time and batch Develop, test and maintain data architecture, database management systems and data infrastructure Source raw and unstructured data through API integration and automated web crawls Develop and deploy data-driven applications via web apps, dashboards, or whatever is appropriate Be willing to work in a research capacity  to find and test and critically review new frameworks and technologies Provide best practices and frameworks for data testing and validation and ensure reliability and accuracy of data Work with data analysts, data scientists, and other stakeholders to ensure effective data integration and accuracy 
Requirements
 Proven experience developing data pipelines Proficiency in Python Experience with web scrapingcrawling frameworks for example Scrapy, Selenium, Cheerio, etc. Experience with ETL frameworks and tools for example Prefect, Apace Airflow, Apache NiFi, Kedro, etc.  Experience with cloud services such as Google Cloud Platform, AWS or Azure Advanced knowledge of SQL and database management systems such as MySQL and MongoDB Strong problem-solving and analytical skills Solid understanding of modern code development practices Degree holder in a scientific, computational or mathematical discipline 

Soft Skills
 Excellent time management skills Excellent written and verbal communication skills Critical thinking  problem-solving Pro-active  self-motivated with a sense of ownership Adopts a leave code better than you found it and fully commented attitude Ability to perform effectively to tight deadlines, work well both independently and as part of a team 
Benefits
 Flexible working Family-friendly benefits Personalised career development Employee wellbeing initiatives Pension Bippit financial coaching Bupa Healthcare 

"
https://startup.jobs/data-engineer-ribbon-health-2-4463642,Engineer,Data Engineer,Ribbon Health ,"New York, United States",,"

At Ribbon, were building the infrastructure to power billions of healthcare decisions. We work with companies across the entire healthcare ecosystem, partnering with innovators to make care accessible, affordable, and high quality. By transforming the healthcare decision-making process, we can influence a persons outcomes for the better. 
We are looking for a Data Engineer who can architect efficient and scalable systems, set data engineering standards for the engineering organization, and enjoys rolling up their sleeves and coding. Our data pipelines today consist of hundreds of datasets from unique sources and continue to grow every day. These pipelines power our machine learning models and API, which powers production workflows that affect real patients. Were excited to bring on experienced developer to build scalable systems that can support the next phase of our growth.
Together, we can create a healthier future where people receive exceptional care!
What we look for at Ribbon 

Passion and drive to make navigating healthcare simple by building products that increase access to care and power every healthcare decision to be accessible, affordable and high quality.

Commitment to Ribbon Health company values, working on an exceptional team, and building an exceptional company.

Grit, hustle, desire, and a get-it-done attitude;  strong comfort with a lean startup environment, where everyone is encouraged to participate in and contribute across all teams.
Dedication to the creation of a diverse, equitable, and inclusive environment where teammates are celebrated for their unique perspectives and work together to make navigating healthcare simple.

What were looking for in this role

You have a strong background in Computer Science or other equivalent field.
Youre an entrepreneur, a self starter, and want to help us build and scale applications and data pipelines.
You love solving ambiguous problems and want to understand how to help us aggregate healthcare data from across the web.
You have a passion for data and want to help us untangle and discover truth in an industry rife with fragmented data.
You have a track record of learning new technologies and languages on the job.
Youre very comfortable with SQL and relational databases; Python experience would be a big plus, but not required.

Your day-to-day


Develop and manage data scrapers You will develop new data scrapers that consume data from 100s of sources across healthcare


Scale our data ingestion framework You will help design and improve upon our current system of record for ingesting data from hundreds of different sources.


Build light-weight automation You will develop systems and tools to configure, monitor, and orchestrate our data infrastructure


Streamline our internal data operations You will build tools to empower internal stakeholders to manage data across our pipelines and allow our internal teams to quickly distribute data to our clients.


We seek to make Ribbon the best career decision anyones ever made. Part of fulfilling this vision is a compelling total compensation package. The base salary range for this role is between 110,000-120,000. Starting pay will be based on a number of factors and commensurate with qualifications  experience. Ribbon also offers a competitive equity package, health insurance, parental leave, 401K matching, and other great benefits.

 

We take care of you
Our goal is to make this the best career decision any of us have ever made. We stand by this goal by offering a wide variety of benefits such as

Fully sponsored medical, vision, and dental coverage
401k with matching
Parental leave and family planning
Healthcare and dependent care FSA
Flexible PTO
Hybrid work environment
Sponsored membership to One Medical
Mental health benefits
Commuter support and CitiBike membership
Employee assistance program
Optional long-term disability insurance
Fully covered short-term disability insurance  optional long term disability insurance
Work from home stipend
Inclusive environment passionate about supporting each other as teammates


 
Ribbon Health is proud to be an Equal Opportunity Employer. All aspects of employment including the decision to hire, promote, discipline, or discharge, will be based on merit, competence, performance, and business needs. We do not discriminate on the basis of race, color, religion, marital status, age, national origin, ancestry, physical or mental disability, medical condition, pregnancy, genetic information, gender, sexual orientation, gender identity or expression, veteran status, or any other status protected under federal, state, or local law.


A successful candidate must be fully vaccinated against COVID-19, unless Ribbon Health grants an exemption based on medical condition andor disability, sincerely held religious belief, or as otherwise required under applicable law.

Dont meet every requirement? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every single qualification. 
At Ribbon, were committed to building a diverse inclusive and authentic workplace with a tangible sense of belonging for all our people. 
So if youre excited about this role but your past experience doesnt align perfectly with everything in the job description, we encourage you to apply anyway. You may be just the right person for this or another role.



"
https://startup.jobs/data-engineer-machine-learning-remote-alltrails-4462836,"Engineer,Machine Learning","Data Engineer, Machine Learning (Remote)",AllTrails ,"San Francisco, United States",Full-Time,"

Data Engineer, Machine Learning

About AllTrails

AllTrails is the most trusted and used outdoors platform in the world. We help people explore the outdoors with hand-curated trail maps along with photos, reviews, and user recordings crowdsourced from our community of millions of registered hikers, mountain bikers and trail runners in 150 countries. AllTrails is frequently ranked as a top-5 Health and Fitness app and has been downloaded by over 50 million people worldwide.
 
Every day, we solve incredibly hard problems so that we can get more people outside having healthy, authentic experiences and a deeper appreciation of the outdoors. Join us! 
What Youll Be Doing

Deploy and build systems that enable machine learning and artificial intelligence product solutions
Work cross-functionally to ensure data scientists have access to clean, reliable, and secure data, the backbone for new algorithmic product features
Build, deploy, and orchestrate large-scale batch and stream data pipelines to transform and move data tofrom our data warehouse and third-party systems
Deliver scalable, testable, maintainable, and high-quality code
Investigate, test-for, monitor, and alert on inconsistencies in our data, data systems, or processing costs
Create tools to improve data and model discoverability and documentation
Ensure data collection and storage adheres to GDPR and other privacy and legal compliance requirements
Uphold best data-quality standards and practices, promoting such knowledge throughout the organization

Requirements

3 years of data engineering experience
Expertise in Python for data cleansing, transformation, modeling, etc.
Professional experience in transforming machine-learning prototypes into solutions that scale with real-world constraints and deploying them into production
Proficiency with SQL and experience working with high volume datasets in SQL-based warehouses such as BigQuery, Redshift, Snowflake, or others
Experience with parallelized data processing frameworks such as Apache Beam, Apache Spark, Google Dataflow, AWS Glue, etc.
Deep understanding of data modeling, access, storage, caching, replication, and optimization techniques
Ability to orchestrate data pipelines through tools such as Apache Airflow
Experienced in container orchestration e.g. Docker
Understanding of the software development lifecycle and CICD
Monitoring and metrics-gathering e.g. Datadog, NewRelic, Cloudwatch, etc
Proficiency with git and working on a shared codebase
Excellent documentation skills
Self motivation and a deep sense of pride in your work
Passion for the outdoors
Comfort with ambiguity, and an instinct for moving quickly
Humility, empathy and open-mindedness - no egos

Bonus Points

Experience working with machine learning development frameworks such as TensorFlow, Caffe2, PyTorch, Spark ML, scikit-learn, or related frameworks
Experience with machine learning workflow management frameworks such as MLFlow, KubeFlow, SageMaker, Neptune, or related frameworks
Experience with GPU-optimized data processing i.e. CUDA
Experience with infrastructure-as-code, such as Terraform
Experience with ELT tools such as dbt or Dataform

What We Offer

A competitive and equitable compensation plan. This is a full-time, salaried position that includes equity.
Physical  mental well-being including health, dental and vision benefits
Trail Days First Friday of each month to hit the trails!
Unlimited PTO
Flexible parental leave
Annual continuing education stipend
Discounts on subscriptions and merchandise for you and your friends  family
An authentic investment in you as a human being and your career as a professional


Nature celebrates you just the way you are and so do we! At AllTrails were passionate about nurturing an inclusive workplace that values diversity. Its no secret that companies that are diverse in background, age, gender identity, race, sexual orientation, physical or mental ability, ethnicity, and perspective are proven to be more successful. Were focused on creating an environment where everyone can do their best work and thrive.

"
https://startup.jobs/data-engineer-2-myfitnesspal-4462407,Engineer,Data Engineer 2,MyFitnessPal ,,,"

At MyFitnessPal, our vision is to be the global catalyst for every body to achieve their healthy. We believe good health starts with what you eat. We provide the tools and resources to reach your fitness goals.
We are looking for a Data Engineer to join the MyFitnessPal Data Engineering team. Our users rely on MyFitnessPal to power their health and fitness journeys every day. As a member of our MyFitnessPal Engineering team, youll have the opportunity to positively impact those users with your expertise in the backend systems that drive the MyFitnessPal ecosystem. In addition to technical expertise, youll find that your teammates value collaboration, mentorship, and inclusive environments.

What youll be doing 


Build and maintain pipelines that provide data critical to operational reporting and data-driven initiatives
Integrate with services and systems across the MyFitnessPal engineering teams
Evaluate and improve existing pipelines, data models, and processes to provide more robust solutions that allow others to move quickly and efficiently
Support resolution of production issues across the Data Platform stack
Engage in code-reviews, working with teammates to learn and grow
Live our core values in all you do

Be Kind and Care
Live Good Health
Be Data-Inspired
Champion Change
Leave it Better than You Found It
Make It Happen



Qualifications to be successful in this role 


2-4 years of data engineering experience working with large datasets and complex data environments, processes, and associated solutions
Experience with a variety of data stores e.g. Snowflake, MySQL, MongoDB, DynamoDB, etc.
Experience with data orchestration tooling e.g. Airflow, Data Factory, etc.
Experience with development languages e.g. Python, SQL, Scala, etc.
Understanding of data modeling for analysis by business intelligence or data science teams
Experience with a variety of API design patterns, such as REST
Experience developing validation and data integrity frameworks
Triaging and debugging production data issues
Experience building high volume data pipelines for downstream analysis supporting operational indicators
Experience with data at scale
Worked alongside client teams to support integration efforts
Familiarity with AWS andor other cloud computing platforms

 
Please consider applying even if you dont meet 100 of the qualifications. Research shows you can still be considered for a position if you meet some of the requirements. At MyFitnessPal, were building a fitness product for everyone and believe our team should reflect that. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply.

Full Time Employee Perks, Benefits, and Culture

Remote equal philosophy enabling you to work from any state in which we have operations in the continental U.S. 
Want to work in an office? We also have a physical office in Austin, TX
Annual, in-person company retreats to work, bond, and enjoy team-building activities
Opportunities for team members to meet and connect in person for company paid lunches or working sessions
Flexible time-off policy  flexible working hours Unlimited PTO Plan
Competitive medical, dental, and vision benefits
Safe Harbor 401K program 
Paid maternity and parental leave
Monthly Wellness Allowance to assist team members to focus on their own physical and mental wellbeing and select wellness initiatives of their own choice
Reward  recognition platform enabling peers to recognize and reward their peers for all the great work they do
MyFitnessPal Premium 
Modern Virtual Learning and Development Library
DEI Committee dedicated to ongoing efforts to foster a diverse and inclusive workplace by setting actionable goals and evaluating progress
Diversity training for employees
A dynamic, motivating, and fun work environment 

At MyFitnessPal, our mission is to enable people to make healthy choices. And it wouldnt be possible without our team. We celebrate the unique POV that each person brings to the table and believe in a collaborative and inclusive environment. As an equal opportunity employer, we prohibit any unlawful discrimination on the basis of race, color, religion, military or veteran status, sex, gender, gender identity or expression, sexual orientation, national origin, age, disability or genetic information. These are our guiding ideologies and apply across all aspects of employment.
MyFitnessPal participates in E-Verify. 


"
https://startup.jobs/data-engineer-bionic-services-ltd-4460337,Engineer,Data Engineer,Bionic Services Ltd ,"London, United Kingdom",Full-Time,"

We are Bionic, the UKs number one business comparison and switching service. Its our mission to make it radically easier to run a small business and earn the lifetime loyalty of British businesses. To do this, we combine world-class human service with smart technology and data. This means our tech-enabled experts can quickly match business owners with the best deals on their business essentials  energy, connectivity, insurance, and finance. We also partner with leading price comparison websites, including Compare the Market, MoneySuperMarket, Uswitch for Business, and Confused.com, who trust us to provide their business energy switching services. Bionic recently realised significant investment from OMERS PE investment company and are now investing heavily in tech and data products and innovation.  As such we are rapidly scaling and are now looking for a Data Engineer to join our Data and Insights team within our wider Tech team.
At Bionic data is core to our business and strategy and this role is part of a team that is critical to delivering on this goal. We define this as ensuring all company users can use data and analysis to make strategic as well as day to day decisions to benefit our customers. To do this, in brief, this role will help lead the transformation of our data and insights function towards real time streaming and hyper-personalisation in our customer interactions.
As part of this transformation, you will help define and enhance our data capabilities as we continue our migration and development of the Data Warehouse in AWS from SQL Server. You will also help manage and develop our data warehouse, ensuring it remains performant, accurate and fit for purpose. You will work closely with the Data  Insights team to refine, improve and build out the database and support the development of performant SQL scripts.
Having a deep understanding of data and how to effectively use it, the Data Engineer will lead a passionate quest of business transformation by optimising our data infrastructure, data model, and the new analytical data warehouse  data store. 
The role will be a hands-on role including database designmodelling, database creation, performance optimization and data loads  ETL processing and SSIS packages.
Using the expertise gained from tried and tested experience, you will work closely with key stakeholders to earn trust and deliver a service that will support our high growth plans and helping to realise additional growth potential. 
 Support the team in the performance, integrity, design, management and security of our data warehouse. Along with the team support ongoing development of the data warehouse, including enhancing our current data sets with additional data sources. Support for all ETL processes, from landing through to populating our data warehouse, ensuring the integrity of our data is maintained throughout and that the ETL processes are stable, accurate and performant. Ensuring the data warehouse continues to be fit for purpose and meeting our business needs. Monitoring database performance and implementing optimisation techniques. Ensuring we are continuing to be innovative, utilising the latest technologies and challenging our ways of working to deliver the best data and reporting possible. Supporting the BI team with regards to SQL expertise, best practises in reporting and data warehouse design Working with the IT Operations Manager, Development Manager and Head of BI in building out a robust and scalable data warehouse platform 
 This is an amazing opportunity for a strong Data Engineer to really shine in a fast-paced collaborative environment with plenty of opportunity to take on real responsibility and excel your career in the direction you want. If you are a strong dev looking to take on some real responsibility and gain experience in architecture this is the perfect role for you.
Requirements
Desired Skills  Strong hands-on experience of enterprise level data solutions projects using the Microsoft BI stack. 3 years experience with Microsoft SQL query design  optimisation Experience with data warehouse design, data pipeline development and maintenance and data integration Experience with on-premises SQL Server solutions and associated BI tool; SSIS. Experience with Python, ideally in the context of data engineering tasks Experience with source control tools, e.g. Git, Azure DevOps, TFS Experience with working with SDLC e.g. Agile Methodology Knowledge of core ETL architectural principles such as modularity, restartability, and table-driven business rules.  Understanding of information delivery architecture including the creation of abstraction layers, SQL query paths and optimization, report level calculations and end user reporting architecture Excellent communication and stakeholder engagement skills 
You might also have  Experience in database design and data modelling for the purpose of data warehousing  reporting  Experience with cloud technologies, AzureAWSGCPSnowflake. Strong BI knowledge Experience with Cozyroc SSIS toolset 

"
https://startup.jobs/data-engineer-octaura-ll-tradingco-llc-4460196,Engineer,Data Engineer,Octaura LL TradingCo LLC ,"New York, United States",Full-Time,"


Were on a mission 

At Octaura, we continually evolve markets to unleash value for clients. Its in our DNA to make a difference and do things differently. 

Existing workflows within our markets are painful for clients they are outdated, overcomplicated, and time-consuming. We want to change that. Octaura fundamentally rebuilds and redefines the markets by streamlining workflows, digitizing platforms, and bringing transactions, data and analytics together for the first time. 


Join our inclusive culture 

At Octaura, everyone belongs.  

Its so important to us that all Octaurians are confident in knowing they have the space to use their voice and talents. We love the diversity we see in the world and we actively want our team to reflect this. Were a values-driven company and by engaging, solving and evolving together, we create a culture that is collaborative, switched-on, and fun to be part of. 


The role in a nutshell 

We are looking for a Data Engineer who can steer the organizations data architecture and management strategy, playing a crucial role in consolidating disparate data sources into a single, easily accessible data pool. The right candidate can navigate the complexity of data systems, advocate for data governance, and have an inherent knack for translating raw data into actionable business intelligence. 
 

This is an exciting time for Octaura as we are rapidly growing and are looking for energetic, collaborative and driven thinkers to join us! Please note this role is based out of our NYC office and our current structure is 4 days Mon-Thurs in the office, 1 day from home.   

Core responsibilities 

Design, build, and maintain scalable data pipelines, leveraging and integrating structured and unstructured data from diverse sources. 
Adopt, implement, and maintain a Cloud Data Infrastructure working with services such as Snowflake, Databricks, or similar. 
Normalize data and ensure that it is secure, reliable, and easily accessible for consumption and sale. 
Implement strategies to manage a centralized data pool, facilitating its use in BI and other reporting tools. 
Uphold data integrity and enforce data security policies in alignment with compliance and privacy standards. 
Collaborate with all Product, Engineering, Sales, Marketing and Executive teams to deliver customized data reports for internal and external stakeholders. 
Feed processed data back into the trading platform to enhance our product and provide valuable insights for our customers investment decisions. 
Stay informed about emerging trends and technologies in the field of Data Engineering, suggesting and implementing new tools and methodologies as appropriate. 

Desired qualifications 

Bachelors or Masters degree in Computer Science, Information Systems, or a related field. 
Minimum of 5 years of experience as a Data Engineer or in a similar role, preferably in financial services or FinTech.
Proficiency in NoSQL databases, MongoDB, and other data warehousing technologies. 
Strong programming skills in Python, with demonstrated ability to manipulate data and build data solutions. 
Extensive experience with ETL tools and processes, data modelling, and data architecture. 
Advanced proficiency with BI tools like Tableau, with the ability to generate complex, large-scale data reports. 
Knowledge of cloud-based data solutions AWS and big data technologies Hadoop, Spark is a plus. 
Strong understanding of data privacy and security principles, along with experience in relevant data protection regulations. 
Exceptional problem-solving skills and the ability to work both independently and collaboratively. 
Excellent communication skills, with the ability to translate complex data insights into understandable and actionable information. 
Experience building and adopting best practices in a start-up environment 


The base pay range for this position in New York is 140,000 - 170,000 annually. Pay may vary depending on job-related knowledge, skills, and experience. Equity and year-end bonus may be provided as part of the compensation package, in addition to a full range of medical, financial, and other benefits, dependent on the position offered. Applicants should apply via Octauras internal or external careers site. 

Octaura Work Perks
At Octaura, our people are our most valuable asset, and we are pleased to offer the following benefits to all full-time employees 
- Competitive compensation and equity 
- Unlimited Paid Time Off  
- Competitive Parental Leave  
- Daily breakfast, coffee and snacks in the office 
- 90 company-paid healthcare 
- Onsite gym  discounted membership 


Were committed to equal opportunity employment 

Octaura is committed to a diverse and inclusive workplace. We are an equal opportunity employer and do not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. 

"
https://startup.jobs/data-engineer-snowflake-getindata-4459672,Engineer,Data Engineer (Snowflake),GetInData ,,,"

About us
GetInData  Part of Xebia is a leading Polish expert company delivering cutting-edge Big Data, Cloud, Analytics, and MLAI solutions. The company was founded in 2014 by data engineers and today brings together 120 big data specialists. We work with international clients from many industries, e.g. media, e-commerce, retail, fintech, banking, and telco, such as Truecaller, Spotify, ING, Acast, Volt, and Play, Allegro. Our clients are both fast-growing scaleups and large corporations that are leaders in their industries.We maintain a laser focus on data technologies, cultivate a very strong engineering culture and support extensive knowledge sharing both within a company and outside through meetups, conferences, and contributions to open-source.We are a go-to partner for companies that need tailored and highly scalable data processing and analytics platforms that give a competitive advantage and unlock the full business potential of their data.In 2022 we joined forces with Xebia Group to broaden our horizons and bring new international opportunities to our specialists and customers.
About roleAs a Data Engineer in GetInData, you will be crucial in providing, developing, implementing, improving, and maintaining the Data platform. You will be responsible for guiding the customer and providing best practices across the company.
Responsibilities

Use best practices in continuous integration and delivery
Work collaboratively with a team of cross-functional engineers
Work with structured, semi-structured, and unstructured data
Development and committing of new functionalities and open-source tools
RD, maintenance, and monitoring of the platforms components
Implementing and executing policies aligned to the strategic plans of the company concerning used technologies, work organization, etc.
Creating and propagating standards of work in projects
Undertaking work that requires the application of fundamental principles in a wide and often unpredictable range of contexts
Contributing to the organizations knowledge database

Technologies used

Python
Snowflake
DBT
Dagster


"
https://startup.jobs/data-engineer-azure-snowflake-willow-4458727,Engineer,Data Engineer-Azure/Snowflake,Willow ,"Brisbane, Australia",Full-Time,"

Willow is an established global start-up in rapid growth. The WillowTwin is a disruptive IoTData SaaS that unlocks the true potential of smart buildings and infrastructure. We are writing a new chapter in human history, with unprecedented resource optimisation and management empowered by data. 
 
For the second year in a row 2020  2021, Willow has been ranked in Linked Ins Australian ""Top 25 Start-ups"". You will be joining a team of performance-driven individuals, backed by the most advanced technology the built world has ever seen. We are chartering a new course, Digital First, the Willow Way. Our Willow World is fast-paced, nurturing and collaborative.


Summary of role 

Data Engineers  work on end-to-end, mostly greenfield, projects using a modern cloud techdata stack within the emerging Digital Twins technology space. This is a dynamic environment with a lot of interesting Data engineering challenges to solve. And its also a great opportunity to upskill on Azure data servicesSnowflake, a leading cloud data warehouse platform. 


Role and Responsibilites

Design, build and maintain data pipelines ensuring data quality, efficient processing, and timely delivery of accurate and trusted data.  
Work on the company-wide rollout of our Snowflake data warehouse. 
Ensure performance, security, and availability of the data warehouse.  
Help setup and maintain CICD pipelines to support the data warehouse.  
Interface with the CyberSec team to ensure consistent application of standard security policies.  


Skills and Experience

Experience building and optimizing data pipelines, architectures and datasets in Azure. 
Demonstrable experience implementing and optimizing both batch and streaming data pipelines.  
Understanding of modern data warehousedata lake modelling.  
Commercial experience with Azure services, including Azure Data Factory, Azure Databricks, Azure Data Lake, Azure Functions, Azure Key Vault, etc.  
Coding experience in at least one modern programming language Python, C, Scala, Java  
Experience in developing automated build and deployment pipelines using YAML, ARM templates and PowerShell.  
Advanced proficiency in writing complex SQL statements and manipulating large structured and semi-structured datasets.  
Expertise in performance tuning and troubleshooting.  
You should be an active learner, passionate about data and new technologies as well as be comfortable recommending new and better ways to do things to the team.  
You need be very comfortable collaborating with data analysts, data scientists, software engineers as well as talking to product managers and other business stakeholders and explaining data concepts.  
Attention to detail and strong problem-solving skills.  
Tertiary qualification in Computer Science or similar relevant field.  

Nice to have

Practical experience with Snowflake cloud data warehouse is very highly regarded  
Experience with any of the BI reporting tools e.g., Power BI, Sigma, Qlik, Tableau.  
Working knowledge of Apache Spark and Databricks platform is a plus.   
Experience with Git or a similar version controlsource code management tool is desired.  
Azure certification.  


Benefits At Willow

We strive to have parity of perks  benefits across regions and while regulation differs from place to place, we believe that taking care of our people is the right thing to do.

 Flexible working arrangements
 Remotehybrid working where applicable
 Competitive salary banding based on the work you do
 Career Learning  Development through Willow Academy Education Program 
 Up to 3 days leave each year to use towards volunteer projects you love 
 Employee Assistance Program
 Embracing parenthood with parental leave 
 Medical insurance coverage U.S and PH
 401K retirement plan U.S 

If you are eager to work in a fast-paced, high-growth tech start-up based on collaboration and open communication, then Willow could be the place for you. See the life and culture here through the eyes of PeopleofWillow 


As a distributed organisation, we value diversity in both backgrounds and experiences. Willow is proud to be an equal opportunity employer and is committed to making diversity, equity, and inclusion a part of everything we do. We believe that regardless of position or tenure, everyone is a leader. We at Willow never give up, we work smart, we care about our fellow human beings, and we always put our best foot forward. This translates to establishing an environment in which everyone can bring their real selves to work and make it easy for others to do the same.

To find out more, visit the website httpswww.willowinc.com


LI-Remote

"
https://startup.jobs/data-engineer-senior-software-engineer-paytmlabs-4458449,"Engineer,Senior,Developer",Data Engineer - Senior Software Engineer,Paytm Labs ,"Bengaluru, India",,"

About Us

Paytm is Indias leading digital payments and financial services company, which is focused on driving consumers and merchants to its platform by offering them a variety of payment use cases. Paytm provides consumers with services like utility payments and money transfers, while empowering them to pay via Paytm Payment Instruments PPI like Paytm Wallet, Paytm UPI, Paytm Payments Bank Netbanking, Paytm FASTag and Paytm Postpaid - Buy Now, Pay Later. To merchants, Paytm offers acquiring devices like Soundbox, EDC, QR and Payment Gateway where payment aggregation is done through PPI and also other banks financial instruments. To further enhance merchants business, Paytm offers merchants commerce services through advertising and Paytm Mini app store. Operating on this platform leverage, the company then offers credit services such as merchant loans, personal loans and BNPL, sourced by its financial partners. 

About the Role 

This position requires someone to work on complex technical projects and closely work with peers in an innovative and fast-paced environment. For this role, we require someone with a strong product design sense  specialized in Hadoop and Spark technologies. 


Requirements 

HeSheThey should have minimum 4-6 years of experience in Big Data technologies.


The position Grow our analytics capabilities with faster, more reliable tools, handling petabytes of data every day.


Brainstorm and create new platforms that can help in our quest to make available to cluster users in all shapes and forms, with low latency and horizontal scalability.


Make changes to our diagnosing any problems across the entire technical stack.


Design and develop a real-time events pipeline for Data ingestion for real-time dash- boarding.


Develop complex and efficient functions to transform raw data sources into powerful, reliable components of our data lake.


Design  implement new components and various emerging technologies in Hadoop Eco- System, and successful execution of various projects.


Be a brand ambassador for Paytm  Stay Hungry, Stay Humble, Stay Relevant! 


Skills that will help you succeed in this role 

Fluent with Strong hands-on experience with Hadoop, MapReduce, Spark, PySpark etc.


Excellent programmingdebugging skills in PythonJavaScala.


Experience with any scripting language such as Python, Bash etc.


Good to have experience of working with noSQL databases like Hbase, Cassandra.Hands-on programming experience with multithreaded applications.


Good to have experience in Database, SQL, messaging queues like Kafka.


Good to have experience in developing streaming applications e.g. Spark Streaming, Flink, Storm, etc.


Good to have experience with AWS and cloud technologies such as S3Experience with caching architectures like Redis etc. 


Why join us  

Because you get an opportunity to make a difference, and have a great time doing that.


You are challenged and encouraged here to do stuff that is meaningful for you and for those we serve.


You should work with us if you think seriously about what technology can do for people.


We are successful, and our successes are rooted in our peoples collective energy and unwavering focus on the customer, and thats how it will always be.

Learn more about the exciting work we do in Tech by reading our Engineering blogs



Compensation If you are the right fit, we believe in creating wealth for you with enviable 500 mn registered users, 21 mn merchants and depth of data in our ecosystem, we are in a unique position to democratize credit for deserving consumers  merchants  and we are committed to it. Indias largest digital lending story is brewing here. Its your opportunity to be a part of the story!



"
https://startup.jobs/data-engineer-interstellar-lab-4458211,Engineer,Data Engineer,Interstellar Lab ,"Paris, France",,"



At Interstellar Lab our mission is to build a future full of life, on Earth and beyond. We believe in a future where technology is enabling humans to live in harmony with nature, a future where biodiversity thrives and expands, a future where terrestrial life become multi-planetary.
To build this future, we design, manufacture and operate controlled-environment biofarms that unleash the power of plants. On Earth and in Space. Our solutions combine Hardware, AI and Bioscience. We use controlled-environment technology to mimic a multitude of conditions to allow plants, fungi and microorganisms to grow faster and better.
On Earth, we are pioneering biofarming at scale, boosting production of bioactive compounds and providing a sustainable and safe source of plant materials. In Space, we provide farming units to grow efficiently food for astronaut while accelerating plant research in microgravity.

 


Job Description and key responsibilitiesWe are seeking a skilled and experienced Data Engineer to join our team. As a Data Engineer, you will play a crucial role in the design, development, and maintenance of our data infrastructure in an IoT environment. Your expertise will revolve around managing data generated by various sources sensors, actuators, manual measurement, etc. and implementing efficient data pipelines, ensuring the smooth flow of data for analysis and visualization.
 
Responsibilities

Design and develop robust and scalable data architectures and pipelines for handling IoT data streams
Monitor data pipelines, perform troubleshooting, and implement proactive measures to identify and resolve any issues related to data ingestion, processing, or storage
Setup data lifecycle management
Aggregate data from disparate sources IoT sensors, cameras, manual input
Help streamline our data science workflows, adding value to our product offerings
Work closely with the data science and business intelligence teams to develop data models and pipelines for research, reporting, and machine learning
Be an advocate for best practices and continued learning

Basic Qualification

Masters degree in software engineering or related field and 2 years professional experience in software engineering; OR 6 years of experience in software engineering in lieu of a degree
Significant experience as a Data Engineer or in a similar software engineering role
Development experience with high level programming language preference for Python and Go
Experience with IoT environment or similar application field time series Database, MQTT protocol, unreliable network, etc.

Preferred skills ans experience

Familiarity with the AWS ecosystem
Experience in building or maintaining ETL processes
Strong proficiency in working with IoT protocols and technologies, such as MQTT, CoAP, or AMQP
Experience with time-series databases, preferably InfluxDB, for efficiently storing and retrieving IoT data
Experience with SQL databases
Solid programming skills in languages such as Python, Java, or Scala for data manipulation and ETL processes
Familiarity with cloud platforms such as AWS preferred, Azure, or GCP
Understanding of data governance, security, and compliance practices in an IoT context

Soft skills

Passionate and curious about space technologies, green technologies, software engineering
Problem-solving attitude
Collaborative team spirit
Ability to rapidly change rolesresponsibilities while working in a high-paced, challenging work environment


Location
Paris Area France

 

"
https://startup.jobs/data-engineer-finance-notion-4455106,"Engineer,Finance","Data Engineer, Finance",Notion ,"San Francisco, United States",,"

About Us
Were on a mission to make it possible for every person, team, and company to be able to tailor their software to solve any problem and take on any challenge. Computers may be our most powerful tools, but most of us cant build or modify the software we use on them every day. At Notion, we want to change this with focus, design, and craft.
Weve been working on this together since 2016, and have customers like Pixar, Mitsubishi, Figma, Plaid, Match Group, and thousands more on this journey with us. Today, were growing fast and excited for new teammates to join us who are the best at what they do. Were passionate about building a company as diverse and creative as the millions of people Notion reaches worldwide.
Notion is an in person company, and currently requires its employees to come to the office for two Anchor Days Mondays  Thursdays and requests that employees spend the majority of their week in the office including a third day.
About The Role
As Notion continues to grow rapidly, were seeking talented data engineers to join our team and help us build the foundational datasets and pipelines for robust financial reporting. Youll be at the forefront of integrating our product, financial, and business systems to create rock solid processes that will propel us forward. If youre passionate about analytics use cases, data models, and solving complex data problems, then we want you on our team.
What Youll Achieve

Youll build core datasets to serve as the sources of truth for Notions financial reporting, integrating data from financial systems, business systems data and Notions product.
Youll partner closely with our Finance, Monetization Engineering, Business Intelligence and Data Science teams to support critical financial reporting and analysis needs.
Youll design, build and monitor pipelines that meet todays requirements but can gracefully scale with our growing data size.
Youll help democratize access to high quality financial data across Finance, Staff and go-to-market teams.

Skills Youll Need to Bring

Youve spent 4 years as a data engineer building core datasets and supporting business verticals as needed, ideally in product and business areas with high data volumes. You are passionate about analytics use cases, data models and solving complex data problems.
Youve built integrations with and reporting datasets for payments, finance and business systems like Stripe, Netsuite, Adaptive, Anaplan, Salesforce andor others.
You are a self-starter and continuously gather and synthesize high-impact needs from business partners, design and implementing the appropriate technical solutions, and effectively communicating about deliverables, timelines and tradeoffs
You have hands-on experience shipping scalable data solutions in the cloud e.g AWS, GCP, Azure, across multiple data stores e.g Snowflake, Redshift, Hive, SQLNoSQL, columnar storage formats and methodologies e.g dimensional modeling, data marts, starsnowflake schemas
You are a SQL expert. You intimately understand aggregation functions, window functions, UDFs, self-joins, partitioning and clustering approaches to run correct and highly-performant queries
You are comfortable with object-oriented programming paradigms e.g Python, Java, Scala

Nice to Haves

You have hands-on experience in designing and building highly scalable and reliable data pipelines using BigData stack e.g Airflow, DBT, Spark, Hive, ParquetORC, ProtobufThrift, etc
You have hands-on experience building payment processing and invoice systems or have worked closely with teams that do this.

We hire talented and passionate people from a variety of backgrounds because we want our global employee base to represent the wide diversity of our customers. If youre excited about a role but your past experience doesnt align perfectly with every bullet point listed in the job description, we still encourage you to apply. If youre a builder at heart, share our company values, and enthusiastic about making software toolmaking ubiquitous, we want to hear from you.
Notion is proud to be an equal opportunity employer. We do not discriminate in hiring or any employment decision based on race, color, religion, national origin, age, sex including pregnancy, childbirth, or related medical conditions, marital status, ancestry, physical or mental disability, genetic information, veteran status, gender identity or expression, sexual orientation, or other applicable legally protected characteristic. Notion considers qualified applicants with criminal histories, consistent with applicable federal, state and local law. Notion is also committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation made due to a disability, please let your recruiter know.
Notion is committed to providing highly competitive cash compensation, equity, and benefits. The compensation offered for this role will be based on multiple factors such as location, the roles scope and complexity, and the candidates experience and expertise, and may vary from the range provided below. For roles based in San Francisco, the estimated base salary range for this role is 130,000 - 250,000 per year. 
LI-Onsite

"
https://startup.jobs/data-engineer-hellofresh-4454967,Engineer,Data Engineer,HelloFresh ,"Amsterdam, Netherlands",,"

HelloFresh has been recognized as the 5 Best Place to Work in 2022 by Great Place to Work Netherlands!  Check out our amazing team and office here and learn more on our socials On LinkedIn and Instagram! Ready to join our team?
 This is how youll change the way people eat foreverWe are looking for a Data Engineer for our ambitious, fast growing team in Amsterdam. Working in the Data Engineering team you take a leading role in developing and optimising our data processes, like our route planning, ordering and production optimisation. At HelloFresh, the Data Engineering team works together closely with all operational teams such as Procurement, Logistics and Production, to ensure we maximise efficiency and optimise our customer experience. Our processes are tailored to each specific business need, which means we need to challenge ourselves to keep an agile mindset, without losing focus on scalability and reliability of our tooling landscape.
Join an ambitious engineering team and get the opportunity to work on crucial tools that enable our supply chain to scale further - Jeroen Entjes, Team lead Data Engineering
 This is how youll make an impact


Develop new workflow tools in Python using Airflow, to help service our customers now and in the future


Provide reliable and scalable tech solutions to the business by using the latest technologies such as Kubernetes, AWS but also our own developed tools


Build key features in our supply chain tech products and collaborate with stakeholders in our procurement, logistics and production teams to get their inputs


Define new technological standards with your team and drive change and improvements in our tools and processes


 These are the ingredients youll bring to the team

3-5 years of experience in total, of which at least 2 years in a technical role 
Experience with React and TypeScript is a big plus
Solid experience with Python you are able to extract, transform and load data, for example using Pandas and familiarity with its style guide
Some experience with SQL you are able to load a table from a data warehouse
Familiarity with agile methodologies, preferably Scrum
Able to translate business questions into data solutions
Fluent in English
Knowledge of AWS and Airflow is a plus
Bachelors degree or higher in a technical field

 This is our recipe 

Freedom to initiate, execute and implement your own ideas
The opportunity to engage and collaborate with our colleagues across the globe
Learning and development opportunities including a personal development budget
24 holidays per year, plus 1 day per additional year of work
A hybrid work setup, with a budget to help set up your home office!
Flexibility to work abroad
Reimbursement of travel expenses to the office 
A discount on your weekly HelloFresh box
Our FreshEnergy program including bootcamp, yoga, boxing, Headspace subscription, a collaboration with OpenUp for your mental wellbeing, webinars and other events
Multiple ways of getting to a work-related appointment, for example by using our company cars or a MyWheels car
Discount on your ClassPass subscription, and a discount for your gym membership via High Five Fitness Network. 
Discount on your additional health insurance through Zilveren Kruis 
Healthy lunches, fruit and snacks in the office 
Monthly pension scheme 

 Grow your career at HelloFresh 
A career at HelloFresh is an opportunity to make an impact in a world-leading meal-kit company. Through our Learning Never Stops core value, we empower you to take your career development to your highest potentialbut we dont just want to say it, we want to show it too. 
Check out some amazing growth stories here 

Joran van der Hels journey from driver to Ground Operations Manager
Valerie Blanks journey from Junior Sampling Coordinator to Team Lead of Product Marketing

As an Engineer at HelloFresh you work closely with an experienced team in our Amsterdam office, while also being able to work together with other officers or our global tech team. If you want a change of scenery or career paths, there will be many opportunities to do so within the international HelloFresh group!
 Hungry for more?

Apply with your CV and a strong motivation to join us In English please
If we see a match, youll have a video call with one of our recruiters to share more about what you can bring to the team. 

Are both sides still positive? Then we proceed to the assessment stage to assess key skills required for the job. 

Next up is an in-person interview with the hiring team to get to know you better and dive into more detail of your skills and how you are going to make an impact. 
Lastly youll have a conversation with our Director of Process Excellence



"
https://startup.jobs/data-engineer-data-platform-team-remote-constructor-4453738,Engineer,Data Engineer: Data Platform Team (Remote),Constructor ,"Lisbon, Portugal",Full-Time,"

About us
Constructor.io powers product search and discovery for some of the largest retailers in the world. We serve billions of requests every week, and youve probably seen our results somewhere and used our product without knowing it. We differentiate ourselves by focusing on metrics over features, and reinventing search and discovery from the ground up as a machine learning challenge with the specific goal of improving metrics like revenue. Were approximately doubling year over year despite the market slow down and have customers in every eCommerce vertical. Were a passionate team of technologists who love solving problems and want to make our customers and coworkers lives better. We value empathy, openness, curiosity, continuous improvement, and are excited by metrics that matter. We believe that empowering everyone in a company to do what they think is best can lead to great things.
 Data Platform Team
The Data Platform team within Data Science and Engineering is an integral unit that serves internal stakeholders. It develops a data platform that
 Stores, processes data, produces artefacts the is used by the backend team to run in production Convenient tools for engineers to create, schedule, and run their data workloads. Data quality validation. Real-time Analytical API that serves analytics to our customers.  Data Science and Engineering consist of a mix of data engineers  analysts owning  collaborating on multiple projects. As a Data Platform team member, you will use world-class analytical, engineering, and data processing techniques to build the foundational infrastructure, tooling, and analytical capabilities and enable the business to move forward.

Challenges you will tackle
Constructor integrates with our customers by providing them with client-side libraries that interact with our API. These libraries transmit logsevents behavioral events that Constructor uses to
 Train ML models Make business decisions Conduct AB tests and so much more 
Logs are handled by Behavioral API  a Python service that collects them and stores in S3. This piece of infrastructure is owned by another team, and were about to take ownership to control the full data cycle from start to finish.
So your first task is
 Separate the API and deploy it as a standalone service Integrate it with the Data Platform Team infra 
We also have other long-term goals for the team, such as
 New task scheduler AirflowPrefectetc. Recommendation systems DB Performance and stability improvements of Analytics Service Spark pipelines cost and performance optimization A smarter data model for the DWH 
Requirements
 You have high proficiency in any programming language Python is preferred. You have experience with backend development with any web framework for Python it can be Django, Flask, FastAPI, . You are proficient at SQL any variant You have experience working with AWS and have knowledge of its services EC2, IAM, S3, Lambda, ECS, ECR,  used for data processing. You have an excellent understanding of data storage, database types and architecture, and you can apply this knowledge to build an effective data infrastructure. You enjoy working with big amounts of data You proactively find opportunities to improve the product and lead this endeavor to success. Bonus points experience working with ClickHouse Bonus points advanced knowledge of AWS CloudFormation, CDK, SNS, SQS Bonus points Familiarity with the big data stack Spark, PrestoAthena, Hive. 
Benefits
 Unlimited vacation time -we strongly encourage all of our employees take at least 3 weeks per year A competitive compensation package including stock options Company sponsored US health coverage 100 paid for employee Fully remote team - choose where you live Work from home stipend! We want you to have the resources you need to set up your home office Apple laptops provided for new employees Training and development budget for every employee, refreshed each year Parental leave for qualified employees Work with smart people who will help you grow and make a meaningful impact 

"
https://startup.jobs/data-engineer-intern-tiketcom-4453251,Engineer,Data Engineer Intern,tiket.com ,"Jakarta, Indonesia",Internship,"

We think you also hate when travel app is giving you a headache, right? A slight misinformation can ruin the trip.
That is exactly what we are tackling as t-fam! Making sure that our 17 million users have the best experience in crafting their own adventure.

LI-Hybrid
Your main duties in flying with us

Create low level framework architecture for ETL pipeline, using GCP, Apache Airflow, Kubernetes
Design and develop abstraction for the created ETL framework
Develop an automation system
Troubleshoot pipeline issues
Participate in scrum meetings, sprint planning, code reviews, etc. 

Requirements

Basic knowledge in Big Data Concepts is preferred
Basic SQLno-SQL skills select, insert, update, delete
Have hands-on experience in PythonJava is preferable
Have hands-on experience in Databases such as MySQL, Postgres, MongoDB
Comfortable with using git 



In the event that you havent received any updates after 3 weeks, your data will be kept and we may contact you for another career destination. Meanwhile, discover more about tiket.com on Instagram, LinkedIn, or YouTube.


"
https://startup.jobs/data-engineer-bi-farfetch-2-4452072,Engineer,Data Engineer - BI,Farfetch ,,Contractor,"

FARFETCH exists for the love of fashion. Our mission is to be the global platform for luxury fashion, connecting creators, curators and consumers.
Were a positive platform for good, bringing together an incredible creative community made up by our people, our partners and our customers. This community is at the heart of our business success. We welcome differences, empower individuality and celebrate diverse skills and perspectives, creating an inclusive environment for everyone. We are FARFETCH for All.

TECHNOLOGY
Were on a mission to build the technology that powers the global platform for luxury fashion. We operate a modular end-to-end technology platform purpose-built to connect the luxury fashion ecosystem worldwide, addressing complex challenges and enjoying it. Were empowered to break traditions and revolutionize, with the freedom and autonomy to make a difference for our customers all over the world.

PORTO
Our Porto office is located in Portugals vibrant second city, known for its history and its creative yet cozy environment. You can have an informal meeting in the treehouse or play the piano during your lunch break!

THE ROLE
Farfetchs Data Teams are focused on everything related to data. Their main purpose is to harness the power of Farfetchs data to deliver insights and reports that support business decisions and also analyze and discover new ways to amaze our customers. These teams cover multiple areas related to data, such as  Business Intelligence, Software and Data Engineering, Data Science and Data Analytics.
Just as the rest of Farfetch, Data Teams are committed into turning the company into a leading e-commerce platform. As so, they are constantly looking for brilliant people who like the challenges that a fast growing, data driven company faces in its path to become a market leader.  
You will be integrated in the Data Engineering team, being responsible for helping maintain and improve the BI architecture and tools.
WHAT YOULL DO

Design and build scalable  reliable data pipelines ETLs for our data platform
Constantly evolve data models  schema design of our Data Warehouse to support self-service needs
Work cross functionally with various teams, creating solutions that deal with large volumes of data.
Work with the team to set and maintain standards and development practices;
Be a keen advocate of quality and continuous improvement.

WHO YOU ARE

You have 2 years of experience as a professional with solid technical background building and maintaining data pipelines in a custom or commercial ETL tool eg. SSIS, Talend, Informatica Airflow is a plus;
You have 2 years of experience working in a Data Warehouse environment with varied forms of data infrastructure, including relational databases, Hadoop, and Column Store; 
Proficient in creating and evolving dimensional data models  schema designs to improve accessibility of data and provide intuitive analytics;
Experience working with cloud environments eg. AWS, GCP, Azure plus;
Proficient in SQL;
You have basic understanding of HadoopBigData ecosystem HDFS, Hive;
Proficient in one of the following programming languages C, Java, Python;
Basic knowledge in distributed computing Spark;
Experience in working with a BI reporting tool eg. Tableau, QlikView, PowerBI, Looker plus;
You have basic understating of continuous delivery principles version control, unit and automated tests;
You have an intermediate level in English, both written and spoken;
You have good analytical and problem solving skills, the ability to work in a fast moving operational environment and you are enthusiastic and with a positive attitude;

REWARDS AND BENEFITS

Health insurance for the whole family, flexible working environment and well-being support and tools
Extra days off, sabbatical program and days for you to give back for the community
Training opportunities and free access to Udemy
Flexible benefits program
FARFETCH Equity plan

EQUAL OPPORTUNITIES STATEMENT
FARFETCH is an equal opportunities employer ensuring that all applicants are treated equally and fairly throughout our recruitment process. We are determined that no applicant experiences discrimination on the basis of sex, race, ethnicity, religion or belief, disability, age, gender identity, ancestry, sexual orientation, veteran status, marriage and civil partnership, pregnancy and maternity, or any other basis prohibited by applicable law. We continue to build our consciously inclusive culture as part of our Positively FARFETCH strategy throughout our business, partnerships and communities.

You will be integrated in the Data Engineering team, being responsible for helping maintain and improve the BI architecture and tools.

"
https://startup.jobs/data-engineer-remote-us-seamlessai-2-4450541,Engineer,Data Engineer - Remote US,Seamless.AI ,,,"

The Opportunity
We are seeking an experienced Data Engineer with a minimum of 3 years of experience in building data ingestion pipelines for large datasets. As a key player in our team, you will be responsible for designing, building, and managing our data infrastructure, supporting our data-driven decision-making capability. Youll need to be proficient in Python, AWS, and common frameworks used for data ingestion, transformation, and consolidation. The ideal candidate will be passionate about data, a strong team player, and have a continuous learning mentality.
About Seamless
Seamless delivers the worlds best sales leads. Through our product, we help sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence; by development of a robust real-time contact and company search engine as well as a suite of technically-advanced tools to support sales and lead generation. We have been recognized as one of Ohios fastest growing companies and have been recently ranked No. 7 in LinkedIns Top 50 Startups of 2022, featured in Forbes as 1 Software company in Ohio in 2022, and on G2s Top 100 Highest Satisfaction Products for 2022 list! 
The Seamless Team
We have an amazing culture and work environment that anyone would want to be a part of. We encourage a culture of positivity. We thrive off of continuous feedback and do whatever it takes to help our team and customers be successful. You will grow as an individual, professionally, and be able to see and feel the impact you are making to the growth of Seamless every day.
Role Responsibilities

Design, develop and optimize data ingestion pipelines to handle real-time and batch data streams using a variety of sources.
Utilize your extensive knowledge of Python and AWS to engineer solutions for the transformation, consolidation, and storage of large datasets.
Collaborate with data scientists and other stakeholders to understand data needs and translate them into data systems and pipelines.
Enhance our data ecosystem by leveraging industry best practices for testing, deployment, and runtime environments.
Drive continuous improvements to data reliability, efficiency, and quality.
Document data architectures, procedures, and data flows, maintaining excellent communication with the team and stakeholders.
Monitor data systems performance, troubleshoot data issues, perform root cause analysis, and ensure the implementation of optimal solutions.
Participate in data governance and ensure adherence to data security and privacy standards.

Candidate Requirements

A minimum of 3 years of experience as a Data Engineer or in a similar role.
Strong proficiency with Python, AWS, and common frameworks used in data ingestion and transformation.
Hands-on experience building and optimizing large scale data pipelines, architectures, and data sets.
Knowledge of data warehousing concepts, including data modeling, data cleaning, and ETL processes.
Strong understanding of database design and data management principles.
Experience with AWS cloud services such as EC2, S3, Redshift, DynamoDB, and others.
Strong problem-solving skills, and the ability to analyze data and design solutions to complex data issues.
Excellent communication and teamwork skills, and a passion for data.
Experience with other programming languages e.g., Java, Scala is a plus.
Familiarity with big data tools e.g., Hadoop, Spark is a plus.

Seamless.AI has been delivering the worlds best sales leads since 2015. Our product is the first real time, B2B search engine helping sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence. We have been recognized as one of Ohios fastest growing companies and won 2020 Best Places to Work and LinkedIns Top 50 Tech Startups in 2020 and 2022. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Visa Sponsorship is not included in our hiring package. Applicants will need to be authorized to work in the U.S.

"
https://startup.jobs/data-engineer-mongodb-4449037,Engineer,Data Engineer,MongoDB ,"New York, United States",,"

The worldwide data management software market is massive IDC forecasts it to be 137.6 billion by 2026!. At MongoDB we are transforming industries and empowering developers to build amazing apps that people use every day. We are the leading modern data platform and the first database provider to IPO in over 20 years. Join our team and be at the forefront of innovation and creativity.
MongoDB is growing rapidly and seeking a Data Engineer to be a key contributor to the companys Internal Data Platform. You will build ETL pipelines that pull data into our Data LakeWarehouse and that will be used to drive forward our growth as a product and as a company. You will take on complex data-related problems using very diverse data sets, and will work with stakeholder groups throughout the company to help them make better data-informed decisions.
This role is based out of our New York City office.
Our ideal candidate has experience with

Building ETL pipelines at scale that can grow without sacrificing performance
Data LakeWarehouse design patterns and concepts, including Delta Lakes
Several programming languages Python, Scala, Java, etc.
Data processing frameworks such as Spark and Pandas
Orchestration tools such as Airflow, Luiji, Azkaban, Cask, etc.
AWS services such as S3, Kinesis, EMR, Lambda, Athena, Glue, IAM, RDS, etc.
Different storage formats such as Parquet, JSON, Avro, and Arrow
Streaming data processing frameworks like Kafka, KSQL, and Spark Streaming
A diverse set of databases MongoDB, Redshift, etc.

You might be an especially great fit if you

Enjoy wrangling huge amounts of data and exploring new data sets
Value code simplicity and performance
Obsess over data everything needs to be accounted for and be thoroughly tested
Plan effective data storage, security, sharing, and publishing within an organization
Constantly thinking of ways to squeeze better performance out of data pipelines

Nice to haves

You are deeply familiar with Spark andor Hive
You have expert experience with Airflow
You understand the differences between different storage formats like Parquet, Avro, Arrow, and JSON and when to use each
You understand the tradeoffs between different schema designs like normalization vs. denormalization
In addition to data pipelines, youre also quite good with Kubernetes, Drone, and Terraform
Youve built an end-to-end production-grade data solution that runs on AWS or GCP
You have experience building machine learning pipelines using tools such as SparkML, Tensorflow, Scikit-Learn, etc.

Responsibilities
 As a Data Engineer, you will

Build large-scale batch and real-time data pipelines with data processing frameworks including Spark and Kinesis
Help drive best practices in continuous integration and delivery
Help drive optimization, testing, and tooling to improve data quality
Collaborate with other software engineers, machine learning experts, and stakeholders, taking learning and leadership opportunities that will arise every single day

To drive the personal growth and business impact of our employees, were committed to developing a supportive and enriching culture for everyone. From employee affinity groups, to fertility assistance and a generous parental leave policy, we value our employees wellbeing and want to support them along every step of their professional and personal journeys. Learn more about what its like to work at MongoDB, and help us make an impact on the world!
MongoDB is committed to providing any necessary accommodations for individuals with disabilities within our application and interview process. To request an accommodation due to a disability, please inform your recruiter.
MongoDB, Inc. provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type and makes all hiring decisions without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

"
https://startup.jobs/data-engineer-jumo-4447983,Engineer,Data Engineer,JUMO ,,,"

About this job
 
At JUMO, weve built a smart financial services platform to make finance accessible to everyone.  Our Data engineering team ensures that our intelligent banking platform is able to acquire data from millions of users across globally distributed sources, such as GSM. Beyond ingress, the data is processed and stored in the cloud to support event-driven analytics and machine learning pipelines.
 
As a JUMO Data Engineer, you will design, build and maintain scalable data architectures.  You will monitor their performance, perform root cause analysis and provide solutions to any issues that might arise. You will work with large, complex data sets, which means experience working with Big Data is essential.  On a typical day, we use Spark, Kafka, Python, SQL, Airflow and AWS technologies, such as Redshift.  We work with Docker and Kubernetes for automating deployment, scaling, and management of containerized applications.  
 
You will

Be responsible for creating robust, mission critical batch and streaming data processing capabilities
Design, implement, and maintain the data pipelines that constitute JUMOs data platform, enabling effective use of data across the organisation
Provide feedback on team members output, encouraging skills development within the team
Work closely with Portfolio Managers and Decision Scientists to understand the real world problems were trying to solve
Be supported by senior leaders as you drive your own development

 
You will need

BSc. in Computer Science, Electrical Engineering or equivalent tertiary degree
Real-world understanding  experience of data pipeline design and development, as well as data processing and storage i.e. experience in designing systems to process and curate large data sets
Experience in streaming technologies preferred, specifically Spark and Kafka 
Experience with cloud technologies, AWS preferred
Experience working with big-data technologies such as Apache Spark, Flink, Hadoop, Kafka or Kinesis, Cassandra, DynamoDB, InfluxDB, MongoDB, Presto, Beam
Command of productionising and monitoring of data pipeline workflows, and working knowledge of the Data Product Lifecycle
Experience in application design and development with at least one of the following languages Python preferred, Scala, Java
RDBMS experience in any relevant technology such as MySQL, PostgreSQL, Redshift and SQL Server.  This includes experience with relational database administration, technical architectures and infrastructure components
Understanding of CICD practices
Productive within a Linux command line environment
Proven ability to contribute software as part of a team.  We are looking for someone that can effectively communicate technical concepts, and apply critical thinking under pressure

Bonus if you have

Experience working with messaging systems RabbitMQ, SNS, Kafka
Experience working with data pipeline orchestration Airflow, Nifi, Streamsets
Experience working with production BI environments and tools Tableau, Superset, Looker

 
We ask a lot of each other at JUMO, but we give a lot too.
 
You will love

Collaborating with smart, engaging people in an inspiring work environment
Working for impact
Growing and learning continuously, with loads of encouragement and support
Boldly taking risks as we navigate new challenges
Flexible work practices enabling your best delivery
Being autonomous and empowered to lead
A stack of leading-edge technologies

 
Remote First
We operate a remote first working approach, where working remotely is our default way of working. Our environment is designed to foster innovation and enable collaboration. You have flexibility where to work from between time zone UTC 0 and UTC 3, as long as you are set up to work remotely and have access to data with a strong and reliable connection as we value online facetime for collaboration at JUMO.
 
Diversity and Inclusion
At JUMO, we believe that diversity strengthens our teams and we strive in our recruitment process to create an environment where people from every background can collaborate and prosper and be themselves.

"
https://startup.jobs/data-engineer-our-future-health-4444417,Engineer,Data Engineer,Our Future Health ,"London, United Kingdom",Full-Time,"

We are looking for Data Engineers to help solve some of the key challenges around a programme of work at industrial scale with global significance. The successful Data Engineer will be contributing towards the delivery of data releases that will be used worldwide and will have experience with either NHS data or Bioinformatics and genetic data. This is a role with an inspiring set of data challenges, for what will eventually be a comprehensive view of the health of 5M people in the UK, including directly gathered information, genetics, NHS records and other linked data.  Our Future Health will be the UKs largest ever health research programme, bringing people together to develop new ways to detect, prevent, and treat diseases. We are a charity, supported by the UK Government, in partnership with charities and industry. We work closely with the NHS and with public authorities across all nations and regions of the UK.  What youll be doing Youll be part of a multidisciplinary team thats creating pipelines that didnt exist before, owning them in production and improving them over time. Your key responsibilities will include but not be limited to  Supporting the build of data pipelines from data providers to our primary data store and trusted research environment. Producing logic for data transformation steps as code, which meet the requirements for our end users and builds well curated, accessible and quality controlled data for analysis. Developing prototypes for pipelines for complex transformations drawing on existing workflows developed in industry and academia. Keeping abreast of best practice in data engineering across industry, research and Government and facilitating the adoption of standards. Providing technical input into the upstream parts of the data pipeline, including the specification and transfer of data from data providers. Ad-hoc data curation activities requiring hands on development of bespoke ETL cleaning scripts using languages such as SQL and Python. Working with researchers to understand the data requirements and helping them to deliver the data needed for their projects.  What you wont be doing  Working in a siloed environment with no freedom to make decisions. Working in a place where you cant see the impact your expertise makes. 
Requirements
To succeed in this role, you will have some of the following skills  The ability to communicate to and between technical and non-technical stakeholders as well as facilitate discussions within a multidisciplinary team, managing different perspectives. A good knowledge and understanding of NHS data such as hospital administrative data, disease registries or primary care data, and how they can be used to support research OR solid understanding and experience of bioinformatics, in particular tools and methods associated with genomic data. Ability to design, build and test pipelines based on feeds from multiple systems using a range of different technologies. You will understand how to create repeatable and reusable products.  Comfortable in designing an appropriate metadata repository and presenting changes to existing metadata repositories. You understand a range of tools for storing and working with metadata.  Knowledge of health record coding systems and data standards e.g., ICD, READ and SNOMED codes. Proficient in a variety of data engineering programming languages and environments such as Python and SQL. An understanding of the impact of emerging trends on the organisation in data tools, analysis techniques and data usage. Understanding and working knowledge of information governance and data security approaches appropriate for sensitive health data. 
Benefits
 Up to 60,000 per annum basic salary. Generous company pension package with employer contributions of up to 12. 30 days annual leave plus bank holidays. Individual development budget Flexible and remote working arrangements and a lovely new office in Holborn, Central London.  Join us - lets prevent disease together.

"
https://startup.jobs/data-engineer-sophia-genetics-4444232,Engineer,Data Engineer,SOPHiA GENETICS ,"Lausanne, Switzerland",Full-Time,"

We believe there is a smarter, more data-driven way to make decisions in health. As we pass 1,000,000 genomic profiles analyzed and look to the future of our platform, we are now searching for a Project Manager who will be help in transitioning our platform to the next level
Our mission is to bring data analytics solutions to market, to support healthcare professionals by maximizing the power of Data-Driven Medicine. SOPHiA GENETICS NASDAQ SOPH combines collective intelligence in Genomics, Radiomics, clinical research data, the DDM -Data-Driven Medicine,we can ensure that the data used to help patients today will also benefit the patients of tomorrow.
We observed that across the healthcare ecosystem, a vast amount of digital healthcare data is being generated, fuelled by technologies such as next-generation sequencing ""NGS"". This data holds promise to accelerate our understanding of biology and disease.
In our journey to contribute to the company mission on patients lives, we are looking for a Software EngineerData Engineer to join our Data Engineering Team! The Data Engineering Team specifies, designs, implements and maintains data-centric systems that are vital for several business processes of the Data Science department.
As a Data Engineer you will directly collaborate with other software engineers, bioinformaticians, product managers and laboratory technicians to gather requirements and transform them into efficient analytical applications. Modelling of genomic information and the design, implementation and optimisation of algorithms to process biological data represent some of the main activities of the team. Additionally, you will be involved in the design and development of data-centric processes, as well as in their integration with existing information systems e.g. LIMS. All such systems are subject to rigorous design, verification and validation processes in order to meet software medical device regulations such as ISO 62304, FDA 21 CFR 820.30 and EU IVD. 
Note this position does not involve data analyst-related activities such statistical analysis or application of machine-learning methods.
Requirements
 PhD in Information Systems, Computer Science or related field or Master and equivalent working experience 3 years or more; Solid knowledge of the algorithmic e.g. analysis of timespace complexity, data structures and database e.g. relational systems, ISO SQL or DBMS-specific SQL domain; Proficient in more than one general purpose programming languages e.g. Java, CC, Python, etc.; Good knowledge of the domain of software engineering, especially of design patterns and architectures; Academic-level knowledge of design of experiments and statistical methods including Statistical Process Control, test of hypotheses  ANOVA, non-parametric tests-, main effect and multi-vari analysis, Response Surface Analysis, linear regression is a plus; Good knowledge of UnixLinux development environments; Analytical mindset and attitude to logical formalisation; Full professional proficiency in English; Good writing and oral communication skills; Good teamwork skills and autonomy. 
Benefits
You will be joining an organisation with the patient at the heart of every decision and action, driven by purpose as we drive exponential growth.
Business recognition and accolades include
 Worlds most innovative companies Top 10 Worlds smartest companies Top 50 100 Best Places to Work in Boston Top 10 European Tech Startup - 2020 Top 10 European biotechs startup to watch - 2021 Top 25 East-Coast Biotech to watch - 2021 
Plus, you will find 
 A flexible, friendly, and international working environment with a collaborative atmosphere Flexible working hours, including hybrid remote  on-site working model An exciting company mission that brings together science and technology to directly impact the lives of patients with life threatening illness. A fast-growing company with plenty of opportunity for personal growth and development A hard technical challenge to solve with exciting modern technology - cloud computing, Big Data, DevOps, machine learning 
Location Rolle, Switzerland. Office-based, including 2 days per week remote  3 days on-site.
Start ASAP or as agreed
Contract type permanent, full-time

Application process
If you think you fit this position, please send a CV and a cover letter. Please note that incomplete applications will not be considered.
After an initial screening process, candidates will be invited for remote interviews.

"
https://startup.jobs/data-engineer-remote-us-human_api-4441716,Engineer,Data Engineer (remote) US,Human API ,,Full-Time,"

At Human API a LexisNexis Risk Solutions Company, we exist to radically accelerate the pace of health innovation for everyone. That is our vision. We unlock siloed health data from everywhere and put it directly into the hands of the consumer, right when they need it enabling a meaningful transaction that creates value in their lives. Some call us the ""PayPal"" of health data, but our dreams are bigger. 

To name a few examples, we help rapidly screen participants for clinical trials, enable life-changing software through accurate wearable data, and we accelerate the buying process for important insurance products.  Tomorrow we will inspire the innovation of impactful digital health products to maximize human longevity and potential. We are looking for talent who are equally inspired by these big ideas and have the grit and determination to make them come to life.

Were looking for people who are good at engineering systems to manipulate, process, and make sense of data. If youre an expert in some areas of data engineering, but not others, well train you up. If you have the knowledge and skills to architect and design systems, well let you do that and follow your lead. 

The core of this role is taking complex data and making it accessible for others.
What youll do

Youll build out our data platform capabilities across a wide selection of themes pipelines, graphs, ml models, annotations, automation, data observability, data lifecycle management, data governance, semantic layers using a set of core services currently AWS, Databricks, and Looker
Manage and scale data pipelines across the terabytes of arbitrarily complex data in our data lake, both structured and unstructured
Performance tuning and scaling internal and external real-time data services

About you

At least 5 years working as an individual technical contributor, in a software engineering role
3 years working with distributed data systems and data engineering
Streaming processing semantics and debugging streaming applications

Advanced proficiency in

Design and maintenance of pipelines built atop distributed data technologies DatabricksSpark, Snowflake, BigQueryRedshift, KafkaKinesis, etc. 
All aspects of managing distributed systems design, scalability, security, deployment, observability, resiliency
CICD principles even better applying devops practices to data
Workflow orchestration Airflow, Prefect, AWS Step Functions, etc.
SQL and Python, and willingness to learn other languages
Data modeling and query optimization

Qualities that will help you succeed

Keen eye for opportunities to better leverage time both your own and others
Desire to take complex problems and making them simple for others
A desire to help others grow
A commitment to continuous self improvement
An ability to operate both individually and collaboratively

Bonus 

Experience with analytics engineering working with dbt, Looker
Experience with machine learning engineering and NLP problems like entity recognitionresolution
Experience with privacy engineering 




LexisNexis Risk Solutions and Human API will deliver a next-generation consumer consent management solution that enables more seamless delivery of data. This approach empowers consumers with better access to their healthcare data and insurance resources that can improve care coordination and automate life insurance underwriting. Currently serve some of the largest financial and health companies in the world from Prudential, Allstate, John Hancock, Omada Health, Thrive Global, AAA, and many more. 

We have thousands of API integrations into enterprise health record systems, wearable devices, as well as health and wellness applications, that all benefit the end consumer and our enterprise customers.  Were looking for independent thinkers who care deeply about the problems were trying to solve. At Human API, we welcome people of all backgrounds.

LI-remote, Trabajo remoto


"
https://startup.jobs/data-engineer-odaia-intelligence-4440204,Engineer,Data Engineer,Odaia Intelligence ,,,"

ABOUT ODAIA

ODAIA nounodaia   oh-day-yeah  
An Ancient Greek word referring to a salespersons tools of the trade.


Learn more about our company here. You can also find more information about the company and our products at odaia.ai.


ODAIA is a remote first organization, all our positions are WFH. 

ODAIA helps pharma sales and marketing teams by using AI to understand and predict customer behaviour. Our enterprise SaaS product MAPTUAL serves some of the most recognized Fortune 500 life sciences companies. By incorporating data and AI predictions in powerful ways, MAPTUALs users understand their customers healthcare professionals, patients on a deeper level and cater to their health needs more effectively. 

ODAIAns what we call ourselves are inspired to reinvent the future of how non-technical people leverage data in their day-to-day lives. We are passionate about solving complex problems in data, AI, engineering, design, and product, so our customers dont have to. We live by the notion that simplicity is the ultimate sophistication; and making simplicity scalable is an even bigger challenge. Thats why we have a crazy talented team led by serial entrepreneurs, tech veterans, and life sciences experts.

OUR MISSION

Reducing patients time to therapy by facilitating meaningful interactions with healthcare providers, through human-centric software powered by AI. 

Were also on a mission to build an innovative, diverse, and ego-free business, where trust, innovation and ownership are valued. Youre on a mission too? Were here for it. We put an emphasis on career development for our employees, and the opportunities to grow are extensive.

WHATS ON OFFER


This is an excellent opportunity to join a rapidly growing team in a technically challenging and rewarding role! Reporting to the Machine Learning Data Scientist Development Manager, the Data  Engineer is experienced and passionate about building creative and novel data solutions. Working collaboratively with other feature teams and cross-functionally throughout the development process, the Data Engineer ensures the product is functionally complete and technically solid. 

WHAT YOU WILL DO 

Understand and learn our data quality, data flow processes while identifying and creating solutions to improve
Create and maintain the data documentation dictionary, entity relationship diagram, etc.
Enhance, and monitor the data processes to ensure consistency and quality
Troubleshoot and provide answers and solutions to ad hoc requests regarding the current data pipeline while unblocking the Engineering and Product teams
Work closely with our Machine Learning team to provide data solutions on large scale projects


WHAT YOU BRING 

3 years of experience working in a role related to Data Engineering
Prior experience in data warehouse  data lake
Understanding of AWS cloud technology for example S3, Glue, Athena, etc 
Hands-on with stored procedures, triggers and implementing CDC 
Familiar with both row and columnar storage databases
High proficiency with SQL programming and professional competency with Python
Experience and knowledge of Test-Driven Development, DevOps, CICD would be an asset.
Experience with troubleshooting the SQL queries, code profiling, and SQL query optimization. 
Problem solver and independent thinker


WHATS WE OFFER 

A highly productive driven startup culture that values agility, passion, ownership, trust, respect, diversity and inclusion
Tremendous growth and unique learning opportunities
A large amount of ownership and autonomy for managing things directly
An inclusive environment, we are an equal opportunities employer and we operate with an anti-oppression mindset
Top-notch health benefits for medical, dental, vision care, mental health, prescription drug coverage, travel insurance and alternative treatments such as acupuncture and chiropractic services
Flexible working hours, providing great balance for personal and family life
We focus on what we achieve and not the number of hours we clock
We trust our employees and empower them to shape their work themselves so that they can achieve the best possible results
An open and flexible vacation policy
Our employees can take what they want, when they want it - as long as they get their work done, get the time approved and get things covered while theyre away
Stock option grants
A generous home office allowance
Career development opportunities, and a solid business model - were in it for the long haul!


LOCATION

We are a remote-first company based in Toronto with frequent in-person collaborative work sessions and social gatherings at a work-share office in downtown Toronto. We leverage many synchronous and asynchronous communication tools as well as virtual social events, and provide an allowance for your ideal WFH setup. 

EMPLOYMENT VERIFICATION

Any conditional offer of employment made to a successful candidate will be subject to the full satisfaction with the results of any background and reference checks.

ACCOMMODATIONS AND ACCESSIBILITY

Accommodations are available on request for candidates with disabilities taking part in all aspects of our hiring process. For more on this, you can inquire about accommodations if youre invited to an interview.

DIVERSITY, EQUITY  INCLUSION

As an equal opportunity employer, ODAIA is committed to creating an environment that respects diversity and inclusion. ODAIA does not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, parental status, veteran status, or disability status.
At ODAIA, we are committed to building an environment where everyone feels included, valued, and heard. We are committed to creating a diverse workplace, and are an equal opportunity employer who does not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, parental status, veteran status, or disability status.
We strongly encourage applications from Indigenous Peoples, racialized people, people with disabilities, people from gender and sexually diverse communities andor people with intersectional identities.

"
https://startup.jobs/data-engineer-pr4753-nisum-4440128,Engineer,Data Engineer - Pr4753,Nisum ,"Hyderabad, India",Full-Time,"


Nisum is a leading global digital commerce firm headquartered in California, with services spanning digital strategy and transformation, insights and analytics, blockchain, business agility, and custom software development. Founded in 2000 with the customer-centric motto Building Success Together, Nisum has grown to over 1,800 professionals across the United States, Chile,Colombia, India, Pakistan and Canada. A preferred advisor to leading Fortune 500 brands, Nisum enables clients to achieve direct business growth by building the advanced technology they need to reach end customers in todays world, with immersive and seamless experiences across digital and physical channels.

What Youll Do

Analytical Skills Ability to make sense of large amounts of data and find insights.
Communication Capable of explaining and presenting complex data in a simple, understandable way to non-technical stakeholders.
Problem-Solving Ability to design and implement new features or enhancements to data systems based on business needs.
Detail Oriented Capable of detecting data-related problems, identifying root causes, and proposing solutions.
Adaptability Comfortable in a fast-paced environment with the ability to adapt to changing business needs.
Teamwork Ability to collaborate effectively with data scientists, BI analysts, IT professionals, and other stakeholders.
Continuous Learning The desire to constantly learn and grow, staying updated with the latest data trends and technologies.
Time Management The ability to prioritize tasks, manage time effectively, and deliver work on time.
Self-Motivated The ability to work independently and take initiative in improving data systems and processes.

What You Know

Over all 5 Years of experience. 
Proficiency in using AWS cloud services including but not limited to DynamoDB, RDS, Redshift, S3, Glue, and AWS Data Pipeline.
Experience with big data tools Hadoop, Spark, Kafka, etc.
Knowledge of various ETL techniques and frameworks.
Strong understanding of SQL and NoSQL databases, including DynamoDB.
Familiarity with data pipeline and workflow management tools.
Experience with building data pipelines using python.
Ability to work with large amounts of data, ensuring data quality and accessibility.
Experience in preparing data for predictive and prescriptive modeling.
Understanding of machine learning concepts and algorithms.

Education
Bachelors degree in Computer Science, Information Systems, Engineering, Computer Applications, or related field
Benefits

In addition to competitive salaries and benefits packages, Nisum India offers its employees some unique and fun extras

Continuous Learning - Year-round training sessions are offered as part of skill enhancement certifications sponsored by the company on an as need basis. We support our team to excel in their field.

Parental Medical Insurance - Nisum believes our team is the heart of our business and we want to make sure to take care of the heart of theirs. We offer opt-in parental medical insurance in addition to our medical benefits.

Activities -From the Nisum Premier Leagues cricket tournaments to hosted Hack-a-thon, Nisum employees can participate in a variety of team building activities such as skits, dances performance in addition to festival celebrations.

Free Meals - Free snacks and dinner is provided on a daily basis, in addition to subsidized lunch.


Nisum is an Equal Opportunity Employer and we are proud of our ongoing efforts to foster diversity and inclusion in the workplace.


"
https://startup.jobs/data-engineer-data-platform-team-newglobe-4436015,Engineer,"Data Engineer, Data Platform Team",NewGlobe ,"Amsterdam, Netherlands",,"

Data Engineer, Data Platform teams
Who We Are - About NewGlobe
NewGlobe is working with visionary governments around the world to dramatically improve the quality of basic education. Founded in 2007, we partner with governments to provide integrated school management, teacher professional development, instructional design innovation, technological system support, child-centered classroom practice, and parent engagement -- all grounded in learning science -- to ensure each teacher is empowered to engage children in transformational learning, Our data-driven approach has been validated by a Nobel-winning researcher and recognition by international leaders in Education. We imagine a world where all children can access an education that unlocks their full potential. 
We need bright minds who want to be part of building a new globe -- a more equitable globe -- to join us. More information newglobe.education
Technology Group
Technology plays a critical role enabling us to provide transformative education at massive scale in highly resource constrained environments. This is one of the key elements that gives us the ability to deliver radically impactful programmes, creating brighter futures for a generation of children across the world each day. Technology spans several key functions, including product management, business intelligence, software development and IT operations. 
Working with us, passionate technologists have a chance to directly change the world.
Software Development Department
The software development department handles all aspects of delivery related to our custom software applications and services - everything that it takes to turn an idea into working software and delivering value to our users. The department is structured into cross-functional, self-organizing teams that work together on projects, short and long, alongside product designers and key business stakeholders.
About the Role
NewGlobe has been data-driven from its inception, using a build-measure-learn approach to all aspects of service delivery. As a scale model business, we have an unprecedented opportunity to bring statistical power to all aspects of an educational operation, from the academics team using randomized controlled trials to validate the best teaching methods to the schools team understanding which factors contribute to teacher turnover to operational teams striving to improve our service delivery efficiency.
However, we have reached an inflection point where we need to take our data game to the next level. Instead of just providing data to our various departments, we need to start bringing to bear more sophisticated techniques to unlock the potential of our significant data sets. While we collect tens of thousands of data points every term from each of our academies  from the time spent on each page of a lesson to the billing and payment history of every pupil  we are yet to fully realize the potential energy of these data sets.
Were looking to build our next generation Data Platform team in Amsterdam, The Netherlands, alongside our software and product teams. The role of the Data Engineer is at the forefront of that transformation, ensuring our data models and data aggregation processes are robust, scalable, and ready for whatever we dream up next.
What You Will Do

Create and test data models for a variety of business data, applications, database structures and metadata tables to meet operational goals for performance and efficiency
Recommend solutions to improve new and existing database systems 
Execute the migration of data from legacy systems to new solutions
Monitor the system performance by performing regular tests, troubleshooting, and integrating new features
Drive technical innovation and lead on new technology exploration
Improve the scalability, stability, accuracy, and efficiency of our existing data systems
Contribute to the effective data governance of business data, including data quality, data management, data policies, business process management, and risk management surrounding the handling of business data
Communicate with business and outside actors to develop requirements for our data platform

What You Should Have


Strong Python and SQL programming skill

Fluency with warehouse design and dimensional modeling star schemas, snowflakes, etc
Experience with cloud MPP data warehouse platforms built on Spark, SQL, Hadoop and advocating for their appropriate adoption at NewGlobe
Familiarity Microsoft technologies, such as Microsoft SQL Server, SSIS, SSRS and .NET web APIs  current systems; 
Comfort with version control, continuous delivery, and test driven development
Experience or interest in results-driven, leanagile environments
Ability to prioritize and execute tasks in a fast paced environment

What We Offer

The opportunity to make a difference for children every day at a proven, internationally recognized, mission-driven company.
Flexible hybrid working arrangement.
Take the lead and help us grow our engineering team
The opportunity to work on challenging and innovative projects
1-week   quarter - work remotely from anywhere. 

Values of Successful Employees at NewGlobe
We are looking for new joiners who are energized by our mission and share our values. 
Detailed doers, creative problem-solvers, relentless advocates,  malleable learners, data-driven decision-makers and curious investigators do well at NewGlobe. Learn more about our values and how to succeed as a job-seeker at NewGlobe on our LinkedIn page.
 

"
https://startup.jobs/data-engineer-exodus-movement-inc-2-4435746,Engineer,Data Engineer,Exodus Movement Inc. ,,,"

Exodus is looking for a Data Engineer who is passionate about product telemetry and measuring the customer journey. In this role you will be responsible for architecting, building and maintaining warehouses of product telemetry and designing the interfaces the business uses to turn this data into decision guiding insight. We are looking for someone who thinks about the whole system, who can balance the necessity of stability and scalability with the pragmatism required at a startup. Most importantly, we are looking for someone with curiosity, who likes to draw the clarity of an answer from the murk of data. Are you the Data Engineer Exodus is looking for?
What You Will Do

With the team, decide which technologies should be used to gather and store essential Exodus data.
Design, prototype, and launch systems to process existing Exodus data in new ways.
Design, prototype, and launch systems that gather new data from public blockchains to better understand Exodus customers.
Build and maintain the API interfaces the business will use for accessing our product telemetry.

Who You Are

Experience with administering data warehousing technologies like Elasticsearch, BigQuery, Spark, etc.
Experience writing ETL jobs in a Kubernetes environment
Experience with high-capacity message queues like AWS Kinesis or Apache Kafka
Familiar with the AWS ecosystem of technologies
Comfortable with JS and applications written with node.js

Nice to Have

Passion for the cryptocurrency technology space
Familiarity with vendors in the data processing  warehousing space

 
About Exodus
Exodus is a multi-asset cryptocurrency wallet with a built-in exchange feature. We started our movement  in 2015 and have been a distributed team since then . Our mission is to help half of the world exit the traditional financial system and move into the crypto financial system by the year 2030. To do that we want to make sure we hire the best of the best people who are intrinsically motivated by what we are trying to achieve and who love what they do professionally. 
What We Offer

Freedom to work wherever you want, whenever you want.
Building the future. Cryptocurrencies lay the foundation to the internet of value, the next major wave in application technology and personal finance.
Collaborative and feedback-driven culture.
Opportunity to grow. 
Fair pay, no matter where you live along with a competitive benefits package.
100 pay in Bitcoin with a buffer to account for price changes and exchange fees. 
All the tools you need to do the job 

Benefits
Health Most of our health insurance plans are covered 100 for you and covered 50 for your dependents. Well also cover dental insurance. If you are outside of the United States, we will reimburse you up to 500 per month for any medical and dental insurance for you and your dependents.
PTO 30 days of paid time off per year on top of a flexible schedule where you can work wherever and whenever. If youre part-time with us, youll still receive 15 days of paid time off.
Unlimited Bereavement We will pay you your full salary for the first two weeks for the loss of any immediate family members but we allow you to take all the time you need to grieve outside of that.
Parental Leave 13 weeks of fully paid leave with and a month of flexible work for the primary caregiver. 4 weeks of paid leave if you are the childs secondary caregiver.
Tax Help Getting paid in Bitcoin new to you? Dont worry! We will reimburse you for speaking with a professional tax specialist in your statecountry to make sure everything is taken care of.
Perks Exodus offers a variety of seasonal perks such as coverage for gym memberships and therapy. We also offer quarterly Wellness Days! We want to make sure all of our employees know they are our priority and give back for your hard work often.
Our Hiring Process
Our hiring process consists of several different stages. 
Recruiter Interview If we like your initial application, expect to schedule an interview with a member of our recruitment team. This interview will focus on getting to know you a bit more and will focus on explaining the culture of the company. We want to know more about why you want to join our team, how you feel about our mission and cryptocurrency now and how it fits into your overall career plan to make sure this is the right place for you.
Assessments Not every role consists of an assessment but if yours does it will be directly related to the job you apply for.
Interview with your future manager This is to make sure that you are a fit for the role you are applying to and to explore your career history. In this interview, well learn more about the hard and soft skills you possess to help determine if youd be a good fit for us.
Interviews with your future colleagues We call these focus interviews and they are a time for you to learn more about your role from someone you will closely collaborate with. Its also a time for us to see how we align in terms of competencies and expected outcomes of the role.
Executive Behavioral Interview For our management roles, we take some more time before the end of the process to better get to know you and how well you align with our core values. Expect this call if you will be leading a team.
 

Pay Transparency Notice Salary and all other total compensation information bonus eligibility, benefits, and equity will be discussed in detail during the hiring process.
Salary Range

150,000184,000 USD



"
https://startup.jobs/data-engineer-ii-2k-19-4434428,Engineer,Data Engineer II,2K ,"Austin, United States",,"

Who We Are
2K is headquartered in Novato, California and is a wholly owned label of Take-Two Interactive Software, Inc. NASDAQ TTWO. Founded in 2005, 2K Games is a global video game company, publishing titles developed by some of the most influential game development studios in the world. Our studios responsible for developing 2Ks portfolio of world-class games across multiple platforms, include Visual Concepts, Firaxis, Hangar 13, CatDaddy, Cloud Chamber, 31st Union, and HB Studios. Our portfolio of titles is expanding due to our global strategic plan, building and acquiring exciting studios whose content continues to inspire all of us! 2K publishes titles in todays most popular gaming genres, including sports, shooters, action, role-playing, strategy, casual, and family entertainment.
Our team of engineers, marketers, artists, writers, data scientists, producers, thinkers and doers, are the professional publishing stewards of our growing library of critically-acclaimed franchises such as NBA 2K, 2K PGA, Battleborn, BioShock, Borderlands, The Quarry, The Darkness, Mafia, Sid Meiers Civilization, Marvels Midnight Suns, WWE 2K, and XCOM.
At 2K, we pride ourselves on creating an inclusive work environment, which means encouraging our teams to Come as You Are and do your best work! We encourage ALL applicants to explore our global positions, even if they dont meet every requirement for the role.  If youre interested in the job and think you have what it takes to work at 2K, we encourage you to apply!
What We Need
Data is the lifeblood of everything we do. We are a data-driven organization providing data products and insights throughout the 2K organization and related companies. We work directly with a wide variety of partners, from executive management to studio heads to analysts. We also partner in providing data to our players in the form of campaigns, promotions, and machine learning-based data products and recommendations.
What You Will Do

You will work in the capacity of a Data Engineer focusing on the data deliverables for Business and Studio Ingesting third-party data into our data lake and warehouse.
You will work with different vendor APIs and services. Working with vendor partners to understand the data they are providing, and document the data sets, lineage, and flows.
Designing and building data models to support Data Science, Business Intelligence, and downstream data sets. Building APIs and data products to better integrate data throughout our systems and processes. Monitoring the data pipelines and communicating any issues to leadership and partners.
Working closely with business partners and developers to ensure proper requirements are documented and agreed to for our different initiatives. Partnering with developers and leadership to coordinate cross-function and cross-team efforts

What Will Make You a Great Fit

3 years of professional experience as a Data Engineer in a fast-paced environment. An expert developer using SQL and SQL-like query languages
Have a strong understanding of different data modeling methodologies Kimball, Inmon, Data Vault. You understand when to apply the correct techniques to solve different problems on different technologies in our stack.
Have deep expertise in Python and have experience organizing Python-based projects. Youve built software services and APIs using Python frameworks such as Flask.
Experience with both proprietary and open-source big data technologies and platforms Snowflake, Vertica, Hive, Spark, Presto, Airflow.
Worked in a cloud environment AWS, Azure, GCP.
Experience defining and crafting automated unit and integration testing frameworks for data projects.
Functional understanding of different privacy and compliance practices around data GDPR, CCPA.
Excellent collaborator by tailoring your communication for different audiences and ensuring effective communication between developers, partners, and leadership.

Bonus Points

Video game andor entertainment and media industries

As an equal opportunity employer, we are committed to ensuring that qualified individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform their essential job functions, and to receive other benefits and privileges of employment. Please contact us if you need reasonable accommodation.
Please note that 2K Games and its studios never uses instant messaging apps or personal email accounts to contact prospective employees or conduct interviews and when emailing, only use 2K.com accounts. 
LI-OnsiteLI-BN1

"
https://startup.jobs/data-engineer-poatek-brazil-applicants-willowtree-4433544,Engineer,Data Engineer (Poatek - Brazil applicants),WillowTree ,"Porto Alegre, Brazil",,"

Data Engineer 
So Paulo-SP, Brazil or Porto Alegre-RS, Brazil
Who We Are
In 2021, Poatek was acquired by WillowTree, an award-winning digital product consultancy recognized as one of the fastest growing and best places to work in the United States. Now a TELUS International company, with offices across the globe, we continue to partner with the worlds leading brands, such as the NBA, Capital One, HBO, PepsiCo, Dominos and more, to design, build, and transform their digital products and strategies. Were looking to grow our Poatek offices in Brazil with top talent excited to collaborate with team members across the globe to deliver innovative solutions for our clients. 
Location and Flexibility
This is a hybrid role, which weve termed Work from Near WFN. This model requires the ability to work on-site from our So Paulo - SP or Porto Alegre - RS office throughout the week based on team needs, while still providing the flexibility to work remotely on other days. Our WFN culture is designed to foster in-person innovation, collaboration, and connection with team members local and visiting from other global offices.
The Opportunity
As a Data Engineer part of our growing Data Science team, you will be responsible for designing, implementing, and maintaining robust and scalable data pipelines, enabling efficient data integration, storage, and processing across our various data sources. You will collaborate with cross-functional teams, including Data Scientists, Software Engineers, and other technical stakeholders, to ensure data quality and support data-driven decision-making.
Responsibilities 

Be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams;
Create and maintain optimal data pipeline architecture;
Assemble large, complex data sets that meet business requirements;
Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc;
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, AWS, and Azure big data technologies;
Work with data and analytics experts to strive for greater functionality in our data systems.

Qualifications

BSc, MSc, andor PhD in Statistics, Mathematics, Physics, CS, or another related subject
Experience in the construction and optimization of data pipelines, architectures, and big data datasets
Proficiency with Apache Spark with a general-purpose programming language such as Python or Scala
Ability to create processes supporting data transformation, data structures, metadata, dependency, and workload management
Should also have experience using the following softwaretools

Experience with big data tools Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases
Experience with data pipeline and workflow management tools
Experience with AWS, Azure cloud services



Why Poatek?
Besides being part of an innovative consultancy company with an international culture, you will have

Be part of an innovative consultancy company
Flexible hours and autonomy
Work in international and agile projects
International travels
Voucher for health activities
English classes
Support in national and international courses
Food allowance
Snacks, drinks and video game in the office
Work in a company with highly-skilled professionals
Transportation or parking spot
Annual attractive bonus.


Equality is a principle here at Poatek. We are committed to building an inclusive team that represents a variety of backgrounds, perspectives, beliefs, and experiences. Therefore we provide equal employment opportunities to all employees and applicants regardless of race, color, religion, gender identity, sexual orientation, national origin, age, or disability.
 
We will only use the information you provide to process your application and to produce tracking statistics. Since we do not request personal data deemed sensitive, we ask you to abstain from sharing those information with us.
For more information on how we use your information, see our Privacy Policy.

"
https://startup.jobs/data-engineer-aristotle-4433355,Engineer,Data Engineer,Aristotle ,"Provo, United States",Full-Time,"

Aristotles Integrity division is a leading provider of identity and age verification services across numerous vertical markets. Our ageidentity verification solutions are used by companies to comply with various regulatory requirements such as AML, KYC and Age Verification.
Please visit httpsintegrity.aristotle.com for more information about this division.
JOB DESCRIPTION
 Develops data load processes for storing and retrieving data from databases and other file systems; Develops data conversion and transformation processes and utilities for large data sets. Develop solutions using .NETSSISSQL Server technologies which meet the requirements of the data consumer groups and comply with the business rules Develop web based reporting systems to monitor system performance, transaction and error rate.  Assist in defining and documenting data transformation rules. Focus on ETL design, development, and performance tuning.  Work closely with other developers to provide data services to existing and new applications modify production data, create and optimize stored procedures, functions, views, etc. Develop and analyze strategies to boost system performance. Prepare documentation and test procedures. Develops software using industry standard programming techniques. Performs unit testing and debugging of application components. 
Requirements
 BSAS in Computer Science or related field. 2 years experience with ETL process.  Proficient in T-SQLMS SQL Server 20052008 programming; understand the inner workings of the SQL Server Query Processing Engine. Knowledge and experience designing, developing, debugging and deploying SQL Server stored procedures, T-SQL, DTS and SSIS packages. Manage multiple priorities, follow a project plan, and meet project deliverables. MS SQL 2000, 2005, 2008 Microsoft SQL Server Reporting Services  MS Office Word, Excel Ability to learn and acquire new technologies as needed. 
 Desired Qualifications but not required
 Proficient in using Microsoft Visio for drawing sequence diagrams, component diagrams and other UML diagrams  Proficient in data modeling using either MS Visio or Erwin data modeler  Industry knowledge of identity verification for fraud, marketing, and risk mitigation solutions.  Internet technologies such as XML, DHTML, CSS and JavaScript ASP.NET 2.0, C  Traditional ASP 

Benefits
All positions are Full-Time, with competitive compensation, medical benefits, paid vacation, 401k plan and stock options. Casual dress code and a non-corporate atmosphere make this a fun place to work and learn in a team environment. Please visit our website at www.aristotle.com.


"
https://startup.jobs/data-engineer-human-interest-4431396,Engineer,Data Engineer,Human Interest ,,,"


Human Interest is on a mission to ensure that people in all lines of work have access to retirement benefits. 
Nearly half of all working Americans are not saving enough for their future. Too often its because they are employed by a company that doesnt offer a retirement plan. Human Interest is changing that by making it affordable and accessible for small and medium-sized businesses to offer employees a path to financial independence through retirement savings.
Were a high-growth tech company changing the retirement industry. Weve raised 500M and are backed by leading investors. Most recently, by funding from BlackRock as well as many other top investors TPG The Rise Fund with SoftBank led our Series D, Glynn Capital, NewView Capital, USVP, Wing, Uncork, and others.
Data is central to our success. It forms the core of all of our customer facing reporting, but is also at the heart of how we make decisions as a company. Were building the first full stack, automated retirement management platform in the industry  and data tells us how to optimize. 
What you get to do every day

Help build a world class data environment
Write, configure, deploy, and maintain the tools needed to deliver clean data, faster
Partner with analytics team to understand the current use cases and supply clean and consistent data models
Work with engineering teams across the company to improve data tracking
Build consistent data models that act as the source of truth for all metrics within the company
Build, deploy and maintain ELTs to pull data out to all third party systems such as salesforce or email automation systems
Partner with product and operations teams to drive efficiency by automating manual tasks

What you bring to the role

3 years of experience building and maintaining production data pipelines, data warehouses 
Experience with monitoring and validation frameworks for data infrastructure 
Strong to expert understanding of data modeling. You have taken built facts and dimensions off of different types of data sources
Strong Python and SQL skills
Experience building ELTs against various sources, including REST endpoints
Experience building and deploying services in AWS. Experience with the AWS data stack is a bonus Glue, Athena, Kinesis, etc.
Desire to learn, think creatively, and share knowledge with others
Proactive mindset - keep an eye out for anything that can be improved, and help us get there
Familiarity with DBT, airflow and Snowflake is preferred
Ability to communicate technical concepts to both technical people in adjacent fields and non-technical people

Why you will love working at Human Interest
Mission - Highly collaborative startup dedicated to supporting employee engagement and growth. Its an opportunity to help solve one of the biggest unsolved problems in America saving for retirement.
Culture - Our operating principles define how we come together as a team to do our work. They reflect Human Interests unique view on whats important and whats right. Documenting this core aspect of our culture helps employees make good decisions on their own. It also helps candidates considering career opportunities critically evaluate whether they will thrive at Human Interest. 

Customer obsession
Long-term orientation
Autonomous and accountable teams
An escalating bar for talent and performance 
Fundamental optimism 

Read more
Compensation - The salary range target for the role seniority described in this job description is 130,000 - 145,000. Final offer amounts depend on multiple factors including candidate experience and expertise, geographic location, compensationequity mix, and market data. This position may also be eligible for additional incentives such as equity awards, short-term incentives, or sales compensation.
Benefits - 

A great 401k plan Our own! Our 401k includes a dollar-for-dollar employer match up to 4 of compensation immediately vested and 0 plan fees
Top-of-the-line health plans, as well as dental and vision insurance
Generous PTO and parental leave policies
Lyra - Enhanced Mental Health Support for Employees and dependents  
Carrot - Fertility healthcare and family forming benefits
Candidly - Student loan resource to help you and your family plan, borrow, and repay student debt
Monthly work-from-home stipend; annual wellness stipend
Employee Resource Groups including Veterans, Lift Evry Voice, Pride, LatinX, Families, and Women in Tech
Fun online and regional events and celebrations and department and company offsites

The vast majority of our positions can be 100 remote


Were a great place to work but dont take our word for it 
Heres a list of our awards and accolades

Certified as a Great Place To Work 2023
Best Places to Work by Built In 2023
A Top Company by Y Combinator 2020-2023
Americas Best Startup Employers by Forbes 2020-2022 
Inc. Fastest Growing Companies 2021

 

Human Interest is an equal opportunity employer. All applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, veteran or military status, pregnancy, or any other characteristics protected under federal, state, or local laws.
Human Interest employees must adhere to the Companys security policies and Code of Ethics. 
Please note Human Interest does not accept unsolicited resumes from any source other than directly from candidates. We will not consider resumes from vendors including and without limitation search firms, staffing agencies, fee-based referral services, and recruiting agencies. 
Pursuant to the San Francisco Fair Chance Ordinance, we will consider employment-qualified applicants with arrest and conviction records. We comply with CCPA guidelines. See more httpshumaninterest.comdisclosures 

"
https://startup.jobs/data-engineer-caylent-4430586,Engineer,Data Engineer,Caylent ,,,"


 
Caylent is a cloud native services company that helps organizations bring the best out of their people and technology using Amazon Web Services AWS. We provide a full-range of AWS services including workload migrations  modernization, cloud native application development, DevOps, data engineering, security  compliance and everything in between. At Caylent, our people always come first. 
We are a fully remote global company with employees in Canada, the United States and Latin America. We celebrate the culture of each of our team members and foster a community of technological curiosity. Come talk to us to learn more about what it means to be a Caylien!

The Mission
At Caylent, a Data Engineer works as an integral part of a cross-functional delivery team to implement data management solutions on the AWS cloud for our customers.  You will work in a hands-on capacity to define RDS data sources, develop ETL jobs and manage data warehouses.  You will participate in daily standup meetings with your team and bi-weekly agile ceremonies with the customer.  Your manager will have a weekly 11 with you to help guide you in your career and make the most of your time at Caylent.


Your Assignment 

Work with a team to deliver top-quality data solutions on AWS for customers
Participate in and contribute to daily standup meetings
Develop ETL and analytics solutions around relational databases
Implement security, access controls and governance on cloud

Your Qualifications


Develop and implement at least two of these

ETL, Orchestration and CICD pipelines
SQL, Stored procedures and Query optimization
Analytics and visualization
MDM and Data Governance



Familiar or experienced with one or more of these on AWS

High Availability HA solutions, readwrite replicas and optimization
RDS and Aurora performance tuning
Large scale application migration and modernization with a heavy focus on DB
Security, access controls and governance on cloud


Excellent understanding of Data Warehouses, RDBMSs like Redshift, Snowflake, Postgres, MySQL
Experience with IaC tools such as CloudFormation, CDK, Terraform, and CICD tools
Experience with AWS Glue, Lambda, SDK
Excellent written and verbal communication skills

Benefits

Pay in USD
100 remote work
Generous holidays and flexible PTO
Competitive phantom equity
Paid for exams and certifications
Peer bonus awards
State of the art laptop and tools
Equipment  Office Stipend
Individual professional development plan
Annual stipend for Learning and Development
Work with an amazing worldwide team and in an incredible corporate culture

Caylent is a place where everyone belongs.  We celebrate diversity and are committed to creating an inclusive environment for all employees. Our approach helps us to build a winning team that represents a variety of backgrounds, perspectives, and abilities. So, regardless of how your diversity expresses itself, you can find a home here at Caylent.   
We are proud to be an equal opportunity employer. We prohibit discrimination and harassment of any kind based on race, color, religion, national origin, sex including pregnancy, sexual orientation, gender identity, gender expression, age, veteran status, genetic information, disability, or other applicable legally protected characteristics. If you would like to request an accommodation due to a disability, please contact us at hrcaylent.com.  






"
https://startup.jobs/data-engineer-ab4726-nisum-4429939,Engineer,Data Engineer - AB4726,Nisum ,"Hyderabad, India",Full-Time,"


Nisum is a leading global digital commerce firm headquartered in California, with services spanning digital strategy and transformation, insights and analytics, blockchain, business agility, and custom software development. Founded in 2000 with the customer-centric motto Building Success Together, Nisum has grown to over 1,800 professionals across the United States, Chile,Colombia, India, Pakistan and Canada. A preferred advisor to leading Fortune 500 brands, Nisum enables clients to achieve direct business growth by building the advanced technology they need to reach end customers in todays world, with immersive and seamless experiences across digital and physical channels.

What Youll Do

Excellent problem-solving skills to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
Proven experience in manipulating, processing, and extracting value from large disconnected datasets.
Supporting and working with cross-functional teams in a dynamic environment.
Advanced SQL knowledge, experience working with relational databases, query authoring SQL, and familiarity with unstructured datasets.
Perform code reviews, ensure code quality, and encourage a culture of excellence.
Communicatework effectively in a team environment

What You Know

7-10 years of experience in a Data Engineer role
Experience with Spark, Scala, Java, MongoDB
Experience with Azure Databricks
Hands-on experience in Hadoop, Spark or PySpark, JavaScalaPython, Azure Databricks, MongoDB, Apache Kafka
Streaming Kafka Streaming. 

Education
Bachelors  Masters degree in specific technical fields like computer science, math, statistics preferred, or equivalent practical experience.
Benefits

In addition to competitive salaries and benefits packages, Nisum India offers its employees some unique and fun extras

 Continuous Learning - Year-round training sessions are offered as part of skill enhancement certifications sponsored by the company on an as need basis. We support our team to excel in their field.

Parental Medical Insurance - Nisum believes our team is the heart of our business and we want to make sure to take care of the heart of theirs. We offer opt-in parental medical insurance in addition to our medical benefits.

Activities - From the Nisum Premier Leagues cricket tournaments to hosted Hack-a-thon, Nisum employees can participate in a variety of team building activities such as skits, dances performance in addition to festival celebrations.
Free Meals - Free snacks and dinner is provided on a daily basis, in addition to subsidized lunch


Nisum is an Equal Opportunity Employer and we are proud of our ongoing efforts to foster diversity and inclusion in the workplace.


"
https://startup.jobs/data-engineer-woodbine-4427631,Engineer,Data Engineer,Woodbine ,"Toronto, Canada",Full-Time,"

Our rich history is a big part of who we are.  With historic roots dating back over 135 years, Woodbine is steeped in tradition. We have been the engine that drives the Ontario horse racing industry, creating winning experiences for all our guests. 

We are also currently on an amazing 20-year journey to build a city within a city and transform the largest privately held land in Toronto into one of Canadas premiere entertainment destinations.  This journey will unlock the value of approximately 700 acres of land, leading to job creation and the revitalization of our communities, while enhancing our ability to share the thrill of horse-racing and cementing our place as an entertainment powerhouse.  

We also take care of our PEOPLE. Here are some of the things we offer

Full Group Benefits Plan
Pension
Paid Vacation
Discounts on Food
Ontario Attractions Discount
Free Parking
Opportunities for Advancement
Social Responsibility Initiatives
 
We need dedicated people to join our cause. Be part of Woodbine Entertainment and get inspired to make work awesome!

Our Values  Beliefs
 
 WOW EVERY GUEST  OWN IT!  LISTEN  CONNECT  PEOPLE MATTER  FUEL THE FUN  PASS IT ON  TRUST IN OUR TEAM  WE CARE  PURSUE YOUR POTENTIAL BE BOLD BE ALL IN  

JOB SUMMARY 

Reporting to the Director, Digital Strategy  Analytics, the Data Engineer is responsible to provide support to all lines of business under Woodbine Entertainment. Accountable for improving and maintaining an analytics ecosystem that will drive insights and support understanding of business information, enabling elevated data driven decision making within the organization. Through individual contribution, incorporates the use of data engineering methodologies leveraging internal and external data sources.
RESPONSIBILITIES

Work with stakeholders throughout the organization to identify opportunities for leveraging company data to drive business solutions.
Design the data pipelines and engineering infrastructure to support our enterprise machine learning systems at scale.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Power Query.
Assess the effectiveness and accuracy of new data sources and data gathering techniques.
Create and maintain a feature store.
Develop processes and tools to monitor and analyze model performance and data accuracy.

QUALIFICATIONS

Strong problem-solving skills.
Experience using computer languages Python, SQL, etc. to manipulate data and draw insights from large data sets.
Experience working with and creating data architectures.
Experience building and optimizing data pipelines, architectures, and data sets.
Experience working with Microsoft Azure and Microsoft Power platforms.
Good written and verbal communication skills for coordinating across teams.
A drive to learn and master new technologies and techniques.
1-3 years of experience manipulating, processing, and extracting value from disconnected datasets.


Guided by our Values and Beliefs, Woodbine Entertainment commits to providing an authentic and empowering work experience! Please reach out if youre ready to embrace change and be part of a new breed of experience while working for one of Canadas certified Great Places to Work!

"
https://startup.jobs/data-engineer-ts-sci-clearance-riverside-research-4427123,Engineer,Data Engineer (TS/SCI clearance),Riverside Research ,"Boston, United States",,"


Riverside Research is an independent National Security Nonprofit dedicated to research and development in the national interest. We provide high-end technical services, research and development, and prototype solutions to some of the countrys most challenging technical problems.
 

Job Number 1246
Riverside Research is seeking full-time Data Engineer to support Director, Air Force Chief Data Office SAFCO sponsored activities across the Air Force Enterprise to ensure the visibility, accessibility, understanding, sharing, and trustworthiness of data across air, space, and cyberspace domains. Candidates will provide subject matter expertise in and perform on multidisciplinary teams that support data preparation and architecture, development of agile algorithmic solutions, evaluate andor execute data governance and data maturity models; and conduct data analytics using state of the art mathematical and machine learningartificial intelligence techniques and other data analytic lines of researcheffort.  
This position can be based out of Boston MA area.
 
All Riverside Research opportunities require U.S. Citizenship
Job Duties

Provide expertise on all data concepts for the broader advanced analytics group, and inspire the adoption of advanced analytics, data engineering and data science across the organization. 
This will include Installing continuous pipelines of large pools of filtered information so that data analystscientists can pull relevant data sets for their analyses. 

Required Qualifications

Top Secret clearance with SCI adjudication
Bachelors degree in the requisite relevant field. A Masters degree in a relevant field may be substituted for 3 years of general experience. 
 7 years or more of experience in the data engineering field, at least three of which must have been in a data analytics environment preferably in DoD or the intelligence community. 
Familiarity with the manipulation of unstructured data in a data analytics environment, and the use of open-source tools, cloud computing, machine learning and data visualization.  
Familiar with specialized languages relevant to the technologies employed such as Apache, Hadoop, etc.  

 

 
Riverside Research strives to be one of Americas premier providers of independent, trusted technical and scientific expertise. We continue to add experienced and technically astute staff who are highly motivated to help our DoD and Intelligence Community IC customers deliver world class programs. As a not-for-profit, technology-oriented defense company, we believe service to customers and support of our staff is our mission. Our goal is to serve as a destination company by providing an industry-leading, positive, and rewarding employee experience for all who join us. We aspire to be a valued partner to our customers and to earn their trust through our unwavering commitment to achieve timely, innovative, cost-effective and mission-focused solutions.
All positions at Riverside Research are subject to background investigations. Employment is contingent upon successful completion of a background investigation including criminal history and identity check.
Riverside Research does not mandate COVID vaccination as a condition of employment. However, proof of vaccination or negative test may be required to enter certain government facilities and sites. Vaccination requirements will depend on the status of the federal contractor mandate and customer site-specific requirements. To protect the health and safety of its employees, their families, and to comply with customer requirements, the company requires all employees to disclose vaccination status upon hire. 
Our EEO PolicyRiverside Research is an equal opportunity employer. We recruit, employ, train, compensate and promote without regard to race, religion, sex, color, national origin, age, gender identity, sexual orientation, marital status, disabilityveteran, status as a protected veteran, or any other basis protected by applicable federal, state and local law.
If you need assistance at any time in our application or interview process, please contact Recruiting at email RecruitingRiversideResearch.org. A member of the Recruiting team will be available to assist.
This contractor and subcontractor shall abide by the requirements of 41 CFR 60-741.5a. This regulation prohibits discrimination against qualified individuals on the basis of disability and requires affirmative action by covered prime contractors and subcontractors to employ and advance in employment qualified individuals with disabilities.
This contractor and subcontractor shall abide by the requirements of 41 CFR 60-300.5a. This regulation prohibits discrimination against qualified protected veterans and requires affirmative action by covered contractors and subcontractors to employ and advance in employment qualified protected veterans.
For more information on ""EEO is the Law,"" please visithttpwww.dol.govofccpregscomplianceposterspdfeeopost.pdf
httpswww.dol.govsitesdolgovfilesofccpregscomplianceposterspdfeeopost.pdf


"
https://startup.jobs/data-engineer-for-an-industry-leading-management-software-for-private-practices-beon-tech-studio-4426989,"Engineer,Developer",Data Engineer for an Industry-Leading Management Software for Private Practices,BEON Tech Studio ,"Los Angeles, United States",,"

Join a team who has been constantly growing for the past 2 years, reaching a 95 growth in headcount and an award-winning company recognized among others by Forbes, Great Place to Work, and Built-In thanks to the product and culture.
This company provides a comprehensive platform, customizable, secure, and easy to use for health and wellness professionals to manage their businesses in private practice. It offers features like AutoPay, online booking, and Telehealth to help professionals save time and provide the best care to their clients.
The platform is empowering professionals to be successful entrepreneurs and effective service providers.
Key Details to Catch Your Eye
 The engineering team is conformed of 10-15 teams with 5-10 members each. A Kanban methodology process. Have a rigorous onboarding process that lasts around 2 weeks. They provide a biweekly coding training program. Excellent structure with documentation of their systems, apps, courses, and learning sessions. Has consistently grown the team for the past year adding 8 new team members. 
What BEONers Love about this Project
BEONers from Brazil, Chile, Costa Rica, and Mexico are already part of this team, and heres what they would like to share with you
I really love the warmth and welcoming work environment. People are so helpful and focused on delivering the best as a group. Besides that, our code quality standards are high. This is very motivating to keep on always learning something new. Nicolas from Brazil.
I love how we work here. Collaboration is amazing, and challenges are a good way to improve as an engineer. Something that surprises me is how all the parts work together, minimizing downtimes and doing the work with huge traffic spikes and workloads. Matias from Chile.
Requirements
Your Skills
 4 years as a Data Engineer Semi SrSr. Deep experience in at least two database engines, preferably MySQL, Snowflake, or Athena.  Knowledge of Metadata-driven and database-centric concepts.  Proficiency in ETL and ETL tools, including Airflow and DBT.  Familiarity with data ingestion tools, such as Kafka and Singer.  Experience with at least one visualization tool, preferably Looker, Tableau, or Periscope.  At least one programming language knowledge, preferably Python.  Experience with APIs. Hands-on experience with cloud-based infrastructure, particularly AWS.  Bachelors degree or equivalent. English proficiency.  
How Youll Stand Out
 Experience with big data tools and engines like Glue, DMS, andor Redshift.  Knowledge of architecture and enterprise data architecture.   The Screening Process
 Verification by the BEON team. 30-minute cultural interview with the Hiring Manager.  1 and a half-hour technical with the Data team members.  30-minute final interview. 

Total expected timeframe  7-10 days.
About BEON.tech
BEON.tech connects the brightest Latin American talent with the most innovative and disruptive U.S. companies. Youll get access to a custom-vetted pool of full-time, long-term, remote software jobs with compensation comparable to U.S.-based positions.
To join BEON.tech is to be in a devs-first company, which means you are the priority when it comes to decision-making, client selection, and growth planning.
Develop your career at the pace you deserve.
Benefits
 USD compensation comparable to U.S.-based positions. A US 1,500 welcome package to get you started with the right gear. Health insurance. Internet service. Trip to Headquarters in Buenos Aires. Flexible payments in crypto, wire transfer, Wise, PayPal, or Payoneer. English conversation club  workshops. Rewards Program Win prizes every month by participating in weekly challenges. The annual winner will earn a trip for two to NYC! Psychotherapy sessions. Unlimited reskilling in Udemy, Educacin IT  OReilly. 

"
https://startup.jobs/data-engineer-bilt-rewards-4426151,Engineer,Data Engineer,Bilt Rewards ,"New York, United States",,"

Role Data Engineer
Reporting to Head of Engineering
Location New York, NY
 
What is Bilt?
Bilt Rewards is the first-ever rewards program that allows renters to earn points on rent and builds a path towards homeownership. With an alliance of the nations largest real estate owners, Bilt Rewards will enable renters in more than two million units across the country to earn points just by paying rent.
Bilt Rewards boasts one of the highest value rewards programs on the market today, including one-to-one point transfers for travel across over 100 major airlines and hotel partners; fitness classes at the countrys top boutique studios; limited-edition and exclusive collections of art and home decor through the Bilt Collection, and the ability to use points for rent credits or towards a future downpayment.
Bilt has also partnered with Mastercard to create the Bilt Mastercard - the first and only credit card that can be used to pay rent with no fees. In October 2022, Bilt announced a valuation of 1.5 billion following a 150 million growth round to advance our loyalty program and credit card for renters.
Whats the role?
We are looking for an experienced data professional to join a renowned team that is transforming the future of renting. As a Data Engineer you will

Design and manage data pipelines from several sources into a centralized data warehouse
Oversee all aspects of the data lifecycle including quality checks, ingestion and data modeling
Catalog and maintain data definitions and metadata for all assets managed in the warehouse


Assist in governance of data to stakeholders throughout the company

In this role you will

Develop data pipelines using tools like BigQuery, DBT, AirFlow, Spark, Beam and more
Own all aspects of data QA, ensuring accurate and fresh data is available to all stakeholders
Orchestrate and manage all aspects of the data warehouse and its assets
Collaborate with engineers, product managers, and data scientists to understand data needs, and deliver meaningful insights

Additional responsibilities may include 

Support BI and analytics use cases requiring advanced tools or a deep knowledge of our data
Input on the organization, usage, and sharing of data both internally and externally
Ad Hoc data requests for business team members

In terms of qualifications, were seeking 

3 years of experience as a Data Engineer andor Data Scientist

Applied ML experience is a plus

Deep knowledge of ETL best practices and frameworks, data warehouse management, and data governance principles

Experience with Spark andor Dataflow is a plus

Experience with Google Cloud Platform GCP or comparable cloud technologies

BigQuery
Dataflow
Google Cloud Storage
Kubernetes  Compute Engines

Experience designing and implementing sophisticated data models to support BI and advanced analytics

Experience with Looker is a plus

Expert SQL and Python proficiency

Experience with a JVM language is a plus

Strong communication skills, attention to detail, and ability to manage deadlines

Annual base salary range is 110,000 - 200,000  bonus  benefits medical, dental, vision  401K match  Equity. Ultimately, when building each comp package we consider job skills, experience and location. 
 

"
https://startup.jobs/data-engineer-iii-2u-alumni-job-board-4425951,Engineer,Data Engineer III,2U Alumni Job Board ,"Lanham, United States",,"

At 2U, we are all in on purpose. We are motivated by our mission  to eliminate the back row in education  and connected by our shared passion to deliver world-class digital education at scale. As the parent company of edX, the worlds leading online learning platform, 2U powers more than 4,000 online higher education offerings  from free courses to full degrees. Together with more than 230 colleges, universities, and corporate partners, we are helping to unlock human potential.
What Were Looking For
The Data Platform organization is reinventing the way 2U and edX stakeholders interact with our internal data estate. We are leveraging best-of-breed technology solutions to democratize data and enable self-service analytics for power users across the company including departments such as Data Science, Marketing, and Finance.
As a key individual contributor, you will work with a cross functional team of data engineers, analytics engineers and product managers responsible to deliver optimized data pipelines and create reliable data products. You should be able to architect systems interactions, be hands-on, and be able to dive deep into any part of the stack, all while maturing our code base and development processes. 
You will drive the design and development of the systems, processes, and data applications necessary to support scaling up of the platform to hundreds of data hungry employees and external business partnersuniversities. Youll also have the opportunity to collaborate with external teams and South African colleagues in an inclusive environment to onboard new data sources into the data lake.
Responsibilities Include, But Are Not Limited To

Initiate and foster relationships with other groups and stakeholders to distill data needs
Organize and coordinate work efficiently to meet deadlines and milestones
Writing maintainable and performant code in Python, SQL, and dbt
Maintaining data pipelines  data integration components
Troubleshoot data problems and perform root cause analysis to resolve operational issues
Implementing technical work according to productdesign specifications
Experience performing unit tests and troubleshooting of complex issues
Analyze, clean, prepare and optimize data for ingestion and consumption
Located or willing to work in North America Eastern time zone

Things That Should Be In Your Background

5 years of experience in software developmentbackend engineering
Solid understanding of relational databases and SQL Snowflake and Redshift
Understanding or knowledge of Linux commands bash scripting
Experience with infrastructure automation Terraform
Familiarity working with containerized services Docker and Kubernetes
Experience with Agile software development
Knowledge of storage, networking, and firewall concepts in AWS
Understanding of modern ETLELT tools such as Airflow and dbt
Self directed with a bias towards action and a consistent record of getting results

Benefits  Culture
Our global employee base is a diverse collection of innovators, dreamers, and doers working together to transform lives through higher education. We believe that every employee can advance our shared purpose, and that life at 2U should be fun and meaningful. If youre excited by the opportunity to provide over 40 million learners and counting with access to world-class online higher education, then join us  and do work that makes a difference. NoBackRow
We offer comprehensive benefits unique per country and excellent worklife balance.Full-time, U.S.benefits include 

Medical, dental, and vision coverage
Life insurance, disability, and 401k employer match
Employee stock purchase plan
Free snacks and drinks in-office
Generous paid holidays and leave policies, including unlimited PTO
Additional time off benefits include volunteer days, parental leave, and a company-wide winter break

The anticipated base salary range for this role is 120,000-142,000, with potential bonus eligibility.  Salary offers are made based on the candidates qualifications, experience, skills, and education as they directly relate to the requirements of the position, budget for the position and cost of labor in the market in which the candidate will be hired.
2U Diversity and Inclusion Statement
At 2U, we are committed to building and sustaining a culture of belonging, respect, and inclusion. We are proud of the steps weve taken to bring together an employee base that embodies diverse walks of life, ideas, genders, ages, races, cultures, sexual orientations, abilities and other unique qualities. We strive to offer a workplace where every employee feels empowered by what makes us different, as well as by how we are alike. 
2U is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodations, please reach out to us at recruitingaccommodations2u.com. 
About 2U Inc. NASDAQ TWOU
For more than a decade, 2U, Inc. has been the digital transformation partner of choice to great non-profit colleges and universities delivering high-quality online education at scale. As the parent company of edX, a leading global online learning platform, 2U provides over 45 million learners with access to world-class education in partnership with more than 230 colleges, universities, and corporations. Our people and technology are powering more than 4,000 digital education offerings  from free courses to full degrees  and helping unlock human potential. To learn more visit 2U.com.About edX
edX is the education movement for restless learners and a leading global online learning platform from 2U, Inc. Nasdaq TWOU. Together with the majority of the worlds top-ranked universities and industry-leading companies, we bring our community of over 45 million learners world-class education to support them at every stage of their lives and careers, from free courses to full degrees. And were not stopping there  were relentlessly pursuing our vision of a world where every learner can access education to unlock their potential, without the barriers of cost or location. Learn more at edX.org.
Learn more at https2u.comcareers NoBackRow
The above statements are intended to describe the general nature and level of work performed by individuals assigned to this position, and are not intended to be construed as an exhaustive list of all responsibilities, duties and skills required. All employees may be required to perform duties outside of their normal responsibilities from time to time, as needed.
2U is an equal opportunity employer that does not discriminate against applicants or employees and ensures equal employment opportunity for all persons regardless of their race, creed, color, religion, sex, sexual orientation, gender identity, pregnancy, national origin, age, marital status, disability, citizenship, military or veterans status, or any other classifications protected by applicable federal, state or local laws. 2Us equal opportunity policy applies to all terms and conditions of employment, including but not limited to recruiting, hiring, training, promotion, job benefits and pay. 

"
https://startup.jobs/data-engineer-gb4790-nisum-4424485,Engineer,Data Engineer GB4790,Nisum ,,Full-Time,"


Nisum is a leading global digital commerce firm headquartered in California, with services spanning digital strategy and transformation, insights and analytics, blockchain, business agility, and custom software development. Founded in 2000 with the customer-centric motto Building Success Together, Nisum has grown to over 1,800 professionals across the United States, Chile,Colombia, India, Pakistan and Canada. A preferred advisor to leading Fortune 500 brands, Nisum enables clients to achieve direct business growth by building the advanced technology they need to reach end customers in todays world, with immersive and seamless experiences across digital and physical channels.

What Youll Do

Defines, designs, develops, and test software componentsapplications using Spark, Sql, and PySpark.
Building solutions using a variety of open-source tools Microsoft Azure services and a proven track record in delivering high-quality work to tight deadlines.
Ability to work with the customer as part of the Agile model of delivery.
Design and Build Modern Data Pipelines and maintain data warehouse schematics, layouts, architectures, and relationalnon-relational databases for data access and Advanced Analytics

What You Know

Hands-on distributed computing development experience using PySpark, Spark SQL, and Databricks delta tables.
Should have experience in structured streaming, stateless, and state full.
Familiarity with Azure resources AKV, Managed identity, SPN, ADLS,  etc...
Must have good craftsmanship skills including unit testing, and code quality, and be a creative thinkerproblem solver.
Experience in CICD and operation tooling integrations is a plus.
Li-Remote

Education
Bachelors degree in Computer Science, Information Systems, Engineering, Computer Applications, or related field
Benefits

In addition to competitive salaries and benefits packages, Nisum US offers its employees some unique and fun extras


Professional Development - We offer in-house technical training and professional learning programs aimed at developing skills across a broad spectrum of topics such as technology, leadership, role-based training, and process expertise. We also offer an annual stipend for employees to attend external courses in order to maintain professional certifications

Health  Wellness Benefits - We believe that your health and welfare are important, and we strive to ensure that you have affordable options available to you, including some plans that are subsidized for employees and their families by up to 90. We also have dental and vision plans in the US where Nisum pays 100 of premiums for employees

Volunteerism Pay - We believe in giving back and in the US, our employees are eligible for up to 40 hours of paid time off each year to volunteer towards the causes that they are most passionate about. This is in addition to personal PTO and paid holidays

Additional Benefits - We offer all the other important benefits to keep employees and their families healthy and financially secure, such as 401k retirement savings with a company match, pre-tax parking and transit programs, disability insurance, and Basic LifeADD, alongside exclusive employee discounts on a wide variety of products and services

Compensation Band
120-125k per year

Nisum is an Equal Opportunity Employer and we are proud of our ongoing efforts to foster diversity and inclusion in the workplace.

"
https://startup.jobs/data-engineer-trustly-4423604,Engineer,Data Engineer,Trustly ,"Stockholm, Sweden",Full-Time,"

At Trustly, were passionate about simplifying the way people transfer, pay, and get refunded online. Our payment solutions serve merchants in various industries. This includes E-com, Financial Services, Gaming and Travel  linking the worlds top brands with millions of consumers who expect security and convenience. 
Our people are from all corners of the globe, and this were proud of. We strongly believe that diversity is what helps us create solutions that are more inclusive. Making sure that were constantly innovating. Our fast-growing team is headquartered in Stockholm, Sweden with additional offices across Europe and the Americas. Together, we are modernizing payments and the work youll do here will make a lasting impact. 

We are a diverse and fast-growing team with our headquarters in Stockholm, Sweden, and 9 additional offices across Europe and North America. Together we are leading the development of the payments industry and the work youll do here will make a great impact.

About The Role

Trustly is developing fast and so are the requirements on data being made available for analytics, data science, reporting and BI. The Data Engineering teams part in this is to provide a platform and tools to enable data producers and consumers to share data in a frictionless way. Trustlys European data platform is built on GCP using components such as Airflow Cloud Composer, Beam Dataflow, BigQuery, PubSub and Google Cloud Storage. We also maintain the companys dbt Cloud installation to enable Analytics Engineers and other downstream SQL power users to build their own data pipelines and not become dependent on a single data team that does everything. Parts of Trustlys payment product run on AWS so knowledge about that ecosystem is important to us as well.
What youll do

Build and operate Trustlys data lake on Google Cloud StorageBigQuery
Develop and maintain batch and streaming data pipelines
Work with transformations of data from one or multiple source systems to feed our analytics and ML applications
Help data consumers within the company find the data they need and assist them with setting up pipelines and exports
Help us identify new use-cases for our data and work with stakeholders from the rest of the organization on how to realize those

Who you are

MSc Software Engineering or equivalent
At least 3-5 years experience from a Data Engineering role
Good coding skills with Python
Proficient in SQL
Experience from building streaming data pipelines Prio1 Kafka
Experience from building cloud data solutions Prio1 GCP, Prio2 AWS
Experience with Terraform
Experience with DBT


We are looking for someone who is not afraid of voicing and acting on new ideas and values good communication with internal andor external stakeholders. If you are passionate about working with different areas across the organization, then this would be an interesting role for you.

Apply now, we would love to talk to you!

"
https://startup.jobs/data-engineer-werken-bij-bridgefund-4423436,Engineer,Data Engineer,Werken bij BridgeFund ,"Amsterdam, Netherlands",,"

Do you get extremely excited about utilizing state-of-the-art technologies such as Databricks, Python, Spark, and SQL? Does unlocking the potential of data to transform how businesses access financial support make your heart jump? Are you that talented and passionate Data Engineer who we are looking for? Your role will be pivotal in leveraging the power of data to drive innovation and growth within the B2B SME Lending space. Still excited? Read on, apply and join our team!

Our chemistry speaks volumes
From 2018, we got off to a flying start and are currently in full scale-up mode. We are constantly working on growth and optimization. This gives you plenty of opportunities to use your creativity and build a new, optimistic and slightly rebellious generation with only one mission to prove that a bank-free world makes doing business easier, more efficient, fairer and therefore more fun.

One of the key things that allows us to grow so fast has everything to do with our culture. Youre probably in the office more than in your own home, so we might as well make sure you feel at home. That is why we are always looking for talent that believes in the same values as we do.


At BridgeFund we speak easy. We are transparent in all directions and trust deeply in our team. Its important to us that youre a straight-talker and a good listener. You never settle and like to fight smart. You aim high, but you aim real. You have a desire to always improve yourself and the business by thinking in possibilities. We are an energetic fun-loving bunch, with lofty goals and a long path ahead. Why not have some fun along the way? Because at BridgeFund work  play.

"
https://startup.jobs/data-engineer-east-coast-us-hours-3cloud-4423342,Engineer,Data Engineer (East Coast US Hours),3Cloud ,"Makati, Philippines",,"


Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you!
At 3Cloud, we hire people who arent afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Clouds core values because they know that it will result in amazing experiences and solutions for our clients and each other.

Job Highlights


Our technology focus is on Azure  youll have plenty of exciting opportunities to grow your skills in Microsoft and Azure in an environment committed to technical excellence and client experience in a very specific, defined space. Six months of experience on our team is worth years somewhere else.

Microsoft Partnerships  Our great global relationship with Microsoft ensures that we have a pipeline of cutting-edge Azure work opportunities, and access to the teams that have built the platform. You wont find this at other places.

Competitive compensation package, salary, allowance, standard benefits including quarterly and annual performance-based cash bonus and other remuneration.

Great working environment and company culture with flexible work location.


 
General Required Technical Skills

Expertise in designing and implementing logical and physical data models for cloud and hybrid data warehouse environments  
Implementing  data architectures to support a variety of data formats and structures including structured, semi-structured and unstructured data  
Experience with multiple full life-cycle data warehouse implementations  
Understanding of data architectures required to support data integration processing  
Experience with data modeling technologies such as ERStudio, ERWin or similar  
Experience with Microsoft Azure Data Platform services including Azure Data Lake Store, Azure Storage, Azure Synapse, Azure Data Factory, Azure SQL database, Logic Apps, APIs  
Demonstrated ability to quickly learn, adopt and apply new technologies  
Data profiling and creation of source to target mappings  
Ability to provision and configure Azure data service resources 

 
Detailed Required Skills

Python  SQL Scripting
SQL, PySpark
General Cloud Architecture competency skillset  - Capable of taking requirements and is build out data pipelines
API Knowledge is required, but preference given to candidates who can create APIs

 
Preferred ExperienceSkillsCertifications

Microsoft Azure Cosmos DB, Data Flows, Express Route, Azure Active Directory  
Experience creating strategies to migrate customers from on-premise environments to Azure   
Power BI and semantic modeling  
AWS Glue  Azure Data Factory
AWS S3  Azure Blob
AWS Athena  Azure Databricks
AWS Redshift  Azure Synapse
AWS ECS  Azure AKS


Dont meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if youre excited about this role but your past experience doesnt align perfectly with every qualification in the job description, we encourage you to apply anyway.
At this time, we cannot sponsor applicants for work visas.


"
https://startup.jobs/data-engineer-senior-digital-on-us-4419339,"Engineer,Senior",Data Engineer (Senior),Digital On Us ,"Monterrey, Mexico",,"

DATA ENGINEER SR. Python
 
We are looking to hire a data analyst to join our data team. Who will take responsibility for managing master data set, developing reports, and troubleshooting data issues. To do well in this role is necessary a very fine eye for detail, experience as a data analyst, and a deep understanding of the popular data analysis tools and databases.
 
Responsibilities
 
 You will be using Data wrangling techniques converting one ""raw"" form into another including data visualization, data aggregation, training a statistical model etc.
 Work with various relational and non-relational data sources with the target being Azure based SQL Data Warehouse  Cosmos DB repositories
 Clean, unify and organize messy and complex data sets for easy access and analysis
 Create different levels of abstractions of data depending on analytics needs
 Hands on data preparation activities using the Azure technology stack especially Azure Databricks is strongly preferred
 Implement discovery solutions for high speed data ingestion
 Work closely with the Data Science team to perform complex analytics and data preparation tasks
 Work with the Sr. Data Engineers on the team to develop APIs
 Sourcing data from multiple applications, profiling, cleansing and conforming to create master data sets for analytics use
 Experience with Complex Data Parsing Big Data Parser and Natural Language Processing NLP Transforms on Azure a plus
 Design solutions for managing highly complex business rules within the Azure ecosystem
 
Minimum Requirements
 
 5-7 years of solid experience in Big Data technologies a must.
 Mid to advanced level knowledge of Python and Pyspark is an absolute must.
 Knowledge of Azure, Hadooop 2.0 ecosystems, HDFS, MapReduce, Hive, Pig, sqoop, Mahout, Spark etc. a must.
 Significant programming experience with above technologies as well as Java, R and Python on Linux a must.
 Experience with Web Scraping frameworks Scrapy or Beautiful Soup or similar
 Extensive experience working with Data APIs Working with RESTful endpoints andor SOAP
 Excellent working knowledge of relational databases, MySQL, Oracle etc.
 Experience with Complex Data Parsing Big Data Parser a must. Should have worked on XML, JSON and other custom Complex Data Parsing formats
 Knowledge of High-Speed Data Ingestion, Real-Time Data Collection and Streaming is a plus
 Bachelors in computer science or related educational background
 
 
We Offer a competitive benefits package for you

Permanent contract for an indefinite period since the first day
Competitive salary
100 Payroll scheme  all legal benefits
Hybrid WFH scheme assist only 12 days per month to the office!
Grocery Vouchers 10
Savings Fund 13
SGMM and SGMm
22 days of Xmas Bonus
50 Vacacional prime
15 Vacations days
5 Personal days
Life insurance
Annual Performance Bonus
Payroll are the 10th and the 25th
Pension Plan
Course and Certifications among other benefits
And more

If you apply for this opportunity we will get you resume and its contain personal data whose treatment has been authorized by its owner for Digital OnUs, S. de RL de CV the ""Company. If you are not the owner of this information or have no relation whatsoever with the subjects treated in it, you are requested in the most attentive way not to make copies of it and  or its attached files and delete it immediately, under the risk of being considered as responsible for the unauthorized treatment of personal data in accordance with the Federal Law on Protection of Personal Data Held by Private Parties, its Regulations, and other applicable regulations. If you are the owner of personal data in possession of the Company and wish to obtain further information regarding the processing of your personal data or the exercise of your ARCO rights, please consult our integral privacy notice on the website httpswww.digitalonus.comprivacy-policy

"
https://startup.jobs/data-engineer-finance-figma-2-4417493,"Engineer,Finance",Data Engineer - Finance,Figma ,"New York, United States",,"

Figma is growing our team of passionate people on a mission to make design accessible to all. Born on the Web, Figma helps entire product teams brainstorm, create, test, and ship better designs, together. From great products to long-lasting companies, we believe that nothing great is made alonecome make with us!

We are looking for an experienced Data Engineer to partner with our Data Science and Data Infrastructure teams to own and scale our data pipelines. Youll also work closely with stakeholders across business teams including sales, marketing, and finance to ensure that the data they need arrives promptly and reliably. Youll play an integral role in building the metrics and self-serve reporting capabilities to unlock Figmas next phase of growth.

 

This is a great role for an individual who is passionate about working with data and data systems, and who loves solving problems. Youll have a good sense for when it makes sense to build fast, scrappy solutions to unblock a key stakeholder vs. when to push back or bring in an outside service. The ideal candidate will be a great communicator who can help coordinate across multiple internal and external teams and takes pride in building end-to-end projects.

 
What Youll Be Doing


Own, build, and maintain scalable data pipelines that connect various cloud data sources.


Develop a deep understanding of Figmas core data models and optimize data pipelines for scale.


Partner with the Data Science and Data Infrastructure teams to build new foundational data sets that are trusted, well understood, and enable self-service.

Work with a wide range of cross-functional stakeholders to build solutions to address their data needs.
Establish best practices for the development of specialized data sets for analytics and modeling. 

Were looking for someone with

3 years in a relevant field
Fluency with both SQL and Python
Familiarity with Snowflake, dbt, and ETLreverse ETL tools.
Understanding of CRM systems such as Salesforce is a big plus.
Excellent judgment and creative problem solving skills
A self-starting mindset along with strong communication and collaboration skills

Read more about our team

By the numbers one year of remote work
Measuring the value of design systems
Figmas Series D  The Decade of Design
Figmas engineering values


We will work to ensure individuals with disabilities are provided reasonable accommodation to participate in the interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please let your recruiter know if you require accommodation.
About Figma
Dylan Field and Evan Wallace co-founded Figma in 2012 with the vision of building tools for designers in the browser. Their goal was to create the first design tool that combined the accessibility of the Web with the functionality of a native app. Today Figma is a platform with tools and spaces to support the entire product development processidea to design to buildand has simplified collaboration for companies like Microsoft, Uber, and The New York Times to name a few.
At Figma, we celebrate and support our differences. We know employing a team rich in diverse backgrounds, experiences, and opinions allows our employees, products, and community to flourish. Figma is an equal opportunity workplacewe are dedicated to equal employment opportunities regardless of race, color, ancestry, religion, sex, national orientation, sexual orientation, age, citizenship, marital status, disability, gender identity, or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements.
By applying for this job, the candidate acknowledges and agrees that any personal data contained in their application or supporting materials will be processed in accordance with the applicable candidate section of Figmas Privacy Policy. 


"
https://startup.jobs/data-engineer-itech-media-4416584,Engineer,Data Engineer,iTech Media ,"Warsaw, Poland",Full-Time,"

iTech is a global digital marketing services company with a focus on digital media, online communities and comparison products. We create and develop flagship brands and optimise platforms to engage, inform and entertain global audiences across the most competitive comparison markets in the world iGaming, Sports Betting and Personal Finance. Weve brought to life over 150 products across 50 countries and over 20 languages, and the opportunities at iTech are unrivalled youll work with and learn from industry experts, deliver your best work and make a real impact.

Were a bootstrapped company, with 280 awesome Arcs our collective noun today around the world. Our purpose is to Help People Make Smart Choices Online. We achieve our goals by building products that help people have fun, avoid the bad choices, and make the right play.


Why we need you 

You will join our Data Engineer team that provides data and the tooling to make sense of it and support a data-driven approach across the organization.. You will become a member of a team with a load of growth potential. It is a great chance to learn technologies like Airlfow, Astronomer, AWS, Terraform and Kafka to name a few.
A team thats always looking for new technologies to implement that will benefit the company. 
What youll be doing

Design, implement and maintain custom ETLELT pipelines.
Manage the workflow of data from ingestion to application.
Create and maintain documentation about data tools, models and schemas.
Create best practices for data extraction, transformation and loading.
Design schemas and perform dimensional data modeling.
Verify data and spot gaps or inconsistencies to help build better pipelines.
Write automated tests for data pipelines.
Work with data transformations processes ETL, ELT.
Working with Snowflake data cloud.
Build on-prem-to-cloud data pipelines.

Youll have

Fluent skills in Python or similar language. 
Strong knowledge of SQL and NoSQL with relational and non-relational databases MySQL, Postgresql, MongoDB, Cassandra or similar.
Knowledge of ETL pipelines tools such as Airflow or similar.
Experience with data warehousing technologies such as Snowflake desirable.



Here at iTech Media were real believers in an interview being a two-way process, this is as much about you finding out everything you need to know about us as well as getting to know you. To help you understand what the process will involve  


-Initial Conversation Talent Team  approx. 30 mins  

- 1st Stage Interview with Engineering Manager and another member of the Chapter  60 mins   

- 2nd Stage Interview Cease study discussion with Data Engineers of the team  60 mins 

All the interview stages are held over Zoom. If you have a disability, a learning difficulty as dyslexia, a medical condition or other individual need for an adjustment to our process; and you believe this may affect your performance in selection, well be happy to make reasonable adjustments to our processes for you. Please let us know in good time, so that we can support you to perform at your best. 

What we can do for you

       Flexible working hours  work your way
       26 days holiday, regardless of tenure, plus we close over end of year holiday period. You also get two days for your birthday and a life event, and two paid volunteering days a year.
       Private healthcare including dental, wellbeing and talking therapy
       Annual training budget, plus regular training days to support your growth and development
       Eligibility for our discretionary bonus scheme
       Company-wide share option scheme
       Pension contribution matched up to 4
       Life insurance cover up to 300,000PLN

You can see more information about all of our awesome benefits here.

Diversity, equity and inclusion at iTech Media 
iTech celebrates diversity in our teams, and we take inclusion seriously here. Were creating an unrivalled opportunity for our teams to grow and belong, and as we scale, we know we need the most talented people with diverse backgrounds, perspectives and skills. If youre good at what you do, come and join us. The more inclusive we are, the more epic experiences we can create for our communities.

Please take a moment to read iTech Medias privacy policy


"
https://startup.jobs/data-engineer-zerofox-4416384,Engineer,Data Engineer,ZeroFOX ,"Santiago, Chile",Full-Time,"

BIG NEWS!
 ZeroFox is officially a public company, and were on a mission to make the internet safer. Read more here httpow.lyIy6K50KbCYo


OPPORTUNITY OVERVIEW
At ZeroFox, we are passionate about bringing world-class protection to the digital-first world. Together, we have built a system capable of collecting and analyzing data from all over the internet, creating a paradigm-shifting cybersecurity product that helps secure and protect our customers and improves the global threat landscape.


We are a team of software engineers, cybersecurity experts, and data scientists who continue to develop unparalleled solutions for the cyber security industry by providing the best Digital Risk Protection SaaS in the global market. Our company, services, and user base are growing faster than ever and wed like you to help us innovate. We believe the greatest solutions in the Digital Risk Protection space come from composing teams with varied backgrounds and experiences that can use their complementary skill sets to come up with the best market-defining solutions.

Along the way, youll expand your potential with constant opportunities to learn and grow alongside exceptionally knowledgeable, competent, and collaborative people.

If you want to be part of a group of talented, empathic individuals with a multitude of different backgrounds, who are committed to each other and to the overall success of the team and our customers, we want to hear from you! At ZeroFox, soft skills are just as important as technical ability!
Role and responsibilities

Collaborate with industry experts across security, machine learning, digital risk protection, and cyber threat intelligence to develop cutting-edge analytics solutions for the ZeroFox platform.
Design, develop, and maintain scalable and reliable data pipelines, ensuring efficient data ingestion, transformation, and storage.
Work closely with data analysts and machine learning engineers to ensure high-quality data is easily accessible for model development and analysis.
Implement and maintain data storage and access solutions, championing best practices and ensuring optimal performance.
Leverage modern data technologies and frameworks, such as BigQuery, Snowflake, Spark, Airflow, and Elastic search to improve data processing capabilities and efficiency.
Ensure data quality and integrity through the implementation of proper data validation and monitoring techniques.
Collaborate with your team to prioritize projects based on business impact, aligning with product roadmaps and OKRs.
Stay up-to-date on industry trends and emerging technologies in data engineering, incorporating new techniques and tools as appropriate.
Effectively communicate complex data engineering problems and solutions to both technical and non-technical stakeholders.

Required qualifications and skills

Bachelors degree in computer science, engineering, mathematics, or a related field.
Experience in data engineering or related backend engineering, typically obtained in 3 years.
Strong understanding of fundamental principles in data engineering, including data pipeline development, data storage, data management, and data access.
Solid fundamental skills in software engineering.
Strong understanding of fundamental principles in relational, non-relational, SQL and NoSQL databases.
Proficient with a variety of data querying languages, especially SQL.
Experience with data technologies and frameworks, such as BigQuery, Snowflake, Spark, Airflow, Elasticsearch, dbt, Databricks, or similar tools.
Excellent communication and presentation skills, with the ability to articulate complex ideas to both technical and non-technical audiences.
Strong analytical, problem-solving, and decision-making skills, coupled with the ability to prioritize tasks and manage resources effectively.

Desired qualifications and skills

Prior experience working with data analytics or machine learning teams.
Understanding of machine learning concepts and the role of data engineering in supporting the development of ML-based solutions.
Demonstrated success in implementing data engineering solutions that have resulted in measurable business impact.
Experience working in agile development environments and familiarity with project management methodologies, such as Scrum or Kanban.

Benefits

Full flexibility to work from home or office
Opportunities to learn and contribute your knowledge to our Digital Risk Protection platform
Competitive compensation 
Flexible work hours and competitive PTO
Complementary health insurance
Daily catered lunches for in-office work
Respectful and nourishing work environment, where every opinion is heard and everyone is encouraged to be an active part of the organizational culture.
Regular team bonding activities like cooking and cocktails classes, bingos, stand up comedy, magic shows, virtual happy hour, lunch and learns, etc.
Additional Parental Leave
Training and growth monthly stipend
Excellent communication between teams, people, organizations and leaders

Interested?

Ready to apply? Visit us at httpswww.zerofox.comcareers to find out more and join the best team in the security industry.
Not ready to apply? Email careersatzerofoxdotcom to speak with a member of the team!


ABOUT ZEROFOX

ZeroFoxs mission is clear we protect customers - their data, their assets and their people - across the internet. Through AI-powered technology, global intelligence collection and services provided by a team of expert analysts and threat hunters, we give customers the protection and intelligence needed to disrupt a new era of attacks on the surface, deep and dark web. Now is a great time to join the Fox Den we recently announced our intent to become a publicly traded company via a merger with LF Acquisition Corp. and were named a Top Workplace by the Baltimore Sun. With 150M in funding to date, recognition from Forrester as best-in-class in brand intelligence and numerous awards and honors, joining the ZeroFox team means joining a culture that is committed to excellence and growth. That means committing to the success of each of our employees so you can be the best version of yourself on the best team. If youre ready to join a team that is mission-oriented, customer-focused, collaborative and dedicated, youve come to the right place.


Equal Opportunity, Diversity  Inclusion
We aim to build a team that represents a variety of backgrounds, perspectives, and skills. We embrace inclusion and ensure equal employment opportunity without discrimination or harassment based on race, color, religion, sex including pregnancy, childbirth, or related medical conditions, sexual orientation, gender identity or expression, age, disability, national origin, marital or domesticcivil partnership status, genetic information, citizenship status, military or veteran status, or any other personal characteristic.

"
https://startup.jobs/data-engineer-workstream-4413513,Engineer,Data Engineer,Workstream ,"Chengdu, China",,"


Workstream is a mission-driven company that believes in building premium, modern software solutions for local businesses. There are 2.7 billion hourly workers, who make up 80 of the global workforce, but theyve been heavily underserved by technology and deserve better. We help local businesses around you hire, manage, and retain qualified workers.
 
Our customers include leading brands from multiple sectors, including Burger King, Carls Jr.Hardees, IHOP, KFC, and Culvers. At series B, we are quickly expanding our product portfolio. We are backed by legendary VCs and industry experts like Founders Fund, BOND, and Coatue.

We are in search of a seasoned Data Engineer who has a strong background in Machine Learning Engineering. In this unique role, you will primarily focus on data engineering tasks while leveraging your ML expertise to collaborate with data scientists for model deployment. The ideal candidate is someone who excels in building robust data infrastructures and also has a proven track record in helping data teams deploy machine learning models in real-world environments. Youll be working hand-in-hand with our platform team, supporting and maintaining our data infrastructure, and concurrently assisting data scientists in model deployment and monitoring.
Day in the Life

Design, build, and maintain efficient, reliable, and complex ETL pipelines to process and analyze large volumes of data from various sources.
Develop and enhance our data lakehouse, driving data quality across departments and building self-service tools for analysts.
Define, build, and own data architecture for a trusted, governed, dimensionally-modeled repository of data.
Collaborate with cross-functional teams including data scientists to assist in deploying and monitoring machine learning models in production environments.
Help data scientists develop and maintain ML API services for seamless integration into the companys infrastructure.
Apply knowledge of real-time, streaming, and batch processing concepts to optimize model performance and data handling.
Participate in code and design reviews to maintain high development standards.

Who You Are

BachelorsMasters degree in Computer Science, Data Science, or a related quantitative field.
Proficiency in Python and software engineering.
Proven experience as a Data Engineer, with a solid understanding of SQL, and Big Data technologies.
Expertise in containerization and orchestration technologies like Docker and Kubernetes.
Knowledge of vector stores, databases, and data warehousing concepts.
Experience in deploying, and monitoring ML API services using Flask or FastAPI.
Strong project management skills, with the ability to collaborate effectively with cross-functional teams.

Preferred Qualifications

Experience with Hevo Data or other streaming vendorsFivetran, Airbyte, DMS
Experience with DBT
Experience with Snowflake or Redshift
Experience with orchestration tools such as Airflow
Experience with data catalog solutions such as Atlan
Experience with Metaflow is a plus
Experience with cloud platforms such as AWS, GCP, or Azure
Experience with specialized ML serving tools like Bento, Seldon Core, Hugging Face Inference, Sagemaker Endpoints is a plus.

What We Offer

A mission-driven and value-based company dedicated to empower deskless workers and local businesses
An early employee opportunity at a Series B hyper-growth startup
Work with the founding team and industry veterans to accelerate your career
Competitive salary and equity
Comprehensive health coverage
Performance-based year-end bonuses
Unlimited PTO
RemoteWFH schedule


Additional Information 
Workstream provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. 
We are committed to the full inclusion of all qualified individuals.


"
https://startup.jobs/data-engineer-azm-career-4412518,Engineer,Data Engineer,AZM: Career ,"Riyadh, Saudi Arabia",,"


Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional business requirements.
Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and other tools and technologies.
Work with business stakeholders to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that help the creation of cutting edge business tools.
Design data integration and data quality frameworks
Coordinate with analytics team members to continually optimize and improve the data pipeline. 


"
https://startup.jobs/data-engineer-swish-analytics-4410653,Engineer,Data Engineer,Swish Analytics ,"San Francisco, United States",,"

Company Overview
Swish Analytics is a sports analytics, betting and fantasy startup building the next generation of predictive sports analytics data products. We believe that oddsmaking is a challenge rooted in engineering, mathematics, and sports betting expertise; not intuition. Were looking for team-oriented individuals with an authentic passion for accurate and predictive real-time data who can execute in a fast-paced, creative, and continually-evolving environment without sacrificing technical excellence. Our challenges are unique, so we hope you are comfortable in uncharted territory and passionate about building systems to support products across a variety of industries and consumerenterprise clients.
Job Description
The Swish Analytics team is seeking Data Engineers to have direct impact on the infrastructure and delivery of our core consumer and enterprise data offerings. Were a team passionate about accurate predictions and real-time data, and hope you find satisfaction in building new products with the latest and greatest technologies.  This is a remote position.
Duties

Architect low-latency, real-time analytics systems including raw data collection, feature development and endpoint production
Build new sports betting data products and predictions offerings Integrate large and complex real-time datasets into new consumer and enterprise products
Develop production-level predictive analytics into enterprise-grade APIs
Contribute to the design and implementation of new, fully-automated sports data delivery frameworks

Requirements

BSBA degree in Mathematics, Computer Science, or related STEM field
Demonstrated experience writing production level code
Proficiency in Python and SQL preferably MySQL
Demonstrated experience with Airflow
Experience building end-to-end ETL pipelines
Experience utilizing REST APIs
Experience with version control git, continuous integration and deployment, shell scripting, and cloud-computing infrastructures AWS
Experience with web scraping and cleaning unstructured data
Knowledge of data science and machine learning concepts
Experience with Kubernetes highly preferred
A strong interest for US sports and sports betting. You love sports, particularly the NFL, NBA, MLB, NHL, College Football, College Basketball, and Tennis, and can use your knowledge of the sport to inform your work with complex datasets

Base salary90-145,000
Swish Analytics is an Equal Opportunity Employer. All candidates who meet the qualifications will be considered without regard to race, color, religion, sex, national origin, age, disability, sexual orientation, pregnancy status, genetic, military, veteran status, marital status, or any other characteristic protected by law. The position responsibilities are not limited to the responsibilities outlined above and are subject to change. At the employers discretion, this position may require successful completion of background and reference checks.

"
https://startup.jobs/data-engineer-hadoop-gcp-capco-4410592,"Engineer,Hadoop","Data Engineer (Hadoop, GCP)",Capco ,,,"

 
CAPCO POLAND
Capco Poland is a global technology and management consultancy specializing in driving digital transformation across the financial services industry. We are passionate about helping our clients succeed in an ever-changing industry.
We also are experts in          focused on development, automation, innovation, and long-term projects in financial services. In Capco, you can code, write, create, and live at your maximum capabilities without getting dull, tired, or foggy.
We are looking for Data Engineers to support one of our best-in-class clients from banking. Our clients Big Data Lake is the largest aggregation of data ever within financial services with over 300 sources and a rapidly growing book of work. The main aim is to build new analysis approaches and cases based on existing data sources and create a Big Data ecosystem. Sometimes it is migration from on-premise to cloud, sometimes establishment of on-premises solutions, and sometimes cloud solutions separately.
 
THINGS YOU WILL DO

Deliver an ecosystem of curated, enriched, and protected sets of data  created from global, raw, structured, and unstructured sources
Collect, store, analyze, and leverage data
Integrate data with the architecture used across the company
Data Engineering and Management
Data development process design, build and test data products that are complex or large-scale
Promote development standards, code reviews, mentoring, testing, scrum story writing
Cooperate with customersstakeholders

 
TECH STACK ETL, Hadoop-Based Analytics Hbase, Hive, mapReduce, Kafka, Spark, BI, ETL, Database, etc, Python, Spark, GCP cloud storage, Big Query, PubSub, Data Flow, Jenkins, GitHub 
 
SKILLS  EXPERIENCES YOU NEED TO GET THE JOB DONE

Experience working with data pipeline building technologies Python, Spark
Experience with Hadoop eco-system and data management frameworks
Good knowledge of ETL and Data warehouse concepts
Good knowledge of SQL and relational database design
Knowledge of GCP cloud storage, Big Query, PubSub, Data Flow
Knowledge of CI CD, Agile, DevOps, Software Development Life Cycle SDLC
Understanding users requirements and functional specification
Understanding of tools and components of Data Architecture
Excellent communication, interpersonal, and decision-making skills
Good English knowledge

 
WHY JOIN CAPCO?

Employment contract andor Business to Business - whichever you prefer
Possibility to work remotely
Speaking English on daily basis, mainly in contact with foreign stakeholders and peers
Multiple employee benefits packages MyBenefit Cafeteria, private medical care, life-insurance
Access to 3.000 Business Courses Platform Udemy
Access to required IT equipment
Paid Referral Program
Participation in charity events e.g. Szlachetna Paczka
Ongoing learning opportunities to help you acquire new skills or deepen existing expertise
Being part of the core squad focused on the growth of the Polish business unit
A flat, non-hierarchical structure that will enable you to work with senior partners and directly with clients
A work culture focused on innovation and creating lasting value for our clients and employees

 
ONLINE RECRUITMENT PROCESS STEPS

Screening call with the Recruiter
TechnicalCompetencies interview with Capco Hiring Manager
Culture fit with Head of Engineering
FeedbackOffer

 
Follow us here    or contact us directly  recruiting.polandcapco.com 
 

"
https://startup.jobs/data-engineer-data-bricks-3cloud-4408264,Engineer,Data Engineer (Data Bricks),3Cloud ,,,"


Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you!
At 3Cloud, we hire people who arent afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Clouds core values because they know that it will result in amazing experiences and solutions for our clients and each other.

Job Highlights


Our technology focus is on Azure  youll have plenty of exciting opportunities to grow your skills in Microsoft and Azure in an environment committed to technical excellence and client experience in a very specific, defined space. Six months of experience on our team is worth years somewhere else.

Microsoft Partnerships  Our great global relationship with Microsoft ensures that we have a pipeline of cutting-edge Azure work opportunities, and access to the teams that have built the platform. You wont find this at other places.

Competitive compensation package, salary, allowance, standard benefits including quarterly and annual performance-based cash bonus and other remuneration.

Great working environment and company culture with flexible work location.

 
Key Responsibilities

Support the development of high performing, reliable and scalable solutions
Clearly communicate technical details to business and management personnel
Work independently or on a team to design and develop database solutions
Assist business development team with pre-sales activities and RFPs

 
Qualifications

Bachelors degree in computer science, engineering, data science or a related field
Minimum of 5 years of experience with database design, at least 2 years with Azure technologies, and previous Consulting experience
Knowledge of Databricks development, including Scala, Python, and Spark
Advanced knowledge of SQL, including ability to write stored procedures, triggers, analytic functions, and performance tuning for Synapse
Ability to develop utilizing the following technologies 


Data Movement Apache Spark, plus Azure Data Factory and legacy SSIS
Data Warehousing Azure Synapse, CosmosDB
Azure Storage Technologies Data Lake, Blob Storage


Expertise in Spark DataFrames API and architecture to ingest and manipulate data, including exploring, preprocessing, joining, filtering, dropping sorting, partitioning, and renamingmanipulating columns in the dataset
Understanding of network infrastructure and security including 

Workspace Deployment
Azure Cloud Concepts
Network Security


Experience with Informatica Informatica migration experience would be nice
Eagerness to contribute in a team-oriented environment
Desire to work in an information systems environment
Excellent communication written and oral and interpersonal skills for both technical and non-technical teams
Passionate about learning new technologies
Analytical approach to problem-solving; ability to use technology to solve business problems
Ability to work in a fast-paced environment

 
Preferred ExperienceSkills

Machine LearningAI Practitioner expertise, including 

Predictive, Prescription, and Descriptive Analytics using Data Science tools and technologies
Experience with Azure ML StudioServices, and R programming language
Understanding of analysis models, including Supervised vs Unsupervised, Regression vs Classification, clustering, and cross-validation
Building, Tuning, and deploying analysis models with Spark ML and ML Flow


Knowledge of Power BI is a plus
Relevant Certifications are a plus


Dont meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if youre excited about this role but your past experience doesnt align perfectly with every qualification in the job description, we encourage you to apply anyway.
At this time, we cannot sponsor applicants for work visas.


"
https://startup.jobs/data-engineer-3cloud-4408263,Engineer,Data Engineer,3Cloud ,,,"


Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you!
At 3Cloud, we hire people who arent afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Clouds core values because they know that it will result in amazing experiences and solutions for our clients and each other.

Job Highlights


Our technology focus is on Azure  youll have plenty of exciting opportunities to grow your skills in Microsoft and Azure in an environment committed to technical excellence and client experience in a very specific, defined space. Six months of experience on our team is worth years somewhere else.

Microsoft Partnerships  Our great global relationship with Microsoft ensures that we have a pipeline of cutting-edge Azure work opportunities, and access to the teams that have built the platform. You wont find this at other places.

Competitive compensation package, salary, allowance, standard benefits including quarterly and annual performance-based cash bonus and other remuneration.

Great working environment and company culture with flexible work location.


 
General Required Technical Skills

Expertise in designing and implementing logical and physical data models for cloud and hybrid data warehouse environments  
Implementing  data architectures to support a variety of data formats and structures including structured, semi-structured and unstructured data  
Experience with multiple full life-cycle data warehouse implementations  
Understanding of data architectures required to support data integration processing  
Experience with data modeling technologies such as ERStudio, ERWin or similar  
Experience with Microsoft Azure Data Platform services including Azure Data Lake Store, Azure Storage, Azure Synapse, Azure Data Factory, Azure SQL database, Logic Apps, APIs  
Demonstrated ability to quickly learn, adopt and apply new technologies  
Data profiling and creation of source to target mappings  
Ability to provision and configure Azure data service resources 

 
Detailed Required Skills

Python  SQL Scripting
SQL, PySpark
General Cloud Architecture competency skillset  - Capable of taking requirements and is build out data pipelines
API Knowledge is required, but preference given to candidates who can create APIs

 
Preferred ExperienceSkillsCertifications

Microsoft Azure Cosmos DB, Data Flows, Express Route, Azure Active Directory  
Experience creating strategies to migrate customers from on-premise environments to Azure   
Power BI and semantic modeling  
AWS Glue  Azure Data Factory
AWS S3  Azure Blob
AWS Athena  Azure Databricks
AWS Redshift  Azure Synapse
AWS ECS  Azure AKS


Dont meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if youre excited about this role but your past experience doesnt align perfectly with every qualification in the job description, we encourage you to apply anyway.
At this time, we cannot sponsor applicants for work visas.


"
https://startup.jobs/data-engineer-wavehq-4404728,Engineer,Data Engineer,Wave HQ ,"Toronto, Canada",Full-Time,"

We believe small businesses are at the heart of our communities, and championing them is worth fighting for. We empower small business owners to manage their finances fearlessly, by offering the simplest, all-in-one financial management solution they cant live without.

About the Role

Reporting to the Senior Manager of Data Platform and Operations, as a Data Engineer you will be building tools and infrastructure to support efforts of the analytics, ML and the business as a whole.

Were looking for a talented, curious self-starter who is driven to solve complex problems and can juggle multiple domains and stakeholders. This highly technical individual will collaborate with all levels of the Data team as well as the various engineering teams to develop data solutions, scale our data infrastructure and advance Wave to the next stage in our transformation as a data-centric organization. 

This role is for someone with proven experience in complicated product environments. Strong communication skills are a must to bridge the gap between technical and non-technical audiences across a spectrum of data maturity.
Heres how youll make a difference

Youre a builder. Youll be responsible for the design, build and deployment of our data pipelines - batch, incremental and stream-based.
Youll make things better. You will collaborate within a cross-functional team in the planning and roll-out of data infrastructure services.
Youll build relationships. As a strong software engineer who works with data, youll have people coming to you for technical assistance. You will be helping them succeed, and your outstanding ability to communicate with people will help them do that.
We love our customers at Wave. Your customers are internal, and external too. You can take a look at existing structures and systems and know how to help our internal customers surface the data they need to excel in serving our external customers.
Youll drive process and tool improvements to enable data-driven decisions across Wave. Your work will mean something and have an impact on the company - our team relies on data, analytics and ML insights being delivered reliably to make smarter business decisions.

Youll thrive here if

Youre self-motivated and have the ability to work autonomously. No ones going to be peering over your shoulder here. We count on you to get your work done, in ambiguous conditions, with tight deadlines, while still producing high-quality work. 
You are all about collaboration. You enjoy working with different teams across Wave.  We follow Scrum practices within an agile framework.
You value personal and team development. You enjoy mentoring junior engineers in honing new skills, while helping your team to identify the most important aspects of engineering and best practices.
You are a stellar communicator. This means you know how to translate technical terms into non-technical language that your grandma could understand.
Enjoy the challenge of helping us build and manage a fault-tolerant data platform that scales.

These will help you succeed

At least 3 years of experience in data engineering, specifically in building data pipelines and data infrastructure. This is important because this is what youll be doing most of the time, and we need someone whos done this a lot.
At least 3 years of experience working with cloud infrastructure, including container development with Kubernetes and Docker infrastructure as code IaC using Terraform and GitOps or other infrastructure automation on AWS.
Experience building messaging and stream processing capabilities using Confluent or Kafka MSK and its related components.
Experience working with multi-stage workflows using serverless services.
Previous experience building data lakes using Delta Lake or Apache Hudi.
Experience performing hands-on development, leading code reviews and testing, and leveraging automated frameworks.
Experience developing and deploying solutions leveraging CICD processes to orchestrate automated batch and NRT Near Real Time pipelines running AWS Glue and dbt data transformations.
Experience using Python, SQL and dbt.
Experience working with cloud integration tools such as AWS Glue or AWS EMR.
Working knowledge of data integration tools such as FiveTran, Stitch and Census.
Knowledge and practical experience with Data Vault 2.0 on Redshift or another data warehouse is a definite bonus!


At Wave, youre treated like the incredible human being you are. 


Work From Where You Work Best We will always have a welcoming, energizing, and world-class office in Toronto with a space for you. Or, if youre more comfortable working from home, the choice is yours.


We Care About Future You You will stretch yourself and you will grow at Wave. You will also be supported on this journey with diverse learning experiences, educational allowances, mentorship, and so much more.

We Support the Full You We make a serious investment in your health  wellness. When we think about benefits we think about body, mind,  soul and we take this stuff very seriously. 

We Take Care of the Fundamentals Fair compensation, all the office perks youd want, and the various goodies youd expect from a growing tech company. This is the obvious stuff, but we dont want you to think we forgot!

We believe that a diverse and inclusive culture creates the best workplace. We embrace our differences, value individuality, and the broad spectrum of every Wavers skills and abilities. We challenge each other from a place of respect and pursuit of continuous growth. We trust each other and encourage everyone to bring their authentic selves to work, everyday. As Wavers, our voices matter, our opinions are met with an open mind. The best ideas win, no matter whose they are.  Contributing to an inclusive culture is a part of all of our job descriptions. 

Weve been continuously recognized as one of Canadas Top Ten Most Admired Corporate Cultures and one of Canadas Great Places to Work in categories including Technology, Millennials, Mental Health, Inclusion and Women.  

Are you ready to be a Waver? Join us!

"
https://startup.jobs/data-engineer-data-platform-rovercom-4402418,Engineer,Data Engineer - Data Platform,Rover.com ,"Barcelona, Spain",Full-Time,"

Who we are 
Want to make an impact? Join our pack and come work and play! with us.

We believe everyone deserves the unconditional love of a petand at Rover, our mission is to make it easier to experience that love. Founded in 2011, the Rover app and website connect dog and cat parents with loving pet sitters and dog walkers in neighborhoods across the US, Canada, and Europe. We empower our community of trusted pet sitters and dog walkers to run their own pet care businesses on Rover with the tools and security of a global company to back them.

Headquartered in Seattle, Washington, we work closely with our teams in Barcelona, London, San Antonio, Spokane and remote locations. Weve got a reputation for being a great place to work, having been named among the 100 Best Companies to Work For in Seattle Business Magazine and Washingtons Best Workplaces in the Puget Sound Business Journal. Were an agile, fast-growing company, and our leadership comes from some of the worlds most respected tech companies. 

At Rover, our furry coworkers are just as important as our human onesand we wouldnt have it any other way. Along with making the joys of pet parenthood more accessible, were committed to fostering a diverse, inclusive, and welcoming community of pet peopleand that starts with our employees.

DATA ENGINEERING AT ROVER
Data powers everything at Rover. We are a small team that works closely with all parts of the organization to ensure that the data available to the business is fresh and accurate, our application databases are healthy, and our reporting infrastructure meets the needs of everyone from analysts to the C-suite. 

Do you want to help modernize our Data Platform by contributing to our ETL tooling? Perhaps youre passionate about working with folks all throughout the business from Product to Data Science. We own all things data-related at Rover, from the application databases to analytics infrastructure, which means youll get the chance to work on exciting and impactful projects and learn a ton along the way.

Your Responsibilities

Contribute to cross team data projects to ensure our pipelines are reliable, efficient, and testable
Communicate efficiently across team boundaries to establish overarching data architecture, and provide guidance to individual teams 
Deliver well-defined, tested, documented, and code-reviewed ETLELT pipelines and DBT models
Build and maintain self-service tools that empower engineers, data scientists, and analysts to quickly get key business insights
Collaborate with data scientists to train, deploy, and leverage Machine Learning models 
Investigate and solve data related bugs and technical issues efficiently

We dont view anything here as a rigid requirement. Instead here are some ideas of the backgrounds were interested in. You might be a good fit if you have 

2-7 years of DataBIAnalyticsSoftware engineering, data science, or similar experience
Expertise in Python or JavaC etc. with a passion for writing high-quality code
Advanced SQL skills with experience writingtuningdebugging queries against large datasets from multiple sources 
Experience building data processing and ETLELT pipelines that are reliable, properly monitored, and easy to maintain
Familiarity with the majority of our tech stack Python, DBT, Redshift, Docker, Spark, Kubernetes, Terraform

Benefits of working at Rover

Competitive compensation
Permanent contract
Company Equity RSUs
Private health insurance
Gym allowance 
Bring your dog to work and unlimited puppy time
Monetary help for adopting a dog plus yearly credit to use on our platform 
Flexible work hours, sometimes youll need to be in at certain times, but on the whole, were pretty flexible when it comes to managing workload and time
Grab snacks, fresh fruit, in our kitchen to keep yourself going
Regular team activities, including happy hours, game nights, and more


We require authorization to work in Spain and to reside in the Barcelona area
Please submit your CV in English.


Rover is an equal opportunity employer committed to promoting a diverse, inclusive and inventive environment with the best employees. Were driven by seeing our people succeed and grow, and we work to ensure everyone contributes to their fullest potential. We consider all qualified applicants without regard to age, race, color, ancestry, national origin, religion, disability, protected veteran status, sex, gender identity or expression, sexual orientation, or any other protected status in accordance with applicable laws, regulations and ordinances. 

We are committed to work with you to look for reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.

"
https://startup.jobs/data-engineer-evolv-consulting-4402061,Engineer,Data Engineer,evolv Consulting ,"Dallas, United States",Full-Time,"


Do you want to support innovative clients at the intersection of cutting-edge technologies and impactful relationships? evolvs culture of inclusion, integrity, collaboration, humility, passion, and purpose has been awarded a Best Place to Work year after year.
 
We are seeking to hire an experienced Data Engineer. Lets evolv together!
 
Responsibilities
 

Lead the design of scalable big data platformssolutions used to ingest and process accurately scaled traffic data
Build end-to-end solutions that enable business stakeholders, BI engineers, and analysts to self-serve their business intelligence needs
Develop and automate large scale, high-performance, scalable platform batch andor streaming to drive faster analytics
Design large-scale, complex applications and frameworks with excellent run-time characteristics such as low-latency, fault-tolerance, and availability
Build and maintain custom frameworks to support engineeringanalytics needs
Partner with analytic consumers and data scientists to build and improve newexisting constructs and solve data engineering problems at scale
Deploy inclusive data quality checks to ensure high quality of data
Evangelize high quality software engineering practices towards building data infrastructure and pipelines at scale
Other duties as assigned

 
Requirements
 

3 years of experience as a Data Engineer or in a similar role data science, business intelligence or business analytics
Experience with SQL, Python, DBT or Airflow, Snowflake, data modeling, data warehousing, and building ETL pipelines

Bachelors degree or higher in a quantitativetechnical field e.g., Computer Science, Economics, Finance, Mathematics, Statistics, Engineering

Proven track record of independently delivering big data solutions
Ability to manage competing priorities simultaneously and drive projects to completion
Knowledge of continuous integration, testing methodologies, TDD and agile development methodologies
Knowledge of Data formats Parquet, ORC etc. and consensus management systems
Exposure to structured or unstructured storage and distributed caching.
Experience in open-source technologies is plus
Structured thinking with ability to easily break down ambiguous problems and propose impactful solutions
Attention to detail and effective verbalwritten communication skills



"
https://startup.jobs/data-engineer-ii-chainalysis-4401057,Engineer,Data Engineer II,Chainalysis ,,,"

Blockchain technology is powering a growing wave of innovation. Businesses and governments around the world are using blockchains to make banking more efficient, connect with their customers, and investigate criminal cases. As adoption of blockchain technology grows, more and more organizations seek access to all this ecosystem has to offer. Thats where Chainalysis comes in. We provide complete knowledge of whats happening on blockchains through our data, services, and solutions. With Chainalysis, organizations can navigate blockchains safely and with confidence.
The global financial ecosystem is changing. Revolutionary blockchain technology has unlocked the potential for people around the world to have more equal access to wealth and information. This transformation has begun with the mass global adoption of cryptocurrencies but like all new financial systems, it needs greater trust to realize its full potential and remain safe from bad actors. Thats where we come in. The Chainalysis blockchain data platform enables businesses, governments, and banks to solve the worlds most high-profile criminal cases, paving the way for an economy built on blockchains.
The Engineering team at Chainalysis is inspired by solving the hardest technical challenges and creating products that build trust in cryptocurrencies. Were a global organization with teams in the UK, Denmark, and the USA who thrive on the challenging work we do and doing it with other exceptionally talented teammates. Our industry changes every day and our job is to build a flexible platform that will allow us to adapt to those rapid changes.
As a Data Engineer, Analytics, youll be responsible for building and maintaining the data layer for our analytics stack, top to bottom. You will connect our analytics stack with the rest of our infrastructure using both streaming and batch processes. Youll write and maintain ELTs and their orchestration in order to produce meaningful and timely insights. Youll work closely with our BI engineers to build a system that has a sensible balance between data access and security, empowering analysts to support critical data-driven decisions across the company.
In one year youll know you were successful if you and have

Youve worked with other engineering teams to understand their data lifecycle, the right integration points and helped them integrate analytics at the design stage of projects.
Youve developed and managed scalable data pipelines and build out new integrations with internal and external data sources
Youve maintained optimal data pipeline architecture, including looking for and proposing improvements to the existing architecture.
Together with the rest of the team, youve evaluated our current analytics stack and are working towards a system thats modern, scalable, maintainable and cost-effective.

 
A background like this helps

Experience in greenfield data engineering projects, specifically in analytics and data infrastructure projects.
Advanced knowledge of modern data pipeline architecture and the AWS ecosystem including Redshift, Snowflake, Fivetran, Stitch, Databricks, Kafka, Airflow on Kubernetes and dbt
Experience performing root cause analysis on data logging and ingestion processes, identifying opportunities to improve instrumentation and observability
Comfort with Python, Java or another JVM language
Experience writing Advanced SQL queries
Have an interest in cryptocurrencies or a desire to learn - we can help!

 
LI-BD1
 

At Chainalysis, we help government agencies, cryptocurrency businesses, and financial institutions track and investigate illicit activity on the blockchain, allowing them to engage confidently with cryptocurrency. We take care of our people with great benefits, professional development opportunities, and fun.
You belong here. 
At Chainalysis, we believe that diversity of experience and thought makes us stronger. With both customers and employees around the world, we are committed to ensuring our team reflects the unique communities around us. Some of the ways were ensuring we keep learning are an internal Diversity Committee, Days of Reflection throughout the year including International Womens Day, Harvey Milk Day, World Humanitarian Day, and UN International Migrants Day, and a commitment to continue revisiting and reevaluating our diversity culture. 
We encourage applicants across any race, ethnicity, gendergender expression, age, spirituality, ability, experience and more. Additionally, if you need any accommodations to make our interview process more accessible to you due to a disability, dont hesitate to let us know. You can learn more here. We cant wait to meet you.  
Applying from the EU? Please review our Chainalysis Applicant Privacy Policy.
  
By submitting this application, I consent to and authorize Chainalysis to contact my former employers, and any and all other persons and organizations for information bearing upon my qualifications for employment.  I further authorize the listed employers, schools and personal references to give Chainalysis without further notice to me any and all information about my previous employment and education, along with other pertinent information they may have, and hereby waive any actions which I may have against either partyies for providing a reference.  I understand any future employment will be contingent on the Company receiving satisfactory employment references.


"
https://startup.jobs/data-engineer-mh101-methods-business-and-digital-t-4398955,Engineer,Data Engineer (MH101),Methods Business and Digital Technology ,"London, United Kingdom",Full-Time,"

Data Engineer
 
Methods is a 100M IT Services Consultancy who has partnered with a range of central government departments and agencies to transform the way the public sector operates in the UK. Established over 30 years ago and UK-based, we apply our skills in transformation, delivery, and collaboration from across the Methods Group, to create end-to-end business and technical solutions that are people-centred, safe, and designed for the future. 
Our human touch sets us apart from other consultancies, system integrators and software houses - with people, technology, and data at the heart of who we are, we believe in creating value and sustainability through everything we do for our clients, staff, communities, and the planet. 
We support our clients in the success of their projects while working collaboratively to share skill sets and solve problems. At Methods we have fun while working hard; we are not afraid of making mistakes and learning from them. 
Predominantly focused on the public-sector, Methods is now building a significant private sector client portfolio. 
Methods was acquired by the Alten Group in early 2022.
Summary Profile 
 
As a Data Engineer at Methods you will be expected to get involved with projects from conception to delivery - leading and supervising other engineers to meet requirements and deliver quality data products. On a day-to-day basis, you will own best practice for data management and data engineering - providing support to other members of the team and raising awareness across the organisation for data issues and solutions. 
When contributing towards projects, youll make effective long-term design solutions and take an active part in sprint reviews and other agile processes. Youll often perform code reviews and provide constructive feedback to other developers. 
Often, you will be called upon to shape and review bids as required based on your expertise. 
In your downtime or in preparation to get involved in new and exciting opportunities, youll be able to use personal development time to keep up to date on the latest trends and technologies. 
 
Requirements
Engineering
 Working with other members of the delivery team across a mix of large and small projects and be responsible for translating data into valuable insights that inform decisions for small to large transformation projects and programmes.  Working with other members of the delivery team you will be responsible for identifying and using the most appropriate analytical techniques, developing fit-for-purpose, resilient, scalable and future-proof data services that meet user needs and design and write and iterate code from prototype to production-ready.  Communicating effectively across organisational, technical and political boundaries to understand the context and how to make complex and technical information and language simple and accessible for non-technical audiences.  
 Creating effective data visualisation, appropriate to the audience.  Working with the Lead Data Engineer to support the growth and development of the team.  Aware of, and keep up to date with advances in digital analytics tools and data manipulation products.  Collecting, collating, cleansing, synthesising and interpreting data to derive meaningful and actionable insights.  Understanding of how to expose data from systems for example, through APIs, link data from multiple systems and deliver streaming services.  
 Producing data models and have an understanding of where to use different types of data models.  Provide training, support and mentoring to a team of Data Engineers and helping to grow the team.  Ensuring that risks associated with deployment are adequately understood and documented.  

Skills  Experience 
 
Essential 
 Experience of creating PowerBI solutions and dashboards from row level data including data structure optimisation through to visualisation and dashboard creation.  Strong T-SQL Development including debugging  troubleshooting.  Experience in SQL Server Integration Services SSIS.  
 Solid Relational Database design skills with an eye for performance optimisation.  an ability to translate business requirements into technical specifications.  Attention to detail and ability to QA own and other team members work.  Good experience with ETL  
Infrastructure, Azure Data Factory  SSIS, SQL On-premCloud. 
Good experience with AnalyticalReporting 
SQL On-premCloud, Analysis Services  Tabular Data Model, Power BITableauT-SQL. 
Understanding of analytical tools; you are numerate. 
 
Desirable 
 Experience with NoSQL type environments, Data Lakes, Lake-Houses Cassandra, MongoDB or Neptune.  Have experience with Python, Scala or Java.  Have cloud based experience, preferably with AWS andor Azure.  Knowledge of statistics principles necessary to interpret data and apply models. For example, knowledge of errors and confidence intervals to understand whether a relation seen in the data is real.  Exposure to high performing, low latency or large volume data systems i.e. 1 billion records, terabyte size database.  
 Working within a continuous integration environment with automated builds, deployment and unit testing.  Exposure to iterativeagile development methodologies such as SCRUM.  Experience of working in a consultancy.  

Additional 
This role will require you to have or be willing to go through Security Clearance. As part of the onboarding process candidates will be asked to complete a Baseline Personnel Security Standard; details of the evidence required to apply may be found on the government website Gov.UK. If you are unable to meet this and any associated criteria, then your employment may be delayed, or rejected. Details of this will be discussed with you at interview.
Benefits
Methods is passionate about its people; we want our colleagues to develop the things they are good at and enjoy.
By joining us you can expect 
Autonomy to develop and grow your skills and experience 
 Be part of exciting project work that is making a difference in society  Strong, inspiring, and thought-provoking leadership  A supportive and collaborative environment  
As well as this we offer 
Development  access to LinkedIn Learning, a management development programme, and training 
Wellness  247 confidential employee assistance programme 
Flexible Working  including home working and part time 
Social  office parties, breakfast Tuesdays, monthly pizza Thursdays, Thirsty Thursdays, and commitment to charitable causes 
Time Off  25 days of annual leave a year, plus bank holidays, with the option to buy 5 extra days each year 
Volunteering  2 paid days per year to volunteer in our local communities or within a charity organisation 
Pension  Salary Exchange Scheme with 4 employer contribution and 5 employee contribution 
Discretionary Company Bonus  based on company and individual performance 
Life Assurance  of 4 times base salary 
Private Medical Insurance  which is non-contributory spouse and dependants included 
Worldwide Travel Insurance  which is non-contributory spouse and dependants included 
Enhanced Maternity and Paternity Pay  
Travel  season ticket loan, cycle to work scheme 
For a full list of benefits please visit our website www.methods.co.ukcareersbenefits 

"
https://startup.jobs/data-engineer-hallowapp-4396488,Engineer,Data Engineer,Hallow ,"Chicago, United States",Full-Time,"

Who we are

Were building a prayer and meditation app. One that is authentically Christian and Catholic, while at the same time a resource for anyone looking to grow deeper in a relationship with God. We believe that people are hungry for peace, and that there is no surer way to find it than by learning to sit in silence with Christ Come to me, all you that are weary and are carrying heavy burdens, and I will give you rest Mt 1128. 

Our goal is to build something truly great. Something that combines the beauty of the Churchs spirituality with world-class product and content execution.

We launched the app in December of 2018 and have been blown away with the incredible growth  traction. Hallow has been downloaded over 10 million times with over 200,000 5-star reviews. Were blessed to be backed by some of the best mission-driven investors in the world and are excited to build out the team to help a lot more folks grow closer to God.

We are a startup. We move quickly and take big swings. We are a small and fast moving team and each person is responsible for making an impact. It is hard work, but also deeply meaningful. Wed be honored  humbled if youd consider joining us. 


Tldr We are seeking an experienced Data Engineer to join our team and lead the effort of scaling the data platform  infrastructure and ensuring we are collecting reliable data at the right time in the right way.

What Youll Do


Build scalable data infrastructure Build data models, ETL processes, and data integrations to ensure a scalable and robust data infrastructure. Build new and support existing data pipelines.

Ensure data quality Collaborate with data analysts and software engineers to ensure data quality, accuracy, and consistency

Improve data performance Monitor and troubleshoot the performance of the data infrastructure and make recommendations for improvements

What Youll Love


Mission This work is incredibly humbling. Every day we hear amazing stories and we get the pleasure of working on something thats impacting lives. One of our favorite user quotes I hate quoting an old, overused cliche, but Ive been very lost. And I think for the first time in a while I may be found.

Growth Youll get to journey with a VC-backed Silicon Valley startup from the beginning. When the company is this small, every employee is a core team member who is expected to help build the company as a whole as we settle into a culture and continue to expand the team.

Flexibility HQ will be in Chicago with the full team and would love to have everyone there. That being said, were super flexible with location  hours; dont care when or where you work, just that it gets done.

Comp We pay competitive market rates both in terms of equity, cash,  benefits. 

What Were Looking For


Passion Were looking for someone excited and passionate about our mission, and passionate about data engineering.

Experience A foundation of 3 years of experience in data engineering; strong knowledge of SQL and OOP in Python or other programming language; experience with Snowflake or Redshift, Airflow, and DBT strongly preferred

Grit  Detail-Oriented A start-up is tough and we really care about what were doing. Youll be jumping into a brand new role on a brand-new team - perseverance and attention to detail are important.


We are honored that youd consider joining the team and look forward to connecting with you.

Best,

Alex at Hallow
CEO  Co-Founder

"
https://startup.jobs/data-engineer-for-an-innovative-private-market-solutions-company-beon-tech-studio-4396162,Engineer,Data Engineer for an Innovative Private Market Solutions Company,BEON Tech Studio ,"New York, United States",,"

Be a part of a leading provider of private market solutions, offering innovative technology and expertise to help companies achieve their liquidity goals. With a deep understanding of the complex and evolving private market landscape, the company provides an end-to-end platform that simplifies secondary market transactions for private companies, their employees, and investors. Their cutting-edge technology and robust compliance tools provide unparalleled efficiency and security for all stakeholders involved in private market transactions. The platform also offers a range of liquidity programs that can be tailored to meet each clients unique needs, including tender offers, block trades, and direct listings. The company has a proven track record of success, working with diverse clients, from startups to established private equity firms and venture capital funds. Their team of experts is dedicated to providing unparalleled service and support to ensure their clients achieve their desired outcomes. Key Details to Catch Your Eye  You will have the opportunity to work with cutting-edge technology and robust compliance tools. Company culture of innovation and collaboration, where each team member has the opportunity to make a meaningful impact.  What Team Members Love About This Project Great company. They have an innovative environment and a ton of market experience. There are no cons; its a great place.Anonymus from Glassdoor.
Requirements
Your Skills
 4 years as a Data or Software Engineer Semi SrSr. Work experience with Apache Kafka. Strong experience with event-driven architecture and systems.  Knowledge of Python. Familiarity with Software Engineering best practices and API Design. Bachelors degree in Software Engineering. English Proficiency.  How Youll Stand Out  Experience working in the financial industry. Familiarity with Ruby On Rails, JavaScript, AWS or DevOps.   The Screening Process  Verification by the BEON team. 1-hour Technical Interview with the Engineering Team. 30-minute Cultural Interview.   Total expected timeframe 4-6 days.
About BEON.tech
BEON.tech connects the brightest Latin American talent with the most innovative and disruptive U.S. companies. Youll get access to a custom-vetted pool of full-time, long-term, remote software jobs with compensation comparable to U.S.-based positions.
To join BEON.tech is to be in a devs-first company, which means you are the priority when it comes to decision-making, client selection, and growth planning.
Develop your career at the pace you deserve.
Benefits
 USD compensation comparable to U.S.-based positions. A US 1,500 welcome package to get you started with the right gear. Health insurance. Internet service. Trip to Headquarters in Buenos Aires. Flexible payments in crypto, wire transfer, Wise, PayPal, or Payoneer. English conversation club  workshops. Rewards Program Win prizes every month by participating in weekly challenges. The annual winner will earn a trip for two to NYC! Psychotherapy sessions. Unlimited reskilling in Udemy, Educacin IT  OReilly. 

"
https://startup.jobs/data-engineer-analyst-deleteme-4394250,"Engineer,Analyst",Data Engineer/Analyst,DeleteMe ,,Full-Time,"

About DeleteMe, The Online Privacy Company
DeleteMe is an online privacy company that makes easy-to-use tools for consumers and businesses to control what personal information companies, third parties, and other people see about them online.

DeleteMe is a rapidly growing SaaS privacy business operating globally and remotely  we are the emerging leader based on  customers and revenues in a fast-growing nascent category of consumer and enterprise security Privacy Services.  What we do - our mission - matters because we are restoring a sense of privacy, fairness, and control of personal data in the possession of others.  Easier, simpler, control underpinned by a suite of new data privacy laws being passed worldwide will play a part in greater personal security, and freedom, and in stronger democracies in an era where data collection is at unprecedented levels.  This is what our work and brand stand for and we are building a large, sustainable, for-profit business to catalyze this.  We have strong B2C and B2B businesses with respective product offerings informed by feedback from an active customer base growing between 30 and 200 yearyear.  DeleteMe is well-capitalized profitable for the last three years with an 8-figure balance sheet and large-scale venture firms as investors.

DeleteMe is led by a passionate team, backed by premier investment firms, and supercharged by a strong mission to empower consumers with privacy.

Job Summary
The Data EngineerAnalyst will help us manage and use data to make good decisions about what we build, how we help our customers, and how we grow our company. The ideal candidate combines the ability to operate and evolve the data pipelines as well as perform data analysis tasks and will work closely with our engineering and product teams to improve the way we collect structure, and store data. Work with our management, marketing, sales, and service teams to understand what they need and drive the requirement process peaking their domain language to get them the data they need the way they want to empower them to make good decisions.

We are building our data practice from the ground up so you will have the opportunity to work on a new modern data engineering tech stack. We use Snowflake as our Data Warehouse and Fivetran as our ETL tool. We use DBT to extract, transform, and load our data and enable CICD capabilities. We are big Open Source users and use Apache Superset as our reporting tool. Reverse ETL is performed using Hightouch and components of Rudderstack. Our tech platform includes AWS, RDS, Terraform, Docker, and Kubernetes. Python is planned to become the to-go tech for data analysis and modeling. 
We are just getting started, so you will be a key driver in building our data engineering practice from the ground up. 

Responsibilities
Maintain and evolve our data pipeline infrastructure; help identify the newbest tools and tech for the job.
Improve our data collection and storage practices to enhance the value of our internal dataBuild pipelines and data flows.
Ensure our data is available where it is needed when it is needed.
Interview business users and document data reporting and analysis requirements, develop ELT, data models, and dashboards.
Design and implement a strategy and systems for data analysis, reporting, querying, and visualization to enable internal and external partners to explore our data, answer their questions, and make good decisions. 

Requirements
4 years of proven working experience in data engineeringwarehousing, data analysis, data modeling, and reporting for a business department of a SaaS company, who wants to be a big part of helping a team achieve big things, who believes in privacy as a mission, and who can both operate independently and help a team be more than the sum of its parts. 

These are some of the specific characteristics we are looking for
Strong technical and people skills. 
Excellent written and verbal communication skills. 
Experience with data engineeringdata warehousinganalytics engineering. 
Experience with building SQL-based data pipelines and data models. 
Experience with database design and implementation. 
Experience with gathering and documenting reporting requirements. 
Experience with data reporting and visualization tools. 
Experience with appropriate programming languages and technologies.

Preferred experience with existing toolsets 
Familiarity with Snowflake is a prerequisiterequirement for this position.
Any SQL DB as a primary data source; NoSQL is a plus.
Fivetran, Snowflake, DBT, Hightouch.
Apache Superset, Tableau, or a similar interactive data visualization software.
Basic knowledge of Python, R, or another standard data analysis languageframework.
BS in Computer Science  STEM  Economics.
Ability to thrive in a fast-paced work environment, where change is constant, and flexibility is key.

What We Offer
Comprehensive health benefits flexible schedule. 100 work from home 401k matching. Paid time off. 12 company paid holidays. Gym membership reimbursement. Birthday time off child care expense reimbursement. DeleteMe provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.  This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.



"
https://startup.jobs/data-engineer-match-group-4393485,Engineer,Data Engineer,Match Group ,"Mexico City, Mexico",Contractor,"

Our Mission
As humans, there are few things more exciting than meeting someone new. At Tinder, were inspired by the challenge of keeping the magic of human connection alive. With tens of millions of users, hundreds of millions of downloads, 2 billion swipes per day, 20 million matches per day, and a presence in 190 countries, our reach is expansiveand rapidly growing. 

We work together to solve complex problems. Behind the simplicity of every match, we think deeply about human relationships, behavioral science, network economics, AI and ML, online and real-world safety, cultural nuances, loneliness, love, sex, and more.

Our Values

Member Safety Our top priority is to build an authentic, equitable, and respectful community

Diversity We strive to build a workplace that reflects the rich diversity of our members

Authenticity  Collaboration We encourage authenticity to nurture a trustworthy, collaborative environment

Big Ideas We champion and rally behind big ideas, empowering people to think and act differently and to be unafraid of having a strong point of view

Innovation We never stand still; were inspired to build for whats next, and we must constantly innovate, experiment, and learn

The Team
The Engineering team is responsible for building innovative features and resilient systems that bring people together. Were always experimenting with new features to engage with our members. Although we are a high-scale tech company, the member-to-engineer ratio is very highmaking the level of impact each engineer gets to have at Tinder enormous.

As a data engineer contract working on Tinders Experimentation Platform, you will design and build critical analytical pipelines at massive scale and develop self-serve technologies to enable a large number of AB or multivariate experiments running in production. You will have tons of responsibility, freedom, and opportunity to have a direct and immediate impact on exploring new ideas to drive user satisfaction and company growth. You will help Tinder to evolve relentlessly through rapid iterations on product features, revenue, user growth and advanced matching algorithms. We would love to talk to you if you are interested in helping Tinder users achieve stronger matches, engage more effectively, and create more love, at a truly global scale.
In this role, you will

Support and improve existing experimentation analytics pipelines and systems in production
Design and develop new data pipelines and streaming processes that are highly available, scalable, and reliable
Maintain and evolve dimensional data models and schema designs to improve performance efficiency
Scale the self-serve experimentation platform to drive product innovations in over 190 countries
Partner with engineering, analytics, and product stakeholders to define experimentation standards, guidelines and best practices

Youll need

2 years experience building and maintaining large scale analytics systems
2 years programming experience in Scala, Java or Python
Experience working with Hadoop, Spark or ElasticSearch
Experience with NoSQL or RDBMS platforms - DynamoDB, MySQL, Redis, or Postgres
Experience with streaming platforms - Apache KafkaFlinkBeam or Aws Kinesis
Experience with batch processing orchestration tools like airflowluigi
Experience with developing and maintaining cloud technologies

Nice to have 

Architecture experience, specifically in Amazon Web Services 
BS MS in Computer Science, Computer Engineering, or a related technical discipline
Exposure to analytics, statistics, or AB Multivariate testing



Commitment to InclusionAt Tinder, we dont just accept difference, we celebrate it. We strive to build a workplace that reflects the rich diversity of our members around the world, and we value unique perspectives and backgrounds. Even if you dont meet all the listed qualifications, we invite you to apply and show us how your skills could transfer. Tinder is proud to be an equal opportunity workplace where we welcome people of all sexes, gender identities, races, ethnicities, disabilities, and other lived experiences. Learn more here httpswww.lifeattinder.comdei


"
https://startup.jobs/data-engineer-retool-2-4391129,Engineer,Data Engineer,Retool ,"San Francisco, United States",,"


ABOUT RETOOL 
At Retool, were changing the way software is built. Weve developed the fastest way to build internal tools, saving companies time, resources, and engineering bandwidth. Whether its refunding orders, underwriting loans, managing marketplaces, rolling out new features, analyzing transactions, or providing customer support, Retool makes it dramatically faster and easier to build internal tools that teams need. We believe that the future of software development lies in being a force multiplier for developers and technical builders, helping them move considerably faster building a lot more software.  Were looking for highly collaborative people as we build a world-class team to support this mission and wed love for you to join us! 

WHY WERE LOOKING FOR YOU
Retool is a fast-growing company with quickly evolving business needs. Were looking to hire engineers to help us build out our data ecosystem to serve the needs of our business today and for broader scale years from now. Were looking for someone who is ready to get their hands dirty, is motivated by having an impact on the business, and is constantly curious. This is the right role for someone who thrives while making sense of the blurry space that is data at a high growth startup. 
WHAT YOULL DO
Youll design and build a foundation that strengthens Retools data culture at scale. Youll take on projects that solidify Retools reporting capabilities and help the company remain data-driven. Youll develop and scale ingestion infrastructure, optimize our ETLs, and design our data architecture for scale, with a keen eye for data warehouse management and tooling. Youll also take on ownership of our data stack to ensure that your teammates are able to access the data they need to make decisions and technical teams are able to quickly implement events. Weve already built out a solid stack on top of Segment, Databricks, DBT, and of course, Retool, but we need your help to ensure it scales with the company as our user base grows.
WHO YOULL WORK WITH
Youll work with stakeholders across the business, including data scientists, finance, marketing, engineering, product, operations, and support. Youll be joining a broader team of Retools who are passionate about serving our customers, enjoy collaborating to build an incredibly innovative product, and enjoy swapping stories. If this sounds like you, wed love to hear from you!
IN THIS ROLE, YOULL

Architect and scale a modern data platform that will be used internally by all of Retool
Build and maintain scalable ETL pipelines to efficiently process and transform large volumes of data from source systems into our data warehouse
Work with our engineering teams to ensure robust instrumentation across areas of the product
Partner with business stakeholders to synthesize and develop requirements for core tables
Implement monitoring and observability to guarantee data quality and consistency
Articulate and implement best practices around ingestion frameworks and data pipeline development

THE SKILLSET YOUll BRING 

Background in Data Engineering, 5 years of experience building and maintaining scalable data infrastructure, including distributed processing solutions e.g. Spark, cloud-based data lakes and warehouses e.g. Databricks, Snowflake, BigQuery, workflow management e.g. Airflow, Luigi, and data transformation tools e.g. DBT 
Experience using Infrastructure as Code IaC to automate data infrastructure provisioning and management i.e. Terraform
Experience implementing and defining best data practices at scale
Experience proactively identifying opportunities to improve ETL  dashboard performance and cost
Excellent business acumen with the ability to translate stakeholder requirements into data models
Proficiency with common git workflows and at least one programming language e.g. Python, Scala, Java
A solution-oriented growth mindset. Youll need to be a self-starter and thrive in a dynamic environment
A bias towards communication and collaboration with business and technical stakeholders
Quantitative rigor and systems thinking


Retool offers generous benefits to all employees. For more information, please visit the benefits and perks section of our careers page!
At this time, Retool is only set up to employ in the US and UK. 


"
https://startup.jobs/data-engineer-analytics-wattpad-4389851,"Stats,Engineer","Data Engineer, Analytics",Wattpad ,"Toronto, Canada",Full-Time,"

Wattpad is a global multiplatform entertainment company whose vision is to entertain and connect the world through stories. Since 2006, weve been on a mission to use the power of community and technology to unleash the full potential of stories to the world. Every month 85 million people around the world spend over 23 billion minutes on Wattpad to share and discover stories they cant find anywhere else.  Our brand banner includes Wattpad, Wattpad WEBTOON Studios, Wattpad Books and Wattpad Brand Partnerships. Were proudly based in Toronto, but our reach is global. Come build the future of entertainment and storytelling, and write your next chapter with us!

We are looking for a Data Engineer, Analytics to join our growing Data organization and support the Analytics squad. The squads main focus is to partner with all of Wattpads business units and product squads, to build ad-hoc reporting to metrics creation to forecasting and predictive insights projects. The goal is to advocate for and facilitate data-informed decision-making throughout the organization.


In this role, you will partner with data engineers, analysts and scientists. You will develop solutions and build analytics infrastructure to deliver trusted data. Data is critical to how we make decisions at Wattpad and you will play a key role in helping Wattpad continue to scale!
What youll do

Build and own analytics infrastructure that delivers good quality data,  reliably
Support the building of robust, scalable data ingestion pipelines
Develop complex data models and transformations that standardize analytics products
Provide technical leadership on data solutions that are forward looking
Establish best practices for data warehousing 
Evaluate new tools, build prototypes and be an advocate for modern, scalable data infrastructure


What were looking for

You have solid experience in designing schemas and data models for use in transformation and BI tools 
You are comfortable building ETLELT flows that extract data from various systems 
You have a good understanding of data engineering best practices batch data processing, streaming, etc.
You understand software engineering fundamentals and principles
You are comfortable leading data architecture and design discussions
You have curiosity towards experimenting with new data toolsmethods and are abreast with the latest trends in data
Bonus You have experience with DBT coreDBT cloud and LookMLLooker



Wattpad is conducting all interviews in a distributed manner using applicable third party software where needed and using visual interface tools such as Google Hangouts and Zoom.

About Wattpad


Who are we? Entrepreneurs and Do-ers. Our vision is to entertain and connect the world through stories, and our mission is to use the power of community and technology to unleash the full potential of stories to the world.

What does that mean? We are visionaries, community builders, passionate problem solvers, storytellers, coffee snobs tea drinkers, too!, curious by nature, and culturally diverse.

What are we obsessed with? Our users. Solving complex problems and maximizing flow. Learning constantly. Building the next great storytelling product. Finding the greatest stories ever told. Dogs and cats, coffee, and good snacks.

How do we work? Autonomously, collaboratively, respectfully. Balancing with work, family, and play...and all while having a great time.

Wattpad is a remote friendly company and encourages remote candidates to apply as long as they are located and authorized to work in either the US or Canada excluding Quebec as a precondition of employment. We are not able to sponsor applicants for work permits. 

If you happen to live near the areas of either Toronto, Ontario or Halifax, Nova Scotia, you may also have the opportunity to work from our beautiful offices - 1 located in Downtown Toronto and the other in Halifax.

Culture and Diversity

Wattpad is an equal opportunity employer. We do not discriminate. Period. 

Wattpad welcomes and encourages applications from people with disabilities. Accommodations are available on request for candidates taking part in all aspects of the selection process. We have taken a leadership position on creating a culture and an organization that truly values diversity. We are committed to fostering a global team that reflects the diversity of the Wattpad community. At Wattpad, we believe cultural fit doesnt mean culturally identical, and diversity of thought helps us to challenge one another to think big and think differently. We consider employment applicants without regard to age, race, colour, national origin, citizenship, religion, creed, sex, sexual orientation, veteran status, marital status, disability status or any other protected status.  

If you have any special needs or accessibility requirements, please let us know. We will do our utmost to accommodate, in accordance with applicable local legislation.

Dont meet all the requirements? Studies show women and people of colour are less likely to apply to jobs if they do not meet all the qualifications. Therefore, in an effort to build a more diverse workplace, we encourage you to apply anyways. You might actually be the right person or you may be a good fit for a number of other openings we currently have.

"
https://startup.jobs/data-engineer-garner-health-4387069,Engineer,Data Engineer,Garner Health ,,,"

Garners mission is to transform the healthcare economy, delivering high quality and affordable care for all. By helping employers restructure their healthcare benefit to provide clear incentives and data-driven insights, we direct employees to higher quality and lower cost healthcare providers. The result is that patients get better health outcomes while doctors are rewarded for practicing well, not performing more procedures. We are backed by top-tier venture capital firms, are growing rapidly and looking to expand our team.
We are looking for a Data Engineer to support our technical teams by ensuring ease of access to data within our organization. The ideal candidate for this role will have strong technical skills, including Python, SQL, and AWS as well as a desire to be a hands-on contributor to building out a data platform from the ground up.
Main Responsibilities

Build the data pipelines that power our business
Collaborate across disciplines to high-quality datasets
Protect our users privacy and security through best practices
Support data pipelines in production

Our Tools
Python, AWS, Snowflake, dbt, Terraform, Postgres
The ideal candidate has

2 years of experience building data pipelines in a fast-paced environment
Strong Python knowledge
Experience with Big Data technologies such as Snowflake, RedShift, BigQuery, or DataBricks
Ability to think in principles and frameworks to understand and decompose abstract problems
An aptitude to learn new technologies and tools quickly

Why You Should Join Our Team

You are mission-driven and want to work at a company that can change the healthcare system
You want to be on a small, fast-paced team that nimbly moves to meet new challenges
You love ideating on new features and working with data to find new insights
Youre excited about researching and working with the latest tools and technologies

The salary range for this position is 100,000 - 145,000. Compensation for this role will depend on a variety of factors including qualifications, skills and applicable laws. In addition to base compensation this role is eligible to participate in our equity incentive and competitive benefits plans. 


If you are hired, we may require proof of full vaccination against COVID-19. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement, in accordance with applicable law.

 


"
https://startup.jobs/data-engineer-flagship-pioneering-inc-4382386,Engineer,Data Engineer,"Flagship Pioneering, Inc. ","Cambridge, United Kingdom",,"

What if you could join an organization that creates, resources, and builds life sciences companies that invent breakthrough technologies to transform human health and sustainability?
Company SummaryFL86, Inc. is a privately held, early-stage company developing a novel genomics platform and therapeutics for diseases with a large unmet need.
FL86 was founded by Flagship Pioneering, an innovative enterprise that conceives, creates, resources, and grows first-in-category life sciences companies. Flagship Pioneering has created over 100 groundbreaking companies over the past twenty years, all of which are pioneering novel and proprietary biological, industrial, and engineering approaches to solve major needs in human health and sustainability. These companies include Moderna MRNA Generate Biomedicines, Sana Biotechnology SANA, Tessera Therapeutics, Evelo Biosciences EVLO, Indigo Agriculture, Seres Therapeutics MCRB, Syros Pharmaceuticals SYRS, and Rubius Therapeutics RUBY. To date, Flagship has deployed over 2.5 billion in capital toward the founding and growth of its pioneering companies alongside more than 19 billion of follow-on investments from other institutions. In 2021, Flagship Pioneering was ranked 12th globally on Fortunes Change the World list, an annual ranking of companies that have made a positive social and environmental impact through activities that are part of their core business strategies.
Position Summary

Building and maintaining the cutting-edge cloud-based data infrastructure that enables our research scientists to generate insight from our novel genomics discovery platform, integrating insights into complex disease biology across multiple therapeutic areas.

Responsibilities

Organizingcataloguing large volumes of genomic data, their associated laboratory and phenotypic metadata, and results from their analysis, according to FAIR principles.
Developing methods for data extraction, transformation and integration to support analytics and interpretation.
Constructing datasets that are easy to analyze and interpret in support of support company objectives.
Implementing methods to monitor and improve data reliability and quality.
Developing and running methods in GitLab for continuous integrationdelivery of cloud infrastructures according to specification.
Implementing and running automated data processing pipelines Nextflow.
Implementing methods to monitor and improve system performance.
Supporting technical and business stakeholders in their use of our data infrastructure.
Knowledge-sharing with external partners and collaborators whilst protecting company IP.
Preparing useful admin and user documentation for both technical and business audiences.
Contributing to the development of world-class system architectures and system developmentmanagement processes.

Qualifications

Undergraduate and ideally post graduate qualification in computationalinformatics-related field.
2 years hands-on experience with cloud technologies, preferably AWS.
Competent in administration of Linux-based systems.
Strong programming skills, ideally Python and R.
Familiarity with DevOps tools and approaches such as CICD, GitGitOps, CloudFormation etc.
Familiarity with big data tools and approaches.
An understanding of molecular biologygenomics and appreciation for the role of next generation sequencing technologies in target-based drug discovery.
Enthusiasm for the latest advances in cloud technology and data science and excitement in their transformative potential.
Demonstrated problem solving ability, and ability to rapidly acquire and apply new technical skills.
Ability to work both independently and collaboratively; multi-tasking in a team-oriented, fast-paced environment.
Excellent communication skills across all levels within an organization.



Location Cambridge, England
 
Flagship Pioneering and our ecosystem companies are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status.
 


Recruitment  Staffing Agencies Flagship Pioneering and its affiliated Flagship Lab companies collectively, FSP do not accept unsolicited resumes from any source other than candidates. The submission of unsolicited resumes by recruitment or staffing agencies to FSP or its employees is strictly prohibited unless contacted directly by Flagship Pioneerings internal Talent Acquisition team. Any resume submitted by an agency in the absence of a signed agreement will automatically become the property of FSP, and FSP will not owe any referral or other fees with respect thereto.


"
https://startup.jobs/data-engineer-two-six-technologies-4382362,Engineer,Data Engineer,Two Six Technologies ,"Arlington, United States",,"

 
Two Six Technologies seeking an experienced and motivated Data Engineer to work within a cross-functional development team focused on advancing the state-of-the-art in cyber security technology enhanced by artificial intelligence and machine learning.
Responsibilities Include

Development of data processing pipelines focused on normalizing and fusing disparate cyber data sets. 
Transition of previously developed CRUD and Change Data Capture CDC services into more modular and scalable solution as our customer set continues to grow.

Minimum Qualifications

Bachelors degree in Computer Science or related field
Minimum of 4 years of relevant work experience in software development e.g., Python, Java
Minimum of 2 years of experience with relational databases Postgres preferred, MySQL, Oracle, etc.
Experience with distributed messaging systems Kafka, Pulsar, etc.
Ability to obtain and maintain Government Security Clearance

Nice if you have

Experience with modern event streaming design patterns
Ability to build abstracted, modularized reusable code components
An active DoD clearance Secret or above
Exposure to DevSecOps and Agile environments and processes
Proficient understanding of code versioning tools, such as Git
Experience optimizing code for performance, scalability, and stability
Prior experience in cybersecurity
Experience with Kubernetes, AWS, Docker

Clearance Requirements

Eligibility for Security clearance

 
LI-ZS1
LI-Hybrid
At Two Six Technologies we are driven by innovation. We deliver technological superiority for our nation, allies, and partners by providing products and expertise to solve complex challenges across a vast area of expertise. Our team works in concert to research, design, prototype, build, and refine revolutionary technology that pushes the boundaries of whats possible. You will have the opportunity to collaborate with scientists, developers, and engineers at the forefront of their field who value creativity, curiosity, and innovation.


Two Six Technologies is an Equal Opportunity Employer and does not discriminate in employment opportunities or practices based on race including traits historically associated with race, such as hair texture, hair type and protective hair styles e.g., braids, twists, locs and twists, color, religion, national origin, sex including pregnancy, childbirth or related medical conditions and lactation, sexual orientation, gender identity or expression, age 40 and over, marital status, disability, genetic information, and protected veteran status or any other characteristic protected by applicable federal, state, or local law. 
If you are an individual with a disability and would like to request reasonable workplace accommodation for any part of our employment process, please send an email to accomodationstwosixtech.com. Information provided will be kept confidential and used only to the extent required to provide needed reasonable accommodations.
Additionally, please be advised that this business uses E-Verify in its hiring practices.
EOE, including disabilityvets. 
By submitting the following application, I hereby certify that to the best of my knowledge, the information provided is true and accurate.



"
https://startup.jobs/data-engineer-number8-4376278,Engineer,Data Engineer,Number8 ,,Contractor,"

Overview Number8 is seeking qualified candidates to fill the role of Data Engineer. In addition to a competitive salary rate and a positive work environment committed to delivering high-quality technology solutions, we also offerFlexible schedules and authentic work-life balance


 Opportunities for continuing education


 Social activities per country sponsored by the company


 Birthday celebration


 Payment in US Dollars


About the role
Our client delivers complete mobile repair and maintenance to car owners with the convenience of online booking and instant price quotes, combined with five-star quality service at fair, affordable prices. The company also delivers repair and scheduled maintenance on-location to fleet operators, car rental agencies, and dealership customers.
We are looking for a Data Engineer to join our growing team of Data Science and Business Intelligence. The successful candidate will work on impactful projects that range from creating various data integration pipelines and data models in the companys data platform to understanding reporting requirements from business stakeholders and providing technical solutions. This is a critical role because the data is used for marketing, payroll, HR, advertising, review platforms like Yelp, part distributors like AutoZone, technicians workload and compensation, online platform data, and overall product. They will help translate the data into metrics and meaning for the company.
The main responsibilities for the role include


 Develop and maintain data and reporting platforms and ensure optimal data delivery architecture is consistent throughout ongoing projects


 Support the data needs of multiple teams, systems, and products


 Pull tickets from Jira and answer urgent ad hoc requests from clients or other internal stakeholders


 Put together metrics or data for tickets and requests, perform unit tests to verify data and metrics accuracy


 Communicate data and metrics with others in their department and in the marketing, HR, payroll, and engineering departments


 Document data and reporting solutions in a Confluence doc


 Reprioritize tasks quickly and be flexible to daily tasks 


 
Job SkillsRequirements - 90 English written and oral at least B2 level with excellent communication skills- 3 years of experience with relational SQL databases, including MSSQL Server and PostgreSQL- 3 years of experience with ETL- Experience with SQL preferably T-SQL- Being data-driven, clear understanding of what data is, how it is structured, understanding business metrics Reporting around data use and is data-driven- Experience with SSIS- Experience with reporting tools like Power BI ideally or Tableau- Strong attention to detail and high-quality standards are critical for this role- Ability to take business requests and break them down into manageable steps to achieve a goal 
Nice to have- Experience with Azure Data Factory- Experience with Power BI- Experience with Python- Experience with SSRS- Experience with Big Data
 
 
 
  Apply today to learn more about this exciting opportunity. We are actively interviewing now for this position.
 

"
https://startup.jobs/data-engineer-europe-remote-gelato-digital-4373642,Engineer,Data Engineer (Europe Remote),Gelato Digital ,"Zug, Switzerland",Full-Time,"

Gelato is Web3s decentralized backend empowering builders to create augmented smart contracts that are automated, gasless  off-chain aware.
Leading Web3 projects rely on Gelato to power the execution of millions of transactions across DeFi, NFT and Gaming.
Join our team and work directly with the founders to build the future of web3 infrastructure. Enjoy a lot of perks, travel with us to cool events, and participate in amazing off-site retreats with the team!
Our mission
Gelatos mission is to accelerate the adoption of web3 technology to create a more transparent  democratic digital life for humanity. Our impact is measured by the time saved to achieve global adoption of web3 applications..
Watch a short summary

What we offer
 One of the strongest tech teams in crypto, working fully remote, with HQ in Zug Switzerland Be part of building the backend of web3 on which some of the most widely used applications such as MakerDAO, Connext, Abracadabra and PancakeSwap rely on today GEL Token package - Get a share of the networks token and participate in making decisions about how the project should be moving forward Worldclass Investors - We are backed by top class VCs and Angels including Dragonfly, Galaxy Digital, ParaFi, Gnosis, Stani Kulechov and many more! 
Responsibilities
 Designing, developing, and deploying a robust and scalable data warehouse  data lake solution to unify, process, transform, and store data from multiple internal data sources Building and maintaining data pipelines to ensure data quality, consistency, and availability across various data sources Implementing APIs for efficient querying and data access by internal and external clients, with a focus on security and performance Integrating Web3 and blockchain technologies into the data warehouse  data lake solution, ensuring compatibility with existing and emerging standards Collaborating with cross-functional teams to define data requirements, identify gaps, and develop solutions that meet business and technical objectives Ensuring data infrastructure is optimized for performance and cost, leveraging AWS  GCP and Kubernetes best practices Establishing and maintaining data governance policies and procedures, including data lineage, data cataloging, and data quality monitoring Implementing monitoring, alerting, and logging mechanisms to ensure the health and stability of the data warehouse  data lake system 
Requirements
 7 years of experience in data engineering, with a focus on building and maintaining data warehouse  data lake solutions Strong experience with Elasticsearch near real-time search and Kibana dashboard Strong experience with PostgreSQL, NoSQL and Redis, in the context of data engineering projects Strong experience with GraphQL  Strong experience in backend development preferably with Node.js  TypeScript Experience in writing Clean Code, unit testing and familiarity with SOLID principles Familiarity with cloud infrastructure providers such as AWS, GCP, Azure and Kubernetes for building and maintaining scalable and resilient data infrastructure Familiarity with Kafka, Kafka Streams, Kafka Connect, and Spark Experience integrating Web3 and blockchain technologies into data engineering solutions Proven experience with high-throughput database systems and efficient API design for data access and querying Experience with data visualization and reporting tools, enabling users to extract insights from the data warehouse  data lake Strong interest in the Web3  crypto industry and experience using crypto applications e.g., Uniswap, Aave, etc. Ability to work with high autonomy, demonstrating strong problem-solving and decision-making skills Ability to work in a fast paced startup, with Agile methodology, promoting fast development  release cycles Excellent written and verbal communication skills, with the ability to effectively collaborate with cross-functional teams and present technical concepts to non-technical stakeholders 
Bonus
 B.S. or higher in computer science, software engineering, or a related technical field Prior experience in designing and implementing data solutions for the Web3  crypto industry Knowledge of the inner workings of various blockchain systems, including EVM and alternative platforms Experience with Web3 libraries e.g., ethers.js, web3.js Familiarity with smart contract development using Solidity or other smart contract languages Experience in building and running bots that interact with smart contracts on Ethereum or other blockchain networks Contributions to open-source projects in the data engineering or Web3  blockchain space Familiarity with data privacy and security regulations, as well as best practices for data governance Experience in designing and implementing CICD pipelines  
Technologies we use and teach
 Node.js TypeScript Redis PostgreSQL Google Cloud Platform GCP Kubernetes Ethers.js Solidity Libp2p Data pipeline and ETL tools Data visualization and reporting tools Various blockchain platforms and associated technologies 
Benefits
 Work very autonomously Unlimited holiday yes you heard that right! Work together with one of the best technical teams on Ethereum Build relationships with top blockchain teams which are already Gelato users, including MakerDAO, Zed Run, Connext, Optimism and many more Chance to travel the world to go to exciting events and connect with key players in this industry Join amazing in-person offsites all over the world 

"
https://startup.jobs/data-engineer-f-m-d-kolibri-games-4365951,Engineer,Data Engineer (f/m/d),Kolibri Games ,"Berlin, Germany",Full-Time,"

We are Kolibri Games - a mobile games developer from Berlin. We believe that with the best people, we can make games that players will enjoy for years to come.

Our games have defined the idle genre, and bring enjoyment to hundreds of millions of players all around the world. This is possible due to the talented people who make us who we are. Kolibris, as we call ourselves, are on a mission to continue making great games and were looking for new talent to join our team. We strive to offer you a fair recruitment process and a great candidate experience, as well as a friendly environment to work in, with plenty of opportunities to learn and grow.

Data Engineer fmd
Data technology in gaming is at the forefront of innovation - we handle billions of data events per month -  and youre here to keep us at that level. By bringing your expertise to the table, you will help us identify software, infrastructure, and architecture to ensure that our Data Platform is always on the bleeding edge.

You will work directly with the Data Platform Team and Embedded Data Users to ensure that our data is always available, trustworthy, and useful. You will evangelize performance, efficiency, and reliability- ensuring that data driven decisions contribute significantly towards making Kolibri Games the most player centric game studio in the world.

Responsibilities

Architect and design data pipelines that can handle billions of data events per month;
Implement different streaming and batch use cases for all of our games;
Build automated tests and monitoring solutions to guarantee high data quality and make sure that stakeholders can work with trustworthy data;
Create a scalable data platform to ensure the handling of greater workloads efficiently;
Work closely with data scientist to efficiently deploy machine learning models;
Come up with ideas and compare different technologies to improve our cutting-edge stack;

Required skills

1-3 Years experience in a related field gaming, advertising, analytics, technology, data, or business intelligence
Experience with Python or similar programing language and knowledge of best software engineering practices;
Previous experience using Data Visualization Tools such as; Looker, Tableau or similar;
Strong SQL-like analytical skills in an enterprise data environment;
Solid understanding of data warehouse and business intelligence tools;
Previous experience working with data warehouse workflow tools like dbt.

Desirable skills

Experience extracting data from third-party APIs;
Love to automate things;
You assist customers with onboarding, training and usability efforts;
A detailed analytical mindset, mixed with a high degree of problem-solving skills.


Kolibri Games is an equal opportunity employer. We come from 40 different countries and many different backgrounds. We celebrate diversity and we are committed to creating an inclusive environment for all employees, regardless of their age, gender identity, sexual orientation, ethnicity, religion, physical appearance or disability.

We are an international studio, so dont forget to send in your application in English. Although it is not mandatory, we always appreciate a cover letter stating your motivation to join us.

We look forward to hearing from you!

Your Benefits  Were game to support you


Competitive Salary - We believe that top performers should receive top payment

Learning Budget - We believe in learning. A generous personal learning budget to spend on learning and development, including books, workshops and attending conferences. We also offer in-house training such as coding and German classes

Flexible Working Hours and Home Office  We believe in a good work-life balance

Equipment - State-of-the-art technical equipment, including laptops and phones, which may also be used in your free time

Health and Fitness - We pay a contribution towards a monthly gym membership or a fitness activity

Relocation - Relocation support to help you move to Berlin

Pension - Opportunity to save for your pension tax-free

Bonus Level  We love to have a good time, too


Food and Drinks - Fresh fruits to keep you healthy and fresh coffee to keep you alert. We also have a fully stocked fridge with Smoothies, Coke, Club Mate, beer etc

Parties and Team Events - Apart from our regular parties, BBQs and movie nights we also have a team event budget you can use to buy games or sports equipment to make working here even more fun

Company Holidays - Every year we go on an amazing company holiday to relax and bond as a team

Friday Celebrations - Company-provided dinner and drinks on Friday afternoons


"
https://startup.jobs/data-engineer-arcadiahealthit-4365592,Engineer,Data Engineer,Arcadia ,"Boston, United States",Full-Time,"


Arcadia is dedicated to happier, healthier days for all. We transform diverse data into a unified fabric for health. Our platform delivers actionable insights for our customers to advance care and research, drive strategic growth, and achieve financial success. For more information, visit arcadia.io.


Please note while these roles will ultimately be remote, there is a mandatory training period for the first few months which requires a hybrid training schedule in our Boston headquarters. You will hear more about our detailed onboarding and training process through out the interviews.

Why This Role Is Important To Arcadia

This position is part of the Arcadia Data Engineering team, we are responsible for the onboarding, enhancement, and support of data feed integrations between Client Claim and Clinical data mgmt. platforms and our Healthcare Solution Platform. Our customers are top Healthcare providers and payers, and we help them integrate their internal systems with our analytic platform. The Data Engineering team is responsible for the data architecture that drives the partnership with customers and other internal organizations to drive success through adoption of cutting edge analytic solutions that leverage new age technologies and best practices. Our Data Engineers require both SQL Database knowledge and design , along with multiple programing languages .

As a Data Engineer, you will drive the successful development of solution architecture and the completion of data pipeline connectors that automate the flow of data between client Claim and Clinical data platforms and our analytic health solution platform. Your efforts will be critical to driving the long-term partnership between Arcadia and our customers.

What Success Looks Like

In 3 months
- Learn the different areas of the data connector life cycle, while having a working knowledge of the technical stacks , storage platforms , data models , and Dev. Cycle
- Work within Data Engineering Scrum team
- Set to work on new ingestion pipelines with full bandwidth available as formal training will end

In 6 months
- Properly contribute to scrum ceremonies and ceremonies within the dev cycles while successfully updating status and progress in Jira  
- Work on higher level enhancement requests and ingestion pipelines
- Ability to Deliver Data related Reviews to clients and other departments regarding code quality and test cases.
- Set your own personal vision of development and career aspirations and set a working path forward with leadership to work on how we can help you attain those goals   

In 12 months 
- Developing a range of data pipelines with varying complexity
- Work with Product, Engineering or Implementation to build out tools for better data integration
- Pick an SME Subject Matter Expert path for what excites you the most
- Working on standardized data connector development
What Youll Be Doing

Design and documentation of connectors  ingestion pipelines
Build and Unit testing of delivery connectors  ingestion pipelines
Support of our processes in partaking in peer code reviews , sprint planning , product grooming , maintaining Jira tasks and peer test reviews
You will be expected to contribute to multiple implementations simultaneously, which will include both new customer setup as well as support and enhancements for existing customers.
The expectations of the day to day of an engineer is as follows

Delivery
Responsible for delivery of work on expected timelines.
Able to identify risk to project success and communicate to leadership
Works mostly independently on delivery wdecreasing involvement from engineering and more senior team members
Consistently deliver increasing connectors of increasing quality with ""lessons learned"" incorporated into next project
Able to apply critical thinking and problem solving skills to propose solutions for complex problems within day to day work

 Technical Domain Knowledge

Working and growing knowledge of new tech stack with less focus on finding efficiency in the technology and greater focus on understanding use of it.
Developing ability to understand technical issues and communicate potential solutions to team members or engineering team

Business Domain Knowledge
Developing working knowledge of the business of healthcare data and how it interacts within the Arcadia products
Understanding of shared value contracts that our customers are in and how data is impacted by them
Developing knowledge of industry data expected values such as PMPM by LOBs, MM trends, etc.

Communication Skills
Developing internal and external professional communication skills including presentation of issues using appropriate industry vocabulary

Team Projects
Responsible for contributing to the advancement of team processes and internal

What Youll Bring

Experience Level 2-5 years post-grad with relevant industry experience or graduate level Degree.

TECH
As a data engineer you will be expected to problem solve some basic coding issues and enhancements with frameworks that are built in Spark Scala, while also leveraging technical skills to partake in idea sessions on process improvement and POC design of how to carry out a solution. 
 
SQL 2-4 year Preferred
Spark 1-2 years Preferred
NoSQL Databases 1-2 years Preferred
Database Architecture 2-3 years Preferred
Cloud Architecture 1-2 years Preferred

DATA
As a data engineer you will be expected to problem solve some basic data analysis issues and work the data to create analytic enhancements.
Healthcare Data 2-4 years Preferred
Healthcare Analytics 1-3 years Preferred

What Youll Get

Chance to be surrounded by a team of extremely talented and dedicated individuals driven to succeed
Be a part of a mission driven company that is transforming the healthcare industry by changing the way patients receive care
A flexible, remote friendly company with personality and heart
Employee driven programs and initiatives for personal and professional development
Be a member of the Arcadian and Barkadian Community


About Arcadia

Arcadia.io helps innovative healthcare systems and health plans around the country transform healthcare to reduce cost while improving patient health.   We do this by aggregating massive amounts of clinical and claims data, applying algorithms to identify opportunities to provide better patient care, and making those opportunities actionable by physicians at the point of care in near-real time.  We are passionate about helping our customers drive meaningful outcomes. We are growing fast and have emerged as the market leader in the highly competitive population health management software and value-based care services markets, and we have been recognized by industry analysts KLAS, IDC, Forrester and Chilmark for our leadership. For a better sense of our brand and products, please explore our website, our online resources, and our interactive Data Gallery.

This position is responsible for following all Security policies and procedures in order to protect all PHI under Arcadias custodianship as well as Arcadia Intellectual Properties.  For any security-specific roles, the responsibilities would be further defined by the hiring manager.

"
https://startup.jobs/data-engineer-dev-ops-inizio-advisory-4361740,"DevOps,Engineer",Data Engineer - Dev Ops,Inizio Advisory ,"Gurugram, India",,"

DevOps engineer skills and responsibilities
 

At least 5-7 years of experience in designing, deploying, and managing AWS infrastructure.
Experience with infrastructure as code tools such as Terraform, Ansible, and CloudFormation.
Hands-on experience with containerization technologies such as Docker and Kubernetes.
Experience with CICD tools such as Jenkins, GitLab CICD, and AWS CodePipeline.
Experience with serverless technologies such as AWS Lambda and AWS API Gateway.
Familiarity with logging and monitoring tools such as CloudWatch, ELK stack, and Prometheus.
Troubleshoot and resolve issues related to infrastructure, applications, and networks.
Work with development teams to ensure that applications are designed and deployed in accordance with DevOps principles.
Experience with Git and version control systems.
Hands-on experience in programming languages such as Python, Bash, and JavaScript.
Excellent communication and collaboration skills.

 
Preferred Qualifications Not mandatory
AWS certifications such as AWS Certified DevOps Engineer, AWS Certified Solutions Architect, and AWS Certified Developer.

"
https://startup.jobs/data-engineer-hardware-reliability-starlink-product-spacex-4360872,"Engineer,Sys Admin","Data Engineer, Hardware Reliability (Starlink Product)",SpaceX ,"Hawthorne, United States",,"

SpaceX was founded under the belief that a future where humanity is out exploring the stars is fundamentally more exciting than one where we are not. Today SpaceX is actively developing the technologies to make this possible, with the ultimate goal of enabling human life on Mars.
DATA ENGINEER, HARDWARE RELIABILITY STARLINK PRODUCT
At SpaceX were leveraging our experience in building rockets and spacecraft to deploy Starlink, the worlds most advanced broadband internet system. Starlink is the worlds largest satellite constellation and is providing fast, reliable internet to 1M users worldwide. We design, build, test, and operate all parts of the system  thousands of satellites, consumer receivers that allow users to connect within minutes of unboxing, and the software that brings it all together. Weve only begun to scratch the surface of Starlinks potential global impact and are looking for best-in-class engineers to help maximize Starlinks utility for communities and businesses around the globe.
As a Data Engineer, you will be responsible for developing the strategy, key metrics, tools, software services and processes for assessing how well key aspects of Starlink are scaling and the effectiveness of the Starlink Network in serving millions of users around the globe. You will work with operators, subsystem responsible engineers, software engineers, and network engineers inside the Starlink organization as well as key contacts with various major external partners to help ensure the growth of this program.
RESPONSIBILITIES

Build and maintain mission-critical infrastructure, tools, processes, and custom software to objectively assess growth areas for the Starlink program
Automate the aggregation of metrics and detection of widespread application issues across Starlink

Establish and maintain relationship with key third party applicationcontent owners


Lead technical investigations about chronic application-level issues


Build ground-based software systems that ingest, transform, and store data 


Apply data analytics, models, and techniques to data products created by space vehicles  


Create catalogs of data and tools that can be used by you and other teams to perform analytics  


Fuse data from multiple sources to create usable information 


BASIC QUALIFICATIONS

Bachelors degree in computer science, data science, physics, mathematics, or a STEM discipline; OR 2 years of professional experience in data engineering in lieu of a degree
Development experience in an object-oriented programming language i.e. C, C, Python

PREFERRED SKILLS AND EXPERIENCE

Professional experience in analytics, data science, or machine learning 
Experience using Spark, Presto, Flink, or Snowflake 
Experience building solutions with Parquet, or similar storage formats 
Knowledge of Kubernetes 
Experience building solutions with in-stream data processing of structured and semi-structured data   
Experience building predictive models and machine learning pipelines clustering analysis, prediction, anomaly detection  
Experience in custom ETL design, implementation and maintenance 
Experience handling large TB datasets 
Experience with developing and deploying tools used for data analysis
Ability to work effectively in a dynamic environment that includes working with changing needs and requirements
Ability to take on projects that require taking initiative and developing new expertise

ADDITIONAL REQUIREMENTS

Must be willing to work extended hours and weekends as needed
Ability to pass Air Force background checks for Cape Canaveral and Vandenberg 

COMPENSATION AND BENEFITS    
Pay range    Data EngineerLevel I 120,000.00 - 145,000.00per year    Data EngineerLevel II 140,000.00 - 170,000.00per year        Your actual level and base salary will be determined on a case-by-case basis and may vary based on the following considerations job-related knowledge and skills, education, and experience.
Base salary is just one part of your total rewards package at SpaceX. You may also be eligible for long-term incentives, in the form of company stock, stock options, or long-term cash awards, as well as potential discretionary bonuses and the ability to purchase additional stock at a discount through an Employee Stock Purchase Plan. You will also receive access to comprehensive medical, vision, and dental coverage, access to a 401k-retirement plan, short  long-term disability insurance, life insurance, paid parental leave, and various other discounts and perks. You may also accrue 3 weeks of paid vacation  will be eligible for 10 or more paid holidays per year. Exempt employees are eligible for 5 days of sick leave per year.
ITAR REQUIREMENTS

To conform to U.S. Government space technology export regulations, including the International Traffic in Arms Regulations ITAR you must be a U.S. citizen, lawful permanent resident of the U.S., protected individual as defined by 8 U.S.C. 1324ba3, or eligible to obtain the required authorizations from the U.S. Department of State. Learn more about the ITAR here.

SpaceX is an Equal Opportunity Employer; employment with SpaceX is governed on the basis of merit, competence and qualifications and will not be influenced in any manner by race, color, religion, gender, national originethnicity, veteran status, disability status, age, sexual orientation, gender identity, marital status, mental or physical disability or any other legally protected status.
Applicants wishing to view a copy of SpaceXs Affirmative Action Plan for veterans and individuals with disabilities, or applicants requiring reasonable accommodation to the applicationinterview process should notify the Human Resources Department at 310 363-6000. 

"
https://startup.jobs/data-engineer-hakkoda-4356067,Engineer,Data Engineer,Hakkoda ,,Full-Time,"


ABOUT HAKKODA 
Hakkoda is a modern data consultancy that empowers data driven organizations to realize the full value of the Snowflake Data Cloud. We provide consulting and managed services in data architecture, data engineering, analytics and data science. We are renowned for bringing our clients deep expertise, being easy to work with, and being  an amazing place to work! We are looking for curious and creative individuals who want to be part of a fast-paced, dynamic environment, where everyones input and efforts are valued. We hire outstanding individuals and give them the opportunity to thrive in a collaborative atmosphere that values learning, growth, and hard work. Our team is distributed across North America, Latin America, and Europe. If you have the desire to be a part of an exciting, challenging, and rapidly-growing Snowflake consulting services company, and if you are passionate about making a difference in this world, we would love to talk to you!.

ROLE DESCRIPTION
A Hakkoda Data Engineer will work in the design and development of Snowflake Data Cloud solutions. This work includes data ingestion pipelines, data architecture, data governance and security.  

WHAT WE ARE LOOKING FOR

We are looking for a savvy Data Engineer to join our growing team of experts. This position will work in the design and development of Snowflake Data Cloud solutions. The work includes data ingestion pipelines, data architecture, data governance and security.  The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing customer projects. This role will require you to be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of working for a start-up company to support our customers next generation of data initiatives.

QUALIFICATIONS AND EXPERIENCE

Bachelors degree in engineering, computer science or equivalent area
3yrs in related technical roles with experience in data management, database development, ETL, andor data prep domains.
Experience developing data warehouses
Experience building ETL  ELT ingestion pipelines.
Knowledge of how to manipulate, process and extract value from large disconnected datasets.
SQL and Python scripting experience require, Scala and Javascript is a plus.
Cloud experience AWS, Azure or GCP is a plus.
Knowledge of any of the following tools is also a plus Snowflake, MatillionFivetran or DBT.
Strong interpersonal skills including assertiveness and ability to build strong client relationships.
Strong project management and organizational skills.
Ability to support and work with cross-functional and agile teams in a dynamic environment.
Advanced English required.


Hakkoda is an exciting, high growth company, and were scaling our team. We are looking for exceptional people who share our values, challenge ordinary thinking, and push the pace of innovation while building a future for themselves and Hakkoda. We are a collaborative team of high achievers. We love to explore, challenge and have a lot of fun along the way. Are you ready for the adventure? Click here to see our culture on display.

Benefits Include
- Medical, Dental, Vision
- Life Insurance
- Stock options upon hire
- Paid parental leave
- Flexible time off
- Work from home benefits
- Technical training and certifications



"
https://startup.jobs/data-engineer-rideco-4355545,Engineer,Data Engineer,RideCo ,"Waterloo, Canada",Full-Time,"

This is an opportunity in the exciting and fast-growing transportation technology industry. Public transit is being transformed from a system of static, scheduled fixed-routes, to a dynamic on-demand network, and youll be one of the pioneers shaping this transformation. 
Data is at the heart of RideCos core offering, and drives the decisions we make in both our product and the services offered to our clients. As a Data Engineer, you will be responsible for building tools and processes to collect, transform, and generate insights and reports on the data needed to make these decisions. As a key technical and analytics resource for Project and Operations managers, youll have a vital role in shaping the design, performance, and success of our clients next-generation on-demand transportation services. 
Responsibilities

Build and maintain end-to-end data pipelines understand source data and end reporting needs in order to design, schedule, and maintain ETL processes that feed into our reports and dashboards. Work with engineering teams to update pipelines as upstream data changes; communicate the effects of these changes with downstream data consumers and update all downstream reporting objects to prevent broken dashboards and reports.
Generate and analyze operational performance reports build comprehensive performance analysis reports spanning rider user-experience, driver user-experience, marketing, operating efficiency, and unit economics. Work with operations and project managers to transform business requirements into concrete insights.
Improve internal processes and tooling identify ways to further systematize and scale our internal processes, including with new analysis, visualization, and ETL tools. Share your ideas with team members and collaboratively champion internal processtooling improvements.
Drive continuous improvement collaboratively develop weeklymonthly actions to drive continuous improvements, and to achieve the companys data objectives e.g. build new reports, database maintenance, etc.. Work with the operations managers, and other stakeholders to execute on initiatives.
Automate identify areas of automation within the organization. Champion efforts to streamline processes and improve team productivity.
Champion data quality, security, and governance educate data consumers on how to use data sets appropriately; design new self-serve data tools that promote data democracy while ensuring governance; advocate for improving data quality in our source systems.

Your playground  what youll learn
At RideCo youll get a chance to play, learn and build with the following tools and technologies, and as part of a cross-functional team that is the worlds foremost innovator in on-demand transit software.

PostgreSQL
Python
Tableau
Microsoft Excel
Building schemas, data normalization, writing optimal SQL queries, debugging existing queries
Development Processes Agile, continuous integration, Jenkins

Required Qualifications and Experience

3-5 years of experience in data engineering,  analytics engineering, or related roles
A deep and intuitive understanding of databases and SQL
Experience developing in Python
Experience building ETL pipelines
Experience working with Tableau or other dashboarding tools
EducationUndergraduate degree in Engineering  Data Science  Computer Science  Applied Mathematics  Statistics, or related discipline 

Compensation and Benefits

Base Salary 70K - 95K  performance-based bonus  stock options
Work-Life Balance  Additional Perks Flex-time work schedules, vacation time, catered lunches, social events, casual dress code
Benefits Plan Medical, dental, prescription, lifehealth spending accounts and more
Work Environment Located in KWs most desirable work space in the heart of Uptown Waterloo
Commuter Program Complimentary rides to and from work 

Who we are 
httpwww.rideco.com
RideCo powers on-demand transit. Public transit agencies use RideCos cloud-based software platform to provide on-demand shared rides in dynamically routed buses and vans. Our clients include some of the worlds largest transportation operators such as Los Angeles Metro, San Antonio Metro, and Calgary Transit.Have you experienced getting frustrated with transfers and waiting while taking a public bus? Have you seen buses drive around in low-density areas with very few passengers on-board and wondered how inefficient that seems to be? Youre likely aware of the first  last mile access challenges faced by transit hubs. We are solving these problems by re-imagining shared mobility. Imagine a world where vehicles have dynamic routes responsive to real-time trip demand. This dynamic shuttle or van sedan would pick you up, on-demand, at or near your doorstep, and take you to your destination or transit hub. Along the way, it may pick-up other passengers going in your direction. Your experience will be seamless less waiting, less walking, fewer transfers, shorter travel time, and timely pickups and drop-offs. RideCos dynamic shuttle platform enables this seamless experience and low-cost shared rides for vehicle fleet operators and their passengers. By seamlessly moving more people in fewer vehicles we are catalyzing a generational shift in how people get around cities and towns. This means commuters spend less time in transit and more time doing what they enjoy.
RideCo powers a diverse range of use cases, including residential suburban travel; first-mile-last-mile connections for transit hubs; and corporate employee transportation. We are investing to scale up and capture the growing demand for on-demand shared rides solutions. 
RideCo is proud to be an equal opportunity employer, we are committed to building and supporting a culture of diversity, inclusion, and accessibility. We hire the best talent regardless of race, color, creed, national origin, ancestry, disability, marital status, age, veteran status, sex, sexual orientation, gender identity, and expression.  Building a team that represents a variety of backgrounds, perspectives, and skills benefits our employees, our customers, our products, and our community.  
In accordance with the Accessibility for Ontarians with Disabilities Act, accommodations are available upon request for candidates taking part in all aspects of the selection process. If you require special accommodation to complete any portion of the application or interview process, please contact peoplerideco.com.

"
https://startup.jobs/data-engineer-wahed-4353948,Engineer,Data Engineer,Wahed ,,Contractor,"

About Wahed

We have a passion to reduce financial inequality and exclusion by building world leading financial products and services aimed at giving access to all. We are a New York headquartered Financial Technology FinTech company focused on serving values-based shariah compliant digital financial services to retail clients globally, starting with wealth management. We have an impressive global team aligned with this purpose and are looking for trail blazers in their fields that will take our customer delivery to new levels. We can promise you a digital first and truly international culture, as well as a fascinating immersion into the world of FinTech and Islamic finance.


Job Brief

We are looking for savvy Data Engineers to join our growing team of analytics experts. They will be responsible for building, expanding and optimizing our data systems and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams and various business departments. They need to be experienced data system builders, have worked on 100s of ETL jobs and be data wranglers who enjoy optimizing data systems and building them from the ground up. They will also ensure optimal data delivery architecture is consistent throughout ongoing projects. We are looking for candidates that are self-motivated and eager to learn new technologies, work well both within a team and on their own, and have an attention to detail.
Job Description

Build and maintain a centralised data system consisting of an optimally built data pipeline architecture, data warehouse, data lake and data hubs
Assemble large, complex data sets that meet functional and non-functional business requirements
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Events and AWS big data technologies but not limited to AWS
Build analytics tools and reports that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Work with various stakeholders and teams at Wahed to assist with data-related technical issues and support their data infrastructure needs
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions
Create data tools for Product team members that assist them in building and optimizing our product into an innovative industry leader

Job Specification

At least 3 to 5 years of Data Engineering experience
A degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field
Strong Python coding skills
Advanced SQL skills
Experience with big data data systems, pipelines, architectures, data sets and ETL tools
Comfortable performing root cause analysis to answer specific business data questions and identify opportunities for improvement
Comfortable working in a Linux environment
Familiar with AWS
Familiar with noSQL databases such as Elasticsearch and event-streaming platforms such as Kafka
Strong analytical skills related to working with complex and non-structured datasets.
Experience in creating and improving processes that support data transformation, data structures, metadata, dependency and workload management
Strong communicator, project management and organizational skills
Experience supporting and working with cross-functional teams in a dynamic environment.
Understand the trade-offs in employing different engineering solutions to a problem, valuing pragmatism over idealism
Eager and open to learning



"
https://startup.jobs/data-engineer-full-remote-cortilia-4353551,Engineer,Data Engineer- Full remote,Cortilia ,"Milan, Italy",Full-Time,"

Are you looking for a new challenging work experience in a Certified BCorp company that is careful to Innovation, Sustainability and People? Do you want to test your skills in a young and dynamic work environment? Are you passionate about food, technology and wishing to make a difference through your job every day?So this job fits you!Cortilia Spa Benefit Corporation is an innovative e-commerce platform that offers a user-friendly customer experience thanks to advanced services. Our business is in constant growth and we have a touchable outlook for the future.We are looking for a highly talented Data Engineer who will help us to achieve goals.
As a Data Engineer your main responsibilities will be
 Design, write, deploy and monitor data pipeline code; Data quality build and maintain infrastructure to ensure data integrity; Enrich corporate data lake with raw information from different sources; Evaluate business needs and objectives developing models that meet the desired standards and requirements; Organize data systems, finding ways to optimize data query and data organization to avoid garbage dump. 

Find out more about us visiting httpsabout.cortilia.it and our Linkedin page.And if you still dont have a Cortilia account, download the app, sign up and checkout with the promo code PEOPLE to get 10 off your first order!
Requirements
 Strong passion for data driven culture and all related aspects; At least 5 years of working experience in SQL; At least 3 years of working experience in ETL  ELT processes; At least 3 years of working experience in relational Database; At least 3 years of working experience with Python and Pyspark in particular; At least 3 years of working experience with Data Lake  Data Warehouse and related tools AWS Glue, AWS Athena, AWS S3 and Deltalake knowledgeexperience with DBT, Airflow and Databricks is not a must but appreciated; At least 3 years of working experience in BigData and Data Event Driven architecture and AWS Kinesis; Working experience in Agile teams will be considered a plus; Knowledge of DevOps practices applied to Data DataOps, MLOps, MLFlow a practical experience will be considered a plus; Knowledge of containerized tools like Docker and Kubernetes will be considered a plus; BS or MS degree in computer science, computer engineering, or other technical discipline; 

What we expect from you
 That you have a goal-oriented mindset always open to improve yourself and people around you with proper and on time feedback. That you have enough curiosity and creativity to innovate existing things and create new ones; Ability to work in a team offering collaboration, excellent communication skills and reliability; That youre a good learner always open to adopt new technologies, tools and methodologies. Good written english comprehension and production is a must your technical documents and code will need to be written in English; During the interview, youll be asked to write python code and SQL queries. Good written english comprehension and production your technical documents and code will need to be written in English; Complete fluency in Italian and English. 
Benefits
 A dynamic and young work environment Fruits and coffee in Cortilia office  Employee discount on www.cortilia.it and www.discovery.cortilia.it  Welfare program Smartworking or Remote Working from Italy  Work with cutting edge technologies 
Recruitment Process
 A quick chat with one of our Talent Acquisition team members to understand your motivation The first interview with the Hiring Manager to deep dive into your experiences A case study to test your hard skills A Meet The Team session where youll get to meet your future colleagues 
Cortilia S.p.A. Societ Benefit supports equal opportunities and ensures a selection process that complies with Diversity, Equity and Inclusion. Cortilia S.p.A. Societ Benefit appreciates and values every person, and condemns any form of discrimination, direct or indirect, made on the basis of nationality, ethnicity, gender, sexual orientation, pregnancy, religion, disability, age and any other category protected by law.

"
https://startup.jobs/data-engineer-bi-engineer-devsu-4351762,Engineer,Data Engineer / BI Engineer,Devsu ,,Full-Time,"

We are looking for a talented and self-motivated Business Intelligence BIData Engineer to work with our client.This role will be responsible for designing and building data platforms and enrichment pipelines that enable data democratization for the game studio. The ideal candidate will possess strong statistical and analytical skills with SQL, as well as experience in data modeling, ETL processes, big data, and data warehousing.

Responsibilities
 Collaborate with game team analysts and Product Managers to plan, design, and build data platforms and enrichment pipelines that power insights to stakeholders. Translate analytical requirements into data solution designs, then carry those designs through implementation into production. Develop and maintain robust and scalable data designspipelines that can be easily ingested by our BI tool Tableau to enable data democratization. Implement strategies to improve data modeling and information design, guide analysts on creating scalable end-to-end data solutions, and promote good governance as a data citizen. Monitor ETL processes, conduct query testing, create dashboards, and generate reports. Develop a deep understanding of the underlying data assets and data products and provide subject matter expertise on content, current, and potential future uses of data. Use data to identify and drive key business decisions, present actionable insights to improve product feature design and user experiences, and provide recommendations for product teams. Use exploratory data analysis techniques to identify meaningful relationships, patterns, or trends from complex data sets and validate your results. Translate problems into structured analyses or data products that directly create applicable insights that maximize value. Ensure a high degree of thoughtfulness and quality in all projects and ensure that processes are adopted and best practices are shared. 
Requirements
 Bachelors degree in Economics, Mathematics, Statistics, or a related field. Strong English Skills B2 or higher 2 years of relevant industry experience in a BI-related role. High proficiency in writing SQL queries and knowledge of cloud-based databases like Snowflake, Redshift, BigQuery, or other big data solutions. Experience in data modeling, ETL processes, big data, and data warehousing. Experience with business intelligence tools such as Tableau, or similar BI tools. Strong technical understanding of data modeling, design, and architecture principles and techniques across master data, transaction data, and derivedanalytic data. Demonstrated ability to work cross-functionally and build relationships with both technical and business partners. Basic knowledge in scripting with R andor Python. Experience analyzing large web-scale, complex, multi-dimensional data sets. Experience presenting to people at different levels of business and technical expertise, including the ability to simplify difficult technical concepts. Excellent communication and interpersonal skills 
Benefits
A stable, long-term contract. Continuous Training. Private Health insurance. Flexible schedule. Work with some of the most talented software engineers in Latin America and the US, doing challenging work and world-class software for clients in the US and around the world.

"
https://startup.jobs/data-engineer-sporty-group-4351191,Engineer,Data Engineer,Sporty Group ,,Full-Time,"

Sportys sites are some of the most popular on the internet, consistently staying in Alexas list of top websites for the countries they operate in

As a Data Engineer at Sporty, you will play a critical role in ensuring the smooth processing and handling of data for our machine learning and data science initiatives. Your primary responsibilities will include designing, building, testing, optimising, and maintaining data pipelines and architectures for various aspects of our rapidly growing business.

Who We Are

Sporty Group is a consumer internet and technology business with an unrivalled sports media, gaming, social and fintech platform which serves millions of daily active users across the globe via technology and operations hubs across more than 10 countries and 3 continents.
The recipe for our success is to discover intelligent and energetic people, who are passionate about our products and serving our users, and attract and retain them with a dynamic and flexible work life which empowers them to create value and rewards them generously based upon their contribution.
We have already built a capable and proven team of 300 high achievers from a diverse set of backgrounds  and we are looking for more talented individuals to drive further growth and contribute to the innovation, creativity and hard work that currently serves our users further via their grit and innovation.

Responsibilities

Design, develop and maintain scalable batch ETL and near-real-time data pipelines and architectures for various parts of our business, on fast and versatile data sources with millions of changes per day
Ensure all data provided is of the highest quality, accuracy, and consistency
Identify, design, and implement internal process improvements for optimising data delivery and re-designing infrastructure for greater scalability
Builds out new API integrations to support continuing increases in data volume and complexity
Communicate with data scientist, MLOps engineers, product owners and BI analysts in order to understand business processes and system architecture for specific product features

Requirements

Bachelors degree, or equivalent experience, in Computer Science, Engineering, Mathematics, or a related technical field
3 years of experience in data engineering, data platforms, BI or related domain
Experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
Experience with large-scale production relational and NoSQL databases
Experience with data modelling
General understanding of data architectures and event-driven architectures
Proficient in SQL
Familiarity with one scripting language, preferably Python
Experience with Apache Airflow
Solid understanding of cloud data services AWS services such as S3, Athena, EC2, RedShift, EMR Elastic MapReduce, EKS, RDS Relational Database Services and Lambda

Nice to have

Apache Spark
Understanding of containerisation and orchestration technologies like DockerKubernetes
Relevant knowledge or experience in the gaming industry

Benefits

Quarterly and flash bonuses
Flexible working hours
Education allowance
Referral bonuses
28 days paid annual leave
2 x annual company retreats Lisbon  Dubai in 2022  Phuket in Q2 2023  1 more TBC!
Highly talented, dependable co-workers in a global, multicultural organisation
Payment via world class online wallet system DEEL
Top of the line equipment supplied by market leader Hofy
We score 100 on The Joel Test 
Our teams are small enough for you to be impactful
Our business is globally established and successful, offering stability and security to our Team Members

Our Mission

Our mission is to be an everyday entertainment platform for everyone

Our Operating Principles

1. Create Value for Users
2. Act in the Long-Term Interests of Sporty 
3. Focus on Product Improvements  Innovation 
4. Be Responsible 
5. Preserve Integrity  Honesty 
6. Respect Confidentiality  Privacy 
7. Ensure Stability, Security  Scalability 
8. Work Hard with Passion  Pride

Interview Process

HackerRank Test 
Remote video screening with our Talent Acquisition Team  live ID check 
Remote 90 min video interview loop with 3 x Team Members 30 mins each 
Pre offer call with Talent Acquisition Team
ID check via Zinc 
24-72 hour feedback loops throughout process 

Working at Sporty

The top-down mentality at Sporty is high performance based, meaning we trust you to do your job with an emphasis on support to help you achieve, grow and de-block any issues when theyre in your way.
Generally employees can choose their own hours, as long as they are collaborating and doing stand-ups etc. The emphasis is really on results. 

As we are a highly structured and established company we are able to offer the security and support of a global business with the allure of a startup environment. Sporty is independently managed and financed, meaning we dont have arbitrary shareholder or VC targets to cater to. 

We literally build, spend and make decisions based on the ethos of building THE best platform of its kind. We are truly a tech company to the core and take excellent care of our Team Members.

"
https://startup.jobs/data-engineer-databricks-2-4349189,Engineer,Data Engineer,Databricks ,,,"

As an Data Engineer  Solutions Engineer in our Professional Services team you will work with clients on short to medium term customer engagements on their big data challenges using the Databricks platform. You will provide data engineering, data science, and cloud technology projects which require integrating with client systems, training, and other technical tasks to help customers to get most value out of their data. RSAs are billable and know how to complete projects according to specification with excellent customer service.
The impact you will have

You will work on a variety of impactful customer technical projects which may include building reference architectures, how-tos and production grade MVPs
Guide strategic customers as they implement transformational big data projects, 3rd party migrations, including end-to-end design, build and deployment of industry-leading big data and AI applications
Consult on architecture and design; bootstrap or implement strategic customer projects which leads to a customers successful understanding, evaluation and adoption of Databricks.
You will work with the Databricks technical team, Project Manager, Architect and Customer team to ensure the technical components of the engagement are delivered to meet customers needs.
Work with Engineering and Databricks Customer Support to provide product and implementation feedback and to guide rapid resolution for engagement specific product and support issues.

What we look for


4 years experience with Big Data Technologies such as Spark, Kafka, Cloud Native and Data Lakes in a customer-facing post-sales, technical architecture or consulting role.

Experience working in the Databricks ecosystem

Comfortable writing code in either Python or Scala.


Experience working across Cloud Platforms GCP  AWS  Azure


Experience working with clients and managing conflicts.

Build skills in technical areas which support the deployment and integration of Databricks-based solutions to complete customer projects.

Benefits 

Group personal pension with company contribution
Equity awards
Paid parental leave
Gym reimbursement
Annual personal development fund
Work headphones reimbursement
Business travel accident insurance


About Databricks
Databricks is the data and AI company. More than 9,000 organizations worldwide  including Comcast, Cond Nast, and over 50 of the Fortune 500  rely on the Databricks Lakehouse Platform to unify their data, analytics and AI. Databricks is headquartered in San Francisco, with offices around the globe. Founded by the original creators of Apache Spark, Delta Lake and MLflow, Databricks is on a mission to help data teams solve the worlds toughest problems. To learn more, follow Databricks on Twitter, LinkedIn and Facebook.
 
Our Commitment to Diversity and Inclusion
At Databricks, we are committed to fostering a diverse and inclusive culture where everyone can excel. We take great care to ensure that our hiring practices are inclusive and meet equal employment opportunity standards. Individuals looking for employment at Databricks are considered without regard to age, color, disability, ethnicity, family or marital status, gender identity or expression, language, national origin, physical and mental ability, political affiliation, race, religion, sexual orientation, socio-economic status, veteran status, and other protected characteristics.
 
Compliance
If access to export-controlled technology or source code is required for performance of job duties, it is within Employers discretion whether to apply for a U.S. government license for such positions, and Employer may decline to proceed with an applicant on this basis alone.


"
https://startup.jobs/data-engineer-backend-software-engineer-intern-contentsquare-4346114,"Backend,Engineer,Developer",Data Engineer / Backend Software Engineer Intern,Contentsquare ,,Internship,"

Contentsquare is a global digital analytics company empowering the brands you interact with every day to build better online experiences for all. Since our founding in France in 2012, we have grown to be a truly global and distributed team  known as the CSquad  representing more than 70 nationalities across the world.

In 2022, we raised 600M in Series F funding and were recognised as a certified Great Place to Work in France, Germany, Israel, US and UK.

As an intern in the Data Engineer team, you will join a team of hard-working and dedicated engineers, crafting, developing and maintaining Contentsquares current and future data architecture. 

We collect several billion events per day and query hundreds of terabytes in real time.

For example, your work may consist of
- Research on different approaches to improve our internal tooling scalability architecture, CPU, memory, network, as a mix of researchPOCimplementation work.
- Run Proof of Concepts to improve our bot detection mechanisms on Contentsquares production data.

Who are you? 
Youre a Data enthusiast, you have experience in at least one backend programming language kudos if Scala, Java or Go is this one. 

You practice on side projects or have proven curiosity about functional programming, Data engineering and databases. Also, if streaming has another meaning than Netflix or Prime Video, and if youre not afraid to ask questions and challenge your peers, this could be an internship for you!

If youd like to know more about us, do not hesitate to check on our YouTube video to see what its like to work at Contentsquare!

Why you should join our RD department? Here is our RD Manifesto

 We write our own story. 

We think for ourselves, keeping an open mind and engaging in constructive criticism. We are transparent in what we do and why we do it. We build and leverage tech expertise to answer business challenges. Learning from all experiences, we deliver continuous improvements in production. We empower all team members to have an end-to-end impact, take initiative, and bring new ideas to life. We stand together and thrive together; team spirit and solidarity matter even more than strong expertise.

 We live a human adventure. 


Why you should join Contentsquare


 Were humans first. We hire dedicated people and provide them with the trust, resources and flexibility to get the job done.
 We invest in our people through career development, mentorship, social events, philanthropic activities, and competitive benefits.
 We are a fast growing company with a track record of success over the past 10 years, yet we operate with the agility of a startup. That means a huge chance to create an immediate and lasting impact.
 Our clients, partners and investors love our industry-leading product.

To keep our employees happy and engaged, we are always assessing the benefitsperks we offer to ensure we are competitive. Here are a few we want to highlight


 Virtual onboarding, Hackathon, and various opportunities to interact with your team and global colleagues both on and offsite each year.
 Work flexibility hybrid and remote work policies.
 Generous paid time-off policy every location is different.
 Immediate eligibility for birthing and non-birthing parental leave.
 Wellbeing allowance.
 Home Office Allowance.
 A Culture Crew in every country to coordinate regular outings such as game nights, movie nights, and happy hours.
 Every full-time employee receives stock options, allowing them to share in the companys success.
 We offer many benefits in various countries -- ask your recruiter for more information.

We are a 2023 Circle Back Initiative Employer  we commit to responding to every applicant


Uniqueness is embedded in our DNA as one of our core values. Even if you dont meet all of the requirements above, we encourage you to apply.

Contentsquare is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected veteran status, age, or any other characteristic protected by law.

Your personal data is used by Contentsquare for recruitment purposes only. Read our Job Candidate Privacy Notice to find out more about data protection at Contentsquare and your rights.



"
https://startup.jobs/data-engineer-dazngroup-4346027,Engineer,Data Engineer,DAZN ,"Hyderabad, India",Full-Time,"

Are you an engineer who loves to make things that just work better? Do you love to work with cutting edge technologies and think about how can this run faster, be deployed quicker or fail less and deliver killer streaming applications that add business value and stick with customers?
 
DAZN is a tech-first sport streaming platform that reaches millions of users every week. We are challenging a traditional industry and giving power back to the fans. Our new Hyderabad tech hub will be the engine that drives us forward to the future. Were pushing boundaries and doing things no-one has done before. Here, you have the opportunity to make your mark and the power to make change happen - to make a difference for our customers. When you join DAZN you will work on projects that impact millions of lives thanks to your critical contributions to our global products
 
This is the perfect place to work if you are passionate about technology and want an opportunity to use your creativity to help grow and scale a global range of IT systems, Infrastructure and IT Services. Our cutting-edge technology allows us to stream sports content to millions of concurrent viewers globally across multiple platforms and devices. DAZNs Cloud based architecture unifies a range of technologies in order to deliver a seamless user experience and support a global user base and company infrastructure.
 
Join us in Indias beautiful City of Pearls and bring your ambition to life.
As our new Data Engineer, youll have the opportunity to

Support building real-time user-facing analytics and data driven operations applications
Be responsible with the rest of the team for the availability, performance, monitoring, emergency response, and capacity planning
Use your love of big data systems, thinking about how to make them run as smoothly and securely as possible, support operational endpoints
Have a strong sense of teamwork and put teams  companys interests first

Youll be set up for success if you have

2 years experience writing clean, robust and testable code, preferably in Typescript
Experience building high performant, low latency and high velocity data pipelines
Working knowledge in AWS services, such as Kinesis, EventBridge, SQS, SNS Topic, S3, Lambda, Kinesis, EKS, Firehose
Experience with infrastructure-as-code preferably Terraform and CICD processes
Comfortable building  maintaining production level data pipelines; streaming or event driven.
Strong analytical and communication skills.

Even better if you have

Exposure to streaming technologies such as Apache Kafka  Google PubSub, Apache Beam, Google Dataflow.
Having worked in an agile environment with scrum  kanban delivery methodologies


At DAZN, we bring ambition to life. We are innovators, game-changers and pioneers. So if you want to push boundaries and make an impact, DAZN is the place to be.
 
As part of our team youll have the opportunity to make your mark and the power to make change happen. Were doing things no-one has done before, giving fans and customers access to sport anytime, anywhere. Were using world-class technology to transform sports and revolutionise the industry and were not going to stop.
 
If youre ambitious, inventive, brave and supportive, then youre the kind of person whos going to enjoy life at DAZN.
 
We are committed to fostering an inclusive environment, both inside and outside of our walls, that values equality and diversity and where everyone can contribute at the highest level and have their voices heard. For us, this means hiring and developing talent across all races, ethnicities, religions, age groups, sexual orientations, gender identities and abilities. We are supported by our talented Employee Resource Group communities proudDAZN, womenDAZN, disabilityDAZN and ParentZONE.
 
If youd like to include a cover letter with your application, please feel free to. Please do not feel you need to apply with a photo or disclose any other information that is not related to your professional experience.
 
Our aim is to make our hiring processes as accessible for everyone as possible, including providing adjustments for interviews where we can.
 
We look forward to hearing from you.

"
https://startup.jobs/data-engineer-first-resonance-4345031,Engineer,Data Engineer,First Resonance ,"Los Angeles, United States",,"


Were seeking a spirited Data Engineer to join our mission of revolutionizing the ion Factory OS for next-gen hardware creators. As a full-time addition at our lively Los Angeles, CA HQ Downtown, youll play a crucial role in our dynamic data team.
Fired up to support eVTOLs, rockets, robots, and autonomous vehicles makers? Our data team is all about backing companies tackling humanitys boldest challenges. Join our diverse squad, renowned for quick learning, sharp thinking, and agile execution.
While spontaneous ping pong duels might occur, our primary focus is on empowering ion customers with cutting-edge data infrastructure. Ready to make an impact on hardware and Industry 4.0? Lets dive in!
Responsibilities  Duties

Design, develop, and maintain scalable ETL processes and data pipelines
Optimize data storage and retrieval using distributed systems and cloud technologies
Collaborate with cross-functional teams to define data requirements and ensure data quality
Monitor, troubleshoot, and resolve data processing issues
Continuously refine and optimize data engineering practices and tools

Minimum Qualifications  Skills

Proficiency in Python or other relevant programming languages for data processing
Strong proficiency in SQL Postgres and Snowflake
Experience in ETL development and tooling DBT preferred
Familiarity with distributed systems and cloud technologies AWS
Solid understanding of data warehousing concepts and best practices
Knowledge of system orchestration tooling Airflow, Luigi, etc.

Preferred Qualifications  Skills

Experience with distributed data storage systems Snowflake, Apache Druid, AWS Redshift
Knowledge of modern data stores and file formats Delta, Iceberg, etc.
Knowledge of data modeling, schema design, and data normalization techniques
Experience with stream processing tools  frameworks Apache Kafka, Debezium, Spark Streaming
Experience with distributed Big Data tools Trino, Apache Spark
Experience with ELK stack for system monitoring and alerts
Exposure to machine learning frameworks and tools TensorFlow, PyTorch
Background in building enterprise applications or working in a manufacturing environment

Benefits  Perks

Health Insurance; medical, vision, dental,  life insurance
Paid Parental Leave
Employee Stock Option Plan
Team outings, group lunches, an open office, happy hours
Paid holidays, sick days
Flexible Fridays and PTO
401K

 
 
First Resonance is an equal opportunity employer dedicated to building an inclusive and diverse workforce. 
JOIN THE TEAM AND INSPIRE THE WORK

First Resonance accelerates the speed and reliability of hardware development for companies manufacturing the next generation of hardware products. This includes electric airplanes, autonomous vehicles, robotics, and more. We are a group of software, hardware, and manufacturing engineers that are bringing the best of modern UX and data science to an industry that has been overly rigid in its innovation. We are removing the barriers preventing radical advancement by providing tools to manufacturing engineers and operators to move information more freely, collaborate with their teams more easily, and use the power of data to predict problems and provide insights that result in better hardware quality and delivery.

"
https://startup.jobs/data-engineer-talos-4341390,Engineer,Data Engineer,Talos ,"New York, United States",,"

Talos Institutional Fabric for the Digital Asset Market
Founded in 2018, Talos provides institutional-grade trading technology for the global digital asset market, powering many of the major players in the crypto ecosystem.  We are a tight-knit but decentralized team of highly-experienced engineers and businesspeople. We are a remote-friendly work environment, with physical hubs in New York, London, Cyprus, and Singapore.

Were Hiring Data Engineer

Talos is hiring a Data Engineer to help us design and build platform infrastructure to process and analyze hundreds of terabytes of market data and client transactions. This platform will be the foundation for external client reports and dashboards, as well as a playground for internal data analysts and algo developers. The ideal candidate will have a strong fundamental knowledge of software engineering and data engineering, and be able to apply their expertise to design and build a modern data processing platform. The platform will be used to analyze algo behavior, including trade information. It will also systematize and expand broader data initiatives, impacting clients execution strategy via meaningful data-driven insights.  
Responsibilities and Duties

Build, implement, and maintain high-quality data pipelines based on data from the Talos platform and external market data sources
Implement reusable frameworks for data processing and data serving to its users 
Analyze data for trends and information most useful to clients; distill data into meaningful decision points for clients
Liaise with teams such as engineering, research, and business development to help set product roadmap priorities

Qualifications

Hands-on experience working with financialtrading data, building production data pipelines with distributed data processing technologies batch or streaming and frameworks such as Spark, Apache Flink, Dataflow, Apache Beam, etc.
Extensive experience programming language e.g. Python, Go, Scala, Java, ability to write in SQL, and passionate about writing clean, maintainable code
Experience with Cloud data warehouse solutions e.g. BigQuery, Snowflake, Redshift, Vertica, Teradata, etc. a plus
Ability to create elegant and intuitive dataset design; advocate for data quality
Curious about the rapidly evolving technologies in this domain; eager to learn and master new tech
Strong verbal and written communication; able to own and deepen direct relationships with our Engineering, Product Management, and business partners
Bachelors degree in Information Systems, Computer Science, or related field
An energetic, creative, and autonomous self-starter
New York preferred

In New York City, the annual base compensation range is 169,000-217,000. This position is eligible for discretionary bonus.

"
https://startup.jobs/data-engineer-ab4649-nisum-4341091,Engineer,Data Engineer - AB4649,Nisum ,"Hyderabad, India",Full-Time,"


Nisum is a leading global digital commerce firm headquartered in California, with services spanning digital strategy and transformation, insights and analytics, blockchain, business agility, and custom software development. Founded in 2000 with the customer-centric motto Building Success Together, Nisum has grown to over 1,800 professionals across the United States, Chile,Colombia, India, Pakistan and Canada. A preferred advisor to leading Fortune 500 brands, Nisum enables clients to achieve direct business growth by building the advanced technology they need to reach end customers in todays world, with immersive and seamless experiences across digital and physical channels.

What Youll Do

Excellent problem-solving skills to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
Proven experience in manipulating, processing, and extracting value from large disconnected datasets.
Supporting and working with cross-functional teams in a dynamic environment.
Advanced SQL knowledge and experience working with relational databases, query authoring SQL, and familiarity with unstructured datasets.
Perform code reviews, ensure code quality and encourage a culture of excellence.
Communicatework effectively in a team environment

What You Know

7-10 years of experience in a Data Engineer role
Experience with Spark, Scala, java, mongoDB
Experience with Azure Databricks
Streaming Kafka Streaming. 
Experience in Hadoop, Spark or PySpark, JavaScalaPython, Azure Databricks, MongoDB, Apache Kafka

Education
Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
Benefits

In addition to competitive salaries and benefits packages, Nisum India offers its employees some unique and fun extras

 Continuous Learning - Year-round training sessions are offered as part of skill enhancement certifications sponsored by the company on an as need basis. We support our team to excel in their field.

Parental Medical Insurance - Nisum believes our team is the heart of our business and we want to make sure to take care of the heart of theirs. We offer opt-in parental medical insurance in addition to our medical benefits.

Activities - From the Nisum Premier Leagues cricket tournaments to hosted Hack-a-thon, Nisum employees can participate in a variety of team building activities such as skits, dances performance in addition to festival celebrations.
Free Meals - Free snacks and dinner is provided on a daily basis, in addition to subsidized lunch


Nisum is an Equal Opportunity Employer and we are proud of our ongoing efforts to foster diversity and inclusion in the workplace.


"
https://startup.jobs/data-engineer-planetlabs-4332899,Engineer,Data Engineer,Planet ,,,"


Welcome to Planet. We believe in using space to help life on Earth.
Planet designs, builds, and operates the largest constellation of imaging satellites in history. This constellation delivers an unprecedented dataset of empirical information via a revolutionary cloud-based platform to authoritative figures in commercial, environmental, and humanitarian sectors. We are both a space company and data company all rolled into one.
Customers and users across the globe use Planets data to develop new technologies, drive revenue, power research, and solve our worlds toughest obstacles.
As we control every component of hardware design, manufacturing, data processing, and software engineering, our office is a truly inspiring mix of experts from a variety of domains.
We have a people-centric approach toward culture and community and we strive to iterate in a way that puts our team members first and prepares our company for growth. Join Planet and be a part of our mission to change the way people see the world.
Planet is a global company with employees working remotely world wide and joining us from offices in San Francisco, Washington DC, Germany, and The Netherlands.

About the Role  
Planet is seeking a Data Engineer to join its cross-functional Data team. The team is responsible for bringing best-in-class Data Products to Planeteers as we look to achieve our North Star over the next decade and enable every Planeteer to measure the impact of their work towards our triple-bottom-line planet, people, profit. This role will be instrumental in designing and delivering Data Warehouses, Self-Service Tooling, Data Ops and Data Platform Solutions for multiple functional areas using modern cloud technologies. You will have the chance to combine a deep knowledge of business and technical mastery to own and deliver the right solution for the right business problem. This position is a hands-on Data Engineering role, with the core focus being on developing and deploying production-grade code, bringing the best of Product Management and DevOps practices to bear. 
Impact Youll Own

Design and implement robust, well-tested data pipelines and products that enable analytics, data science, and machine learning in a cloud environment supporting hundreds of concurrent users
Collaborate with fellow dataanalytics engineers to establish and uphold best practices around our data modeling, CICD, and self-service tooling offerings.  Provide technical design and code reviews for peers within your team and across the broader org. 
Participate in design and architectural review sessions with the data team to ensure we provide accurate, usable data to analytics, data science and business users
Contribute to the planning and execution of the data team roadmap alongside team, stakeholders and other engineers
Keep current on big data, open source and and data visualization technology trends. Evaluate and work on proof-of-concepts to support merit-based recommendations on technologies

What You Bring

3 years of experience with SQL-based ETL development, data warehousing pipeline development and optimization with large data sets
Experience with CICD and source control tooling GitHub, GitLab
Experience with Python, Java, or equivalent scripting language. 
Experience with public cloud reference architectures and services, such as Google Cloud Platform or AWS
Excellent communication and presentation skills and ability to work cross functionally through collaboration with engineering and business partners
Superior organizational and analytical skills with hypothesis-driven problem solving and turning data into actionable insights
Adaptability and openness to changes and challenges, and a self-starter can-do attitude

What Makes You Stand Out

Experience with DBT and GCP especially GBQ, GCS, PubSub
Experience implementing data platform and data development best practices, including solutions for CICD, Infrastructure as Code e.g. Terraform, data access, observability, monitoring, and lineage
Experience working first-hand in analytics and connecting business questions to underlying data products and data models
Experience with geospatial datasets
Experience working closely with Data Science and Machine Learning stakeholders and helping them get large data models productionized

Benefits While Working at Planet

Comprehensive Health Plan
Wellness program and onsite massages in specific offices
Flexible Time Off
Recognition Programs
Commuter Benefits
Learning and Tuition Reimbursement
Parental Leave
Offsites and Happy Hours
Volunteering Benefits

Compensation
The US base salary range for this full-time position at the commencement of employment is 97,600 - 160,000.  Additionally, this role might be eligible for discretionary short-term and long-term incentives bonus and equity. The final salary range is determined by job related experience, skills and location.  The range displays our typical hiring range for new hire salaries in US locations only.  Your recruiter can share more about the specific salary range for your preferred location during the hiring process.
LI-OT1
LI-REMOTE

Why we care so much about Belonging. 
Were dedicated to helping the whole Planet, and to do that we must strive to represent all of it within each of our offices and on all of our teams. Thats why Planet is guided by an ultimate  north star of Belonging, dreaming big as we approach our ongoing work with diversity, equity and inclusion.  If this job intrigues you, but youre thinking you might not have all the qualifications, please... do apply!  At Planet, we are looking for well-rounded people from around the world who can contribute to more ways than just what is listed in this job description.  We dont just fill positions, we aspire to fulfill peoples careers, most excited about folks who are motivated by our underlying humanitarian efforts.  We are a few orbits around the sun before we get to where we want to be, so we hope youre excited to come along for the ride. 
EEO statement
Planet is committed to building a community where everyone belongs and we invite people from all backgrounds to apply. Planet is an equal opportunity employer, and committed to providing employment opportunities regardless of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, pregnancy, childbirth and breastfeeding, age, sexual orientation, military or veteran status, or any other protected classification, in accordance with applicable federal, state, and local laws. Know Your Rights.
Accommodations
Planet is an inclusive community and we know that everyone has their own needs. If you have a disability or special need that requires accommodation during the hiring process, please call Planets front office at 669-214-9404 or contact your recruiter with your request. Your message will be confidential and we will be happy to assist you.
Privacy Policy By clicking ""Apply Now"" at the top of this job posting, I acknowledge that I have read the Planet Data Privacy Notice for California Staff Members and Applicants, and hereby consent to the collection, processing, use, and storage of my personal information as described therein.
Privacy Policy European Applicants By clicking ""Apply Now"" at the top of this job posting, I acknowledge that I have read the Candidate Privacy Notice GDPR Planet Labs Europe, and hereby consent to the collection, processing, use, and storage of my personal information as described therein.


"
https://startup.jobs/data-engineer-pagaya-israel-4331325,Engineer,Data Engineer,Pagaya Israel ,"Tel Aviv-Yafo, Israel",,"

About Pagaya
Shape the Future of FinancePagaya is building a leading artificial intelligence network to help our partners grow their businesses and better serve their customers.
Pagaya powers a leading artificial intelligence network that enables banks, fintechs, merchants, lenders, and other B2C businesses to provide their customers with greater access to financial services. We help partners grow their customer base while managing risk, all with a seamless customer experience.   
Pagayas network enables our partners customers access to credit across Auto, Credit Card, Personal loans, and Point of Sale markets. We are also developing products in insurance, real estate, and more. Our network is fully automated and operating at scale -  with the support of the Pagaya network, our partners have processed millions of applications, with a new application typically analyzed every second. 
Lets create better outcomes together!
About the Role
We are looking for passionate Data Engineer with expertise in cloud technologies, big data and experience working with internal customer-facing business requirements.
This role includes working with Data Science, Analysts, BI and Product departments closely, which requires experience and skill to design and develop main components for our data pipeline.
Responsibilities

Build Data-Infrastructure products to empower engineering teams and data scientists to build production-grade products
Working in a production environment that is getting bigger each day
Leading and commanding innovative new products which can scale and integrate to current best-practices
Working with current-technologies including DBT, Spark, Airflow, SQS, S3, Athena, EventBridge and regularly take them to the edge
Collaborating and working with and exceptionally skilled teams that loves getting things done, including architects and engineers, to create high quality deliverables

Requirements

At least 3 years of experience as a Data Engineer.
Deep understanding of data concepts - modeling, optimizing and scaling - Must.
Experience with at least 3 of these technologies DBT, Snowflake, , Airflow, SQS, S3, Athena, EventBridge, Kinesis.
Excellent coding skills preferably in Python.
Fluent with SQL.
Experience Dealing with high-availability production systems.
Familiarity with Spark - a must.
Cloud AWS, DBT proficiency - an advantage.
Experience working in Fintech industry - an advantage.
An independent, self-learner who is passionate about data, understands business processes and can translate business needs into data models.

Our Team
Pagaya was founded in 2016 by seasoned research, finance, and technology entrepreneurs, and we are now 500 strong in New York, Los Angeles, and Tel Aviv.
We move fast and smart, identifying new opportunities and building end-to-end solutions from AI models and unique data sources. Every Pagaya team member is solving new and exciting challenges every day in a culture based on partnership, collaboration, and community.
Join a team of builders who are working every day to enable better outcomes for our partners and their customers.
Our Values
Our values are at the heart of everything we do. We believe great solutions are built through a great community.


Continuous Learning Its okay to not know something yet, but have the desire to grow and improve.


Win for all We exist to make sure all participants in the system win, which in turn helps Pagaya win.


Debate and commit Share openly, question respectfully, and once a decision is made, commit to it fully.


The Pagaya way Break systems down to their most foundational element, and rebuild them unique to Pagaya.


More than just a job
We believe health, happiness, and productivity go hand-in-hand. Thats why were continually looking to enhance the ways we support you with benefits programs and perks that allow every Pagayan to do the best work of their life. 
Pagaya is an equal opportunity employer. Pagaya is encouraging diversity and actively seeking applicants from all backgrounds, as are committed to creating a diverse workforce together with an inclusive environment for all. Employment is decided on the basis of qualifications, skills, and business needs.

"
https://startup.jobs/data-engineer-investor-vertical-similarweb-4329739,"Engineer,Finance","Data Engineer, Investor Vertical",SimilarWeb ,"New York, United States",,"

Similarweb is the leading digital intelligence platform used by over 4000 global customers. Our wide range of solutions is used throughout companies ranging from Fortune 5 through SMB.
We help our customers succeed in todays digital world through data-focused insights that drive competitive benchmarks, strategic analysis, sales processes, investments, and more.  
In 2021, we went public on the New York Stock Exchange, and we havent stopped growing since!
Were looking for a Data Expert to join our DaaS Data as a Service team. Reporting to our Director of DaaS, the DaaS Data Expert collaborates with our Sales team, providing in-depth data expertise and operational support to win over customers with appropriate use cases.
Why is this role so important at Similarweb?
Youll represent Similarwebs DaaS motion externally and act as our evangelist, through conferences and events, advocating for Similarwebs vast capabilities in the fields of Non-UI data delivery. Youll empower our salesforce to maintain intelligent discussions with data professionals and become the global expert for the DaaS product use cases.
The roles mission will be twofold

Support closing larger, complex deals that require bespoke data engineering primarily with customers within the Investment space.
Help build awareness and demand for our DaaS offerings, in collaboration with our Sales, Marketing, and Product teams.

So, what will you be doing all day?

Design bespoke data products to support complex deals, and navigate cross-functional teams to resolve customer needs during the sales cycle.
Act as the main point of contact for detailed research to help sales identify customer business goals, pains, and technical requirements.
Assemble large, complex sets of data that meet customer needs.
Be the expert on technical data questions from clients and prospects regarding availability, methodology, or use cases of our data.
Conduct internal training, prepare  conduct demos, workshops, and events with clients, and interact with other Similarweb teams to win more business for the DaaS solutions.
Collaborate with Executive, Product, Sales, and DaaS teams to support their data infrastructure needs.
Create strategic and technical sales strategies, presentations, and product demonstrations.
Attend events and outbound marketing activities as the face of SMWB DaaS, including periodical client visits to understand customer pains, and vet solutions further.
Identify data marketplace partnership opportunities non-revenue generating and execute with our partnership team.

This is the perfect job for someone who has

7 years of data scienceengineering experience

Python, SQL, and Excel proficiency - a must; R- a plus 

Over two years of commercial experience, ideally in data deals and solution consulting, sales, or account management experience in B2B DaaS 
Strong communication skills and comfort interacting with clients
Experience leading teams

A plus - experience in data partnerships 

An in-depth understanding of the B2B sales landscape  digital data industry
Experience with Salesforce CRM for forecasting, planning, and management
A self-starter mentality, requiring little direction in a fast-paced environment
A positive and optimistic, can-do attitude 
Public speaking experience, ideally data-related topics

All Similarweb offices work in a hybrid model 3 days in the office, 2 days at home, so you can enjoy the flexibility of working from home with the benefits of building face to face connections with fellow Similarwebbers
The base salary range for this position in New York City is 116,480 to 193,170  benefits including medical, dental and vision insurance, 401K plan, potential equity, employee stock purchase plan and paid sick and parental leave.
Individual compensation is based upon a number of factors, including qualifications and relevant experience.
The base salary range above is for the New York City metro area, and could vary for candidates in other locations.
About the team
The DAAS team is a new motion at Similarweb, it is significantly different from SaaS products and requires to really be a consultant of the prospects, understand the pains and tailor-make the right deal for them. The teams uniqueness comes from its startup mindset and the internal positioning as the most promising growth engine of the company, hence highly in focus. 
Why youll love being a Similarwebber
Youll actually love the product you work with Our customers arent our only raving fans. When we asked our employees why they chose to come work at Similarweb, 99 of them said the product. Imagine how exciting your job is when you get to work with the most powerful digital intelligence platform in the world.
Youll find a home for your big ideas  We encourage an open dialogue and empower employees to bring their ideas to the table. Youll find the resources you need to take initiative and create meaningful change within the organization. 
We offer competitive perks  benefits We take your well-being seriously, and offer competitive compensation packages to all employees. We also put a strong emphasis on community, with regular team outings and happy hours.
You can grow your career in any direction you choose Interested in becoming a VP or want to transition into a different department? Whether its Career Week, personalized coaching, or our ongoing learning solutions, youll find all the tools and opportunities you need to develop your career right here.
Diversity isnt just a buzzword People want to work in a place where they can be themselves. We strive to create a workplace that is reflective of the communities we serve, where everyone is empowered to bring their full, authentic selves to work. We are committed to inclusivity across race, gender, ethnicity, culture, sexual orientation, age, religion, spirituality, identity and experience. We believe our culture of equality and mutual respect also helps us better understand and serve our customers in a world that is becoming more global, more diverse, and more digital every day.
We will handle your application and information related to your application in accordance with the Applicant Privacy Policy available here, as applicable httpswww.similarweb.comcorplegalapplicant-privacy-policies
Please note Were unable to sponsor employment visas at this time.
LI-SS LI-Hybrid

 

We will handle your application and information related to your application in accordance with the Applicant Privacy Policy available here.

"
https://startup.jobs/data-engineer-patreon-4329152,Engineer,Data Engineer,Patreon ,"San Francisco, United States",,"

Patreon is the best place for creators to build memberships by providing exclusive access to their work and a deeper connection with their communities. Were building a content and community platform where creators can engage directly with their fans and monetize their creativity, while maintaining full ownership over the work they make and the communities they create.
Were leaders in the membership space with 250,000 active creators and over 3.5 billion paid directly to creators on our platform. Our team is building tools to optimize the creator-to-fan relationship, including native video, enhanced podcasting features, improved creation tools, and new community experiences. Were continuing to invest heavily in building the most talented team in the Creator Economy and are looking for a Data Engineer to support our mission.
This role is available to those wishing to work in our SF and NY offices on a hybrid work model or those wishing to be fully remote in the United States.
About the Role

Work on a tight-knit team of highly motivated and experienced data engineers with frequent collaboration with data scientists, product managers and product engineers.
Work on both data analytics and data infrastructure type projects in a fast-paced, high-growth startup environment.
Build core data sets and metrics to power analytics, reports and experimentation.
Write real-time and batch data pipelines to support a wide range of projects and features including our creator-facing analytics product, executive reporting, FPA, marketing initiatives, model training, data science analytics, AB testing, etc.
Help manage and build out our data platform and suite of data tools.
Be a driver of a data-centric culture at Patreon. Work autonomously on large green field initiatives and help define data best practices at the company.

About You

Expert in SQL, Spark and Python or Scala
Significant experience modeling data and developing core data sets and metrics to support analytics, reports and experimentation. Solid understanding of how to use data to inform the product roadmap.
Enjoy collaborating with Data Scientists, Product Managers and Product Engineers. Comfortable playing the role of a Project Manager in order to drive results.
Have previously built real-time and batch data pipelines using tooling such as Airflow, Spark, Kafka, S3, Fivetran, Census, etc. Experience working with event tracking frameworks, data observability frameworks and experimentation frameworks.
Experience managing and working with Data Warehouses and Data Lakes such as Redshift, Big Query, Snowflake, Delta Lake, etc.
Highly motivated self-starter that is keen to make an impact and is unafraid of tackling large, complicated problems and putting in the work to ensure high craft deliverables.

About Patreon
Patreon powers creators to do what they love and get paid by the people who love what they do. Our team is passionate about making this mission and our core values come to life every day in our work. Through this work, our Patronauts


Put Creators First  Theyre the reason were here. When creators win, we win.

Build with Craft  We sign our name to every deliverable, just like the creators we serve.

Make it Happen  We dont quit. We learn and deliver.
Win Together  We grow as individuals. We win as a team.

We hire talented and passionate people from different backgrounds across the organization. If youre excited about a role but your past experience doesnt match with every bullet point outlined above, we strongly encourage you to apply anyway. If youre a creator at heart, are energized by our mission, and share our company values, wed love to hear from you.
Patreon is proud to be an equal-opportunity employer. We provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, veteran status, or any other protected class.
Patreon offers a competitive benefits package including and not limited to salary, equity plans, healthcare, unlimited paid time off, company holidays and recharge days, commuter benefits, lifestyle stipends, learning and development stipends, patronage, parental leave, and 401k plan with matching.

The posted range represents the expected salary range for this job requisition and does not include any other potential components of the compensation package, benefits and perks previously outlined. Ultimately, in determining pay, well consider your experience, leveling, location and other job-related factors.
San Francisco Pay Range

139,000193,500 USD



"
https://startup.jobs/data-engineer-inatohealth-4328863,Engineer,Data Engineer,Inato ,"Paris, France",Full-Time,"

WHO WE ARE
Inato is a Tech for Good company striving to bring clinical research to each and every patient, regardless of who they are or where they live. To do this, we are building the worlds first clinical trial platform to create greater visibility, access, and engagement across a more diverse population of doctors and their patients.


Drug development is a challenging, intellectually complex, and rewarding endeavor we enable global pharmaceutical companies to confidently partner with community-based researchers to increase patient access to the latest medical innovations. The platform currently offers clinical trials from leading companies to over 1,000 sites across the globe. And we are well poised for growth in 2023.

We are a growing team of passionate pharmaceutical experts, software engineers, professional services members, and many more - all bringing their unique perspective to solve the challenges facing clinical research. 
Our team members live by our company values to be bold, resilient, caring, and pragmatic. 
If this sounds like you, join us!


The role
We are seeking an experienced Data Engineer to join our fast-growing data team.
In this role, you will play a critical part in helping us build a scalable data infrastructure that supports the data teams mission to provide high-value analysis and tools to Inatos team so they can make faster and better data-driven decisions.

This is an exciting opportunity to make a meaningful impact in a small and dynamic team that is changing the face of the industry.
Our stack includes Google BigQuery, Segment, Airbyte, Looker Studio, Husprey, Retool, dbt, MixPanel, and more.
 Responsibilities

Design, build and maintain our data pipelines and infrastructure to ensure efficient and reliable data ingestion and processing.
Collaborate with the Data and Product team to expose data to the product and implement machine learning models into production.
Monitor and troubleshoot our data infrastructure to ensure maximum uptime and performance.
Work with cross-functional teams to ensure data availability, quality, and consistency across the organization.

 Requirements

3 years of experience in data engineering, with experience in building and maintaining data pipelines at scale.
Strong experience with data processing languages such as Python and SQL.
Experience with database technologies such as PostgreSQL.
Familiarity with cloud-based data infrastructure and storage, preferably GCP.
Experience with ETLELT tools such as Airflow, Segment, Airbyte
Familiarity with infrastructure tools such as Terraform, and Kubernetes.

 Perks 

Remote-first philosophy  flexible hours
Amazing office in Grands Boulevards, Paris where you can meet with colleagues if this is important for you
Top-of-the-line equipments
Compensatory time RTT
Free health insurance Alan Blue, 100 paid by Inato
Meal vouchers Swile
Contribution to healthy activities Gymlib
Free books  learning material


Inato is an Equal Opportunity Employer for any minority, disability, gender identity, or sexual orientation.

"
https://startup.jobs/data-engineer-ii-pax8-2-4328461,Engineer,Data Engineer II,Pax8 ,,,"


Pax8 is the leading value-added cloud-based SaaS distributor, simplifying the cloud journey for our partners by integrating technology, business intelligence and proactive service to deliver an unparalleled experience. Serving thousands of partners through the indirect sales channel, our mission is to be the worlds favorite place to buy cloud products. We are a fast-growing, dynamic and  high-energy startup organization, allowing you to make a meaningful impact on the business. Culture is important to us, and at Pax8, its business, and it IS personal. We are passionate, creative and humorously offbeat. We work hard, keep it fun, and expect the best. 
 
We Elev8 each other. We Advoc8 for our partners. We Innov8 continuously. We Celebr8 life.
No matter who you are, Pax8 is a place you can call home. We know theres no such thing as a perfect"" candidate, so we dont look for the right ""fit""  instead, we look for the add. We encourage you to apply for a role at Pax8 even if you dont meet 100 of the bullet points. We believe in cultivating an environment with a diversity of perspectives, in hopes that we can all thrive in an inclusive environment. 
We are only as great as our people. And we have great people all over the world. No matter where you live and work, youre a part of the Pax8 team. This means embracing hybrid- and remote-work whenever possible.  

Position Summary 
The Data Engineer II designs and develops systems that collect, transform, store, and manage data for end users. They perform coding, debugging, testing, and troubleshooting throughout the development process. The Engineer mentors junior level Engineers. They collaborate with other functional groups to conduct software design reviews.
Essential Responsibilities

Builds pipelines to ingest new data sources
Transforms data to support varied use cases
Optimizes existing data pipelines and improves existing code quality
Includes testing in all aspects of the development process
Mentors junior level Engineers
Analyzes potential problems and finds solutions to pressing data issues
Participates in on-call rotation

Ideal Skills, Experience, and Competencies

At least two 2 to six 6 years of relevant data engineering experience
Intermediate experience with Python
Advanced experience with SQL
Experience with Data Modeling
Moderate experience with a JVM language
Intermediate experience with Apache Spark or other distributed processing engines
Intermediate experience with Apache Kafka or other stream processing frameworks
Moderate experience with Terraform, Docker, Kubernetes, or other similar infrastructure tooling
Intermediate experience with cloud data tools such as S3, Glue, and Athena
Exposure to building CICD pipelines
Effective problem solving and troubleshooting abilities
Ability to consistently achieve results, even under tough circumstances

Required Education  Certifications

B.A.B.S. in related field or equivalent work experience

Compensation



Qualified candidates can expect a salary beginning at 115,000 or more depending on experience



LI-Remote LI-JF1 dice-J BI-Remote
 

Note Compensation is benchmarked on local Denver Metro area market rates. Qualified candidates in other locations can expect a salary package that may be adjusted based off applicable cost of wages in their respective location.

At Pax8 we believe that your Total Rewards should include a benefits package that shows how much we value our greatest assets. All FTE Pax8 people enjoy the following benefits

Non-Commissioned Bonus Plans or Variable Commission
401k plan with employer match
Medical, Dental  Vision Insurance
Employee Assistance Program
Employer Paid Short  Long Term Disability, Life and ADD Insurance
Flexible, Open Vacation
Paid Sick Time Off
Extended Leave for Life events
RTD Eco Pass For local Colorado Employees
Career Development Programs
Stock Option Eligibility
Employee-led Resource Groups


 Pax8 is an EEOC Employer.


"
https://startup.jobs/data-engineer-clarity-ai-4325980,Engineer,Data Engineer,Clarity AI ,"Madrid, Spain",Full-Time,"


At Clarity AI, we are committed to bringing social impact to markets. We are a tech company started in 2017, leveraging AI and machine learning technologies to ensure environmental, social and governance dimensions are the focal point of decision making within financial markets.
We are now a team of more than 180 highly passionate individuals coming from all over the world. Together, we have established Clarity AI as a leading startup backed by investors and strategic partners such as SoftBank, BlackRock, and Deutsche Brse who believe in us and share our goals. We have plans to continue growing our teams in Spain, Portugal, the UK, and the US this year so if you would like to join us on this rocket ship, keep reading!
Clarity is looking for a Data Engineer for the Tech team. If youre a SoftwareData Engineer, specialized in Data and enjoy solving complex problems with code, developing and you are not afraid of learning new things, we are looking for you.
You will be part of the team delivering the different parts of our production-ready product while co-designing and implementing an architecture that can scale up with the product and the company.
Our tech stack is documented here httpsstackshare.ioclarity-aiclarity-ai-data
Location This position can be based within Spain.
KEY RESPONSIBILITIES
 You will help to build a data platform to enable other teams to self-service high-quality data while performing data operations to support their day-to-day actions. You will join the data engineering team which is responsible for developing models, practises and systems to support the data lifecycle.  Youll support designing and creating all aspects of our ever-growing set of external and internal data pipelines, understand the problems, and tie them back to data engineering solutions  You will be responsible of designing and building our ETLs   Youll transform raw data from different sources batch and near-real-time into intuitive data models, using a diverse set of tools such as Spark, Hadoop, Redshift, Kafka, etc. to build robust data pipelines of high quality in a scalable fashion regarding volume and maintainability. In order to develop feature and product features, you will form part of cross functional squads including people of many different teams. Work closely with Platform, Data Science and Backend teams, as well as product and tech teams. Fix, troubleshoot and solve bugs and issues 
Requirements
WHAT WE ARE LOOKING FOR
We like engineers who adapt quickly to new challenges and break them down in smaller pieces that a highly productive team will find high-quality solutions for. As an Engineer, you will have a solid technical foundation and a strong focus on growing your development skills.
They use best practices to ship high-quality code and continue to push their knowledge.
 You have more than 3 years of professional experience as a Software DeveloperData Engineer, and hold a degree in a quantitative field CS, Engineer, Math, Physics,  . You have built production data pipelines in the cloud, setting up data-lake and server-less solutions; on top of that, you have hands-on experience with schema design and data modeling. Proven knowledge of PythonPandas is required, JavaScala will be a plus You have strong SQL skills and knowledge and familiarity with other distributed data stores such as Redis, ElasticSearch, Druid, Cassandra, Hbase.  You are comfortable working in containerized cloud developments, and familiar with the integration of public and private APIs. Strong communication skills. The right candidate can thrive in an environment of asynchronous conversation, since a part of the Clarity team will be based remotely.  Open to try new technologies and programming languages. Fluency in English 
It will be a plus if you had
 Experience in SaaS fast growing business would be preferable, although not required. Ideally experience in the financial sector or regulated markets. Familiarity with some other technologies  frameworks Spring Boot, Jersey, Play.. is a plus 
Benefits
Our people are our main asset. We have built a unique team and we all share 3 differential aspects Excellence, Passion and Values
 Our team comprises professionals from leading tech, consulting and banking firms, entrepreneurs, PhDs from top research institutions, and MBA graduates from top business schools. Claritys Founder and CEO, Rebeca Minguela, is a successful entrepreneur who has been recognized as one of the most distinguished leaders under the age of 40 by prestigious institutions like the World Economic Forum. Rebeca is joined by an excellent leadership team coming from diverse backgrounds, countries and experiences. 
We are different. We work hard to become the best place to work and pride ourselves on our culture
 Fact based Promoting objective, fact-based and solution-oriented discussions. Diverse Encouraging diversity of personalities, cultures and experiences. Transparent Communicating feedback transparently, constructively and in real-time. Meritocratic Striving for excellence and rewarding the best. Flexible Working flexibly in the broadest sense schedule, location, vacation, styles. 

Clarity AI believes diversity, inclusion, and belonging are essential for creating an innovative and successful workplace. Thats why we created our own ERG groups. 
Our own ERG groups act as a voluntary employee resource group, they commit to promoting these values and creating a safe and supportive community for all employees. They are committed to creating a workplace that champions sustainability, diversity, equity, and inclusion DEI.
To do this, they will advocate for sustainability initiatives, host and participate in events, workshops, provide a safe space for employees to share their experiences, ideas, and solutions, and collaborate with other departments and employee groups to improve the companys sustainability goals.
By actively promoting and engaging in sustainability efforts, we can help create a more equitable and resilient future for our planet and all its inhabitants.

We care about our employees and think they deserve the best.
 ESOP Phantom stock We want our employees to really feel part of Clarity. Thats why we give them the opportunity to participate in our success with ESOP or phantom stock options. Training budget Continued learning is important to us so we provide everyone with a yearly training budget. Additional benefits are available depending on your location. 
We are a US company with a UK and Spanish subsidiary. We have offices in New York City, London and Madrid and have possibilities for remote working for many of our roles.
Clarity has received several awards
 World Economic Forum Pioneer One of the most innovative projects in the US. Harvard Innovation Lab Top startup with impact worldwide by IMPACT Growth 2017 Top 10 Fintech startup worldwide by BBVA Open Talent Young Global Leader to Rebeca Minguela 2017 
Privacy Policy
Responsibility Clarity AI Inc. 160 Greentree Drive, Suite 101, City of Dover, County of Kent, Delaware 19904, United States.Purpose management of selection processes for new employees.Legitimation necessary processing for contract execution.Recipients company of the Clarity Group in the country that would carry out the contract or where position is being offered.Rights the interested party may exercise the rights of access, rectification, opposition, limitation, suppression, portability and not to be the object of automated decisions according to the Privacy Policy.More info to consult the privacy policy in detail, click on the following link Privacy Policy


"
https://startup.jobs/data-engineer-pure-storage-4322922,Engineer,Data Engineer,Pure Storage ,"Prague, Czechia",,"


BE PART OF BUILDING THE FUTURE.
What do NASA and emerging space companies have in common with COVID vaccine RD teams or with Roblox and the Metaverse? 
The answer is data, -- all fast moving, fast growing industries rely on data for a competitive edge in their industries. And the most advanced companies are realizing the full data advantage by partnering with Pure Storage. Pures vision is to redefine the storage experience and empower innovators by simplifying how people consume and interact with data. With 11,000 customers including 58 of the Fortune 500, weve only scratched the surface of our ambitions. 
Pure is blazing trails and setting records

For nine straight years, Gartner has named Pure a leader in the Magic Quadrant 
Our customer-first culture and unwavering commitment to innovation have earned us a certified Net Promoter Score in the top 1 of B2B companies globally

Industry analysts and press applaud Pures leadership across these dimensions

And, our 5,000 employees are emboldened to make Pure a faster, stronger, smarter company as we go

If you, like us, say bring it on to exciting challenges that change the world, we have endless opportunities where you can make your mark.

PURE IN PRAGUE
We opened our RD Second Home besides the one in Silicon Valley in Prague in January 2020 and since then we have experienced unbelievable growth. Our Engineers are solving complex engineering problems from low-level system software to cloud computing that help us innovate our cutting-edge products. Besides our world-class Software Engineering teams, we have many other talented colleagues working in Product Management, Sales, Finance, and HR to name a few. Whats more, weve developed these teams from talent across the globe.
ABOUT THE ROLE...
You will be responsible for design, development and expansion of data assets that support business intelligence, advanced analytics, and reporting. You will also develop, maintain, and support the enterprise data warehouse system, corresponding data marts, and data interfaces for use in reporting, analytics, and various applications. Deep experience building data management Data savviness and a keen understanding of data management life cycle is critical for this role. Crisp communication and the ability to influence decisions is also important. Program Management capabilities will be needed to successfully deliver in this role.  
IN THIS ROLE YOU WILL BE...

Working on full lifecycle application development
Designing, developing and maintaining customer-facing and internal-facing web applications in collaboration with our UX team in Angular and Typescript, and backend microservices in Java, Kotlin, Python, NodeJS
Experimenting with new technologies and architectural patterns such as micro frontends in order to push the state-of-the-art and innovate new solutions
Collaborating with peers and stake-holders to identify requirements and take solutions from initial design to production
Bringing a focus on design, development, unit testing, code reviews, documentation, continuous integration and continuous deployment

WHAT YOULL NEED TO BRING TO THIS ROLE...

Experience designing, implementing and testing various product components
4 years in any of the following programming languages, Javascript, Typescript, Java, Python
Ability to thrive in a highly collaborative and team-oriented environment
Must have Knowledge of AWS, networking infrastructure and SQL
Must be willing and able to work in an open office, team environment


BE YOUCORPORATE CLONES NEED NOT APPLY.
Pure is where you ask big questions, think differently, and make an impact. This is not just a job, but a place where you have a voice and can accelerate your career. We value unique thoughts and celebrate individuality, and with ample opportunity to learn, develop yourself, and expand into different roles, joining Pure is an investment in your career journey.
Through our Pure Equality program, which supports a flourishing field of employee resource groups, we nourish the personal and professional lives of our team members. And our Pure Good Foundation gives back to local and global communities through volunteering and grants.
And because we understand the value of bringing your full and best self to work, we offer a variety of perks to manage a healthy balance, including flexible time off, wellness resources, and company-sponsored team events.
PURE IS COMMITTED TO EQUALITY.
Research shows that in order to apply for a job, women feel they need to meet 100 of the criteria while men usually apply after meeting about 60. Regardless of how you identify, if you believe you can do the job and are a good match, we encourage you to apply.
Pure is proud to be an equal opportunity and affirmative action employer. We do not discriminate based upon race, religion, color, national origin, sex including pregnancy, childbirth, or related medical conditions, sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or any other characteristic legally protected by the laws of the jurisdiction in which you are being considered for hire. 
If you need assistance or an accommodation due to a disability, you may contact us at TA-Opspurestorage.com.
APPLICANT  CANDIDATE PERSONAL INFORMATION PRIVACY NOTICE.
If youre wondering how or why Pure collects or uses information you provide, we invite you to check out our Applicant  Candidate Personal Information Protection Notice.


"
https://startup.jobs/data-engineer-f-m-d-celtra-inc-4320207,Engineer,Data Engineer (f/m/d),"Celtra, Inc. ",,Full-Time,"

Are you curious, eager to learn, and passionate data enthusiast on a mission to help ingest and model data for better data-driven decisions in modern data architecture? Then this is a role for you!
Celtra is expanding the Data Insights team in Ljubljana, Slovenia with a talented Data Engineer, who will play the key role in improving our Data Landscape to enable data-driven decisions across all levels of our global company. You will have the opportunity to work with latest technologies for data integration and data wrangling while working closely with the data team colleagues as well as other engineers. You will be moving fast with the company strategy help to map Celtra data-environment.
Your responsibilities

Assist with the design, development, and maintenance of our data infrastructure using Matillion, Snowflake, Google Cloud, and AWS.
Implementing ETLELT pipelines to ingest data from a variety of sources into our data warehouse.
Keep existing data sources fresh against data quality issues, design data quality assurance framework and improve the processes for developing new ones raising the level of quality expected from our work.
Improve data understanding. Support data definition, data catalog, and data lineage efforts.
Continuously seek ways to improve our data infrastructure and processes.
Ability to create efficient DW or DL structures to minimize the cost of orchestrationprocessing and ingestion of data.
Working closely with other data members to understand existing datasets and strategic business objectives.
Collaborate with other cross-functional teams to ensure data is available, reliable and accessible.
Designing data models.

You will thrive in this position if you have 

Experience in data engineeringETL orchestration Matillion, FiveTran, .
Experience working with various data sources SQL, Web API, XML, JSON, flat files.
Experience with cloud-based data warehousing concepts, methodologies, and best practices Snowflake.
Building data models and data marts within a data warehouse.
Technical knowledge of BI system designs database designstructure and data structure star, snowflake schemas, denormalized designs.
Strong SQL skills are required, knowledge of other languages like Python is recommended.
Comfortable working in fast-paced, multi-task environment.
Self-motivated, take initiative.
Pragmatic and capable of solving complex issues.

If you do not fulfill all the criteria above, but you think you would be a great fit, we encourage you to apply either way!
What we offer?

Remote working. You can choose to work from home indefinitely or join us in our Ljubljana office.
Flexible schedule. We are not a clock in, clock out company. Youre fully trusted to create the right conditions for your best productivity.
Continuous education. Up to 1000 EUR per year for conferences, books, and courses of your choice.
Equipment of your choice.
Additional pension and health insurance.
WFH budget. An additional budget of 500 EUR to set up your home space.
Employee well-being. Dedicated budget for any kind of sports activities such as fitness, yoga, etc.

As we are hiring on a permanent employment agremeent and as this position is open in Ljubljana SI or Zagreb CRO you need to fulfill legal labor requirements non-EU citizens need working permits, and both non-EU and EU citizens need residence permits. 
About Celtra
Celtra helps enterprise advertisers, media, and agencies design, approve, and deliver digital creative across the ever-growing number of campaigns, markets, designs, and variants. Celtras Creative Automation  Enablement Software for helps brands move faster than ever while dramatically scaling content production. Companies like adidas, TripAdvisor, Spotify, Unilever, NBCU, Lululemon, YETI, Vice, and hundreds more partner with Celtra to cut production costs while increasing efficiencies and output in the cloud.
Empowering Creativity through Diversity  Inclusion
Our mission is to empower creativity - and we cannot fulfill our mission without different perspectives. Diversity drives innovation, and Celtra is committed to diversity, equity, inclusion, and belonging. 
Every employee is empowered at Celtra - no matter your race, age, religion, gender identity, sexual orientation, physical or mental ability, or ethnicity.  We hire the best, and develop our teams through continuous education and mentorship, in a community where everyone can bring their whole selves to work.
 
LI-Remote

"
https://startup.jobs/data-engineer-ii-iii-hyperion-epm-rackspace-4318297,Engineer,Data Engineer II/III (Hyperion EPM),Rackspace ,,Full-Time,"

Job Description Summary

Minimum of 6 years of experience on EPM products.
Maintain and support of OracleHyperions EPM suite for Essbase, Planning, Hyperion Financial Management and FDMEE.
Should have experience on EPM suite build from scratch with all Hyperion components on distributed environment.
Should have Proficiency in installation and configuration of Hyperion suite.
Must have knowledge on end to end SSLSSO setup with Hyperion suite.
     
Job Description

Minimum of 6 years of experience on EPM products
Maintain and support of OracleHyperions EPM suite for Essbase, Planning, Hyperion Financial Management and FDMEE
Should have experience on EPM suite build from scratch with all Hyperion components on distributed environment
Should have Proficiency on installation and configuration of Hyperion suite
Must have knowledge on end to end SSLSSO setup with Hyperion suite
Good knowledge on Database to maintain Hyperion applications
Should have experience on Load balancer, Hyperion application clustering
Should have experience on Migration activities by using LCM
Working experience on Hyperion application migrations from older versions to newer versions
Good technical knowledge on Essbase, planning, FDMEE and HFM applications
Collaborate with functional support teams to understand and help document end user requirements
Perform technical requirements gathering and analysis, providing time estimates and work plans for required systems changes
Perform unit testing and supportcoordinate end user acceptance testing activities
Manage and prioritize tasks such that they are completed on a timely basis and to users satisfaction
Prepare well written documentation such as technical specs, test plans, systems architecture, analysis etc as required
Understanding of financial data and business processes is a must
Working knowledge of Hyperion EPM suite HFM, Planning and Essbase as well as all related toolsets EIS, EAS, FDMFDMEE and Smartview
Must be able to use critical thinking skills to determine best approach to problem solving
Ability to understand end user requirements and translate to effective solutions
Experience with any or all of the following a plus Hyperion Enterprise
Strong interpersonal skills, as well as good written and verbal skills
Excellent time management, problem solving and teamwork skills 


About Rackspace Technology
We are the multicloud solutions experts. We combine our expertise with the worlds leading technologies  across applications, data and security  to deliver end-to-end solutions. We have a proven record of advising customers based on their business challenges, designing solutions that scale, building and managing those solutions, and optimizing returns into the future. Named a best place to work, year after year according to Fortune, Forbes and Glassdoor, we attract and develop world-class talent. Join us on our mission to embrace technology, empower customers and deliver the future.
 
 
More on Rackspace Technology
Though were all different, Rackers thrive through our connection to a central goal to be a valued member of a winning team on an inspiring mission. We bring our whole selves to work every day. And we embrace the notion that unique perspectives fuel innovation and enable us to best serve our customers and communities around the globe. We welcome you to apply today and want you to know that we are committed to offering equal employment opportunity without regard to age, color, disability, gender reassignment or identity or expression, genetic information, marital or civil partner status, pregnancy or maternity status, military or veteran status, nationality, ethnic or national origin, race, religion or belief, sexual orientation, or any legally protected characteristic. If you have a disability or special need that requires accommodation, please let us know.

"
https://startup.jobs/data-engineer-youcom-4317496,Engineer,Data Engineer,You.com ,,,"

About You.com


You.com is the worlds first open search engine platform that summarizes the web for users, with no ads, superior privacy choices, and personalization through preferred sources.


You.com is more than just a new search engine  its a movement to make the internet a place of trust, facts, and kindness  our guiding principles that were committing to from day one.

Its an audacious goal, but were ambitious people. Were looking for a few more ambitious folks to join our team.
Were not just wide-eyed dreamers  were pragmatic doers, too. Our founder and CEO, Richard Socher previously started an AI company called MetaMind. Salesforce acquired the company, and Richard became Chief Scientist, leading the companys AI efforts. Prior to MetaMind, Richard received the best Ph.D. thesis award from Stanford for his groundbreaking work on deep learning. Bryan McCann, co-founder and CTO, is a scientist and philosopher who led natural language processing teams at Salesforce after completing his Masters in C.S. at Stanford. Our founding team members have built companies worth hundreds of millions of dollars and scaled software to serve millions of users. For fun, we run marathons, paramotor, write poetry, read Latin, hike, and camp in the middle of nowhere.
If this sounds intriguing, say hello!

About the Job

Were building privacy-respecting analytics at web scale, to learn what our users love and continuously improve our product. Exec, Marketing, Product, and Eng teams rely on our analyses to make decisions, draw insights, and plan their strategy. We are the lantern in the dark, helping truth seekers find the way.

As a data engineer you will help us strengthen the foundation of our work, bringing in expertise in event collection, processing pipelines, and data management. You bring expertise in ETL or ELT, data warehouses, instrumentation and pipelines, and you get excited when the scale of data increases. You are curious about AI and want to contribute to the future of search. This is not an entry-level position.
Responsibilities

Design, build, and maintain data pipelines to support our search engine product
Develop and maintain data warehousing solutions to enable efficient data analysis and reporting
Collaborate with data scientists and software engineers to ensure data quality and consistency across the platform
Troubleshoot and fix issues related to data processing and storage
Continuously evaluate and improve our data infrastructure to ensure scalability and reliability
You are excited by data at scale.

Technically

3 years of experience working with distributed processing frameworks, such as DatabricksSpark and stream processing, event driven technologies such as Kafka
You have built ETL and ELT pipelines
You have worked with user event data and time series data
You have worked on feature engineering and data enrichment in the context of a large scale consumer product
You have an eye for data privacy, and are aware of best practices around data security and access
You are comfortable working with data of all formats, from relational DBs, to non relational DBs

Culturally

You are a kind, friendly person and strive to create a kind, inclusive work place filled with smiles and laughter
You take and give feedback graciously as part of growing individually and as a team
You will take joy in collaborative brainstorming and proposing novel extensions
You want to play an active role throughout the process of delivering your work to users
You are always willing to learn what you do not know
You are dependable and take pride in your work
You enjoy jumping in to help others with whatever part of the product they are building
You want to be part of defining and building a team around a vision and technology that has a direct path to improving the daily lives of people all over the world


LI-REMOTE


More about You.com


You.com is an equal opportunity employer your race, color, religion, sex, sexual orientation, gender identity, national origin, or disability status dont matter. Were committed to building a diverse, inclusive, and supportive workplace that is effectively distributed around the world.

Were a remote-first company, but work hours are generally within the Pacific Timezone 7 hours behind UTC. We get together in-person regularly as a team, but as long as your able to maintain significant overlap with Pacific hours during the workday were comfortable hiring in almost any location location-specific legal requirements permitting.

Benefits
 Competitive salary and equity
 Great health, dental, and vision insurance for you and your family
 401k plan
 Unlimited time off 4 weeks encouraged annually
 Generous parental leave
 Flexible work hours

"
https://startup.jobs/data-engineer-stripe-4315225,Engineer,Data Engineer,Stripe ,,,"

Who we are
About Stripe
Stripe is a financial infrastructure platform for businesses. Millions of companiesfrom the worlds largest enterprises to the most ambitious startupsuse Stripe to accept payments, grow their revenue, and accelerate new business opportunities. Our mission is to increase the GDP of the internet, and we have a staggering amount of work ahead. That means you have an unprecedented opportunity to put the global economy within everyones reach while doing the most important work of your career.
About the team
The Data Science team builds data and intelligence into our product, sales, and operations. This spans across building data foundations and applying statistical techniques and machine learning to measure and optimize our product, build data-driven products, and conduct in-depth analysis to inform strategic decisions.
What youll do
Were looking for people with a strong background in data engineering and analytics to help us scale while maintaining correct and complete data. Youll be working with a variety of internal teams -- Engineering, Business -- to help them solve their data needs. Your work will provide teams with visibility into how Stripes products are being used and how we can better serve our customers.
Responsibilities

Youll be working with a variety of internal teams -- Engineering, Business -- to help them solve their data needs
Your work will provide teams with visibility into how Stripes products are being used and how we can better serve our customers
Identify data needs for business and product teams, understand their specific requirements for metrics and analysis, and build efficient and scalable data pipelines to enable data-driven decisions across Stripe
Design, develop, and own data pipelines and models that power internal analytics for product and business teams
Help the Data Science team apply and generalize statistical and econometric models on large datasets
Drive the collection of new data and the refinement of existing data sources, develop relationships with production engineering teams to manage our data structures as the Stripe product evolves
Develop strong subject matter expertise and manage the SLAs for those data pipelines

Who you are
If you are data curious, excited about designing data pipelines, and motivated by having an impact on the business, we want to hear from you.
Minimum requirements

Have a strong engineering background and are interested in data
5 years of experience with writing and debugging data pipelines using a distributed data framework HadoopSparkPig etc
Have an inquisitive nature in diving into data inconsistencies to pinpoint issues
Strong coding skills in Scala, Python, Java or another language for building performance data pipelines.
Strong understanding and practical experience with systems such as Hadoop, Spark, Presto, Iceberg, and Airflow
The ability to communicate cross-functionally with solid stakeholder management to derive requirements and architect scalable solutions.


"
https://startup.jobs/data-engineer-cloudops-focused-mid-senior-deptagency-4315179,"Engineer,Senior",Data Engineer - CloudOps focused (mid/senior),Dept ,"Zagreb, Croatia",,"

DEPT Croatia is looking for a Data Engineer for our offices in ZagrebSplit.  
As a Data Engineer, you will be working on complex data integrations setups. The focus of our projects is digital marketing, but were not afraid to start in new areas. You collect raw data, build data models and run activation jobs to use your data into other platforms. No is not an option, you strive to solve every challenge and use your creativity to come up with solutions.
Youll be working closely together with Data Science colleagues to provide them with data they need for their models. These models predict the next actionproductmessage or other cool stuff. In addition, youll deploy the models and build scalable API-s to make real time predictions. 
You will guide your clients from your expertise and will be giving them advice to make sure the client becomes more Data mature and reaches their goals. You will also be involved in workshops and brainstorms with clients to spot small or big data opportunities.
Our DEPT culture is very important to us. That is why we are eager to find not only the most technically skilled and talented experts in the game, but also a perfect DEPT-fit. 
Our ideal candidate communicates clearly and brings out the best in situations and people around them. 
Together with your team, you will create and deliver top-quality solutions for our clients. Some of which are brands such as Microsoft, Samsung, Panasonic, JBL, Adidas, About you and many more.
 
DATA AT DEPT

A group of data-hungry colleagues that will provide you organised support
If you grow, we grow. Thats why well cheer you on with coaching and a development plan
Clients that will make you feel like a proud ambassador
Best of both worlds an international leading agency with the energy of a local studio

WHAT WE ARE LOOKING FOR

3 years of experience in a data engineering role must
Experience in Python must 
Experience in SQL must
Experience in CICD and Docker preferable
Experience in Google Cloud Platform, Amazon Web Services or Azure preferable
Experience in Airflow, DBT, BigQuery, Snowflake preferable
An analytical and conceptual thinker

Fluency in English


WHAT WE OFFER 

Transparent career development and pay
Flexible working hours
Hybridremote work
A rewarding, supportive and healthy company culture
Meal allowance
Travel allowance
Apple MacBook Pro or a PC of equivalent performances
Additional desk budget
Christmas and yearly bonuses
Referral bonus
Baby bonus 10 000 HRK  1.296,11 EUR
Fully paid sick leave
Supplemental and additional health insurance
Paid days off for special events weddings, etc.
A minimum of 25 vacation days 20 5 collective leave days
Global team-building in the Netherlands at DEPT fest
Multisport programme
Other bonuses, rewards and benefits; some benefits we have, some we are working on  and we are always up for recommendations 

 
About DEPT 
We are a pioneering technology and marketing services company that creates end-to-end digital experiences for brands such as Google, KFC, Philips, Audi, Twitch, Patagonia, eBay and more. Our team of 4000 digital specialists across 30 locations on 5 continents delivers pioneering work. 
Our culture is built on diversity, knowledge and individuality and this culture ensures that our experts do what they excel in and gives them freedom to become even better. 
We love individuals who take charge, we develop talent, share knowledge and support each other in our goal to become the best version of ourselves.
Since 2021 we have been Climate Neutral and B Corporation certified and we are committed to making a positive impact on the planet.
To apply, use our application form.If you are still in doubt about choosing us, dont hesitate to reach out and get a clearer picture of what DEPT actually is. 
 
Note Visa or relocation costs are not supported. This position is located in Croatia and requires Croatian citizenship.

"
https://startup.jobs/data-engineer-gouplift-4312495,Engineer,Data Engineer,Uplift ,"Toronto, Canada",Full-Time,"

Uplifts mission is to help people get more out of life, one thoughtful purchase at a time. Our enterprise Buy Now, Pay Later solution is used by the worlds most loved brands including Southwest Airlines, Carnival Cruise Line, Universal Studios, and more. With flexible pay over time installments, we empower consumers to buy what matters most while unlocking higher conversions and customer lifetime value for our partners. 

Our team is rapidly growing and comes from diverse backgrounds of leading technology and financial brands. Our HQ is in Sunnyvale, California with offices in Toronto, Ontario, New York, Reno, Nevada, and Guadalajara, Mexico.

Working at Uplift allows you to push your limits, challenge the status quo, and collaborate with some of the brightest minds in the industry. Were committed to building a diverse team and inclusive culture and believe your potential should only be limited by how big you can dream. We make this a reality by empowering you with the tools, resources, and support you need to grow your career.

Uplift is seeking a Data Engineer to join our Data Engineering team. You will be responsible for constructing a new data lake on Databricks and optimizing existing data use cases, from Business Intelligence to Machine Learning operations. As a member of our team, youll have the opportunity to work with cutting-edge technologies like Spark, Delta Lake, and ML flow. We invite you to come help us build an outstanding data platform for all of our business needs.
What you will do and achieve 

Design, build, and maintain the ETL using AWS and Databricks
Develop and maintain near real-time data pipelines 
Build Python libraries, tools, serverless applications and workflows
Creates applications using test-driven development and agile methodologies
Internal process improvements such as automating manual processes, building alertingmonitoring bots
Support daily operations of troubleshooting of Databricks and Snowflake jobs
Collaborate closely with product and business teams to influence technological decision-making and to support reporting needs
Work with analysts and data scientists to extract actionable insights from data that shape the direction of the company
Actively engage in design and code reviews - learn from your peers and teach your peers

Who you are 

3 years of related experience with a Bachelors degree; or 2 years and a Masters degree
Experience with Big Data, ETL, and data modeling
Solid coder with Python
Strong SQL knowledge and experience working with a variety of databases
Experience with modern cloud data warehouses preferably Databricks or Snowflake
Knowledge of Linux, AWS, and Docker
Experience in developing and operating high-volume, high-available and scalable environments
Working knowledge of API and stream based data extraction processes
Ability to align with rapid business changes new requirements, evolving goals and strategies and technological advancements
Experience supporting and working with cross-functional teams in startup culture

Life at Uplift


Health Insurance and RRSP plan some plans cover 99-100 premiums for medical, dental, and vision insurance and a RRSP

WorkLife Harmony Flexible, remote-first work culture. Uplift fosters a culture where employees can achieve both their professional and personal goals. This balance is especially true for our working parents

Shared Success competitive salary and Pre-IPO stock options

Health and Wellness Perks Uplift is proud to reimburse our employees for exercise, wellness products and activities as well as free counseling and coaching for physical, mental and emotional support

Professional Development We are committed to the growth and development of all of our employees. Uplift invests in professional conferences, certifications, and training for employees who want to grow in their careers

Pick-A-Perk money that can go towards something of your choosing within tuition reimbursement, student loan payment reimbursement, vacation savings account, charitable donations, or home office expenses




We want you!

If you made it this far, chances are youre as excited about working to change how people experience BNPL as we are  and we love that. Please apply even if youre unsure about whether you meet every single requirement in this posting. Uplift is looking for smart, intellectually curious people who are invested in our mission, not just those who can check all the boxes.

Uplift is proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status.

Note Uplift does not accept agency resumes. Please do not forward resumes to any recruiting alias or employee. Uplift is not responsible for any fees related to unsolicited resumes.

"
https://startup.jobs/data-engineer-canada-two95-international-inc-4311985,Engineer,Data Engineer (Canada),Two95 International Inc. ,"Montreal, Canada",,"

Title Data Engineer Location Remote Duration Contract Rate Open
Requirements
Technical expertise   BScMSc in Computer Science, Computer Science, Information Systems or related Technical Discipline  1-4 years experience in Data Engineer role 5-10 years for senior data engineer Deep knowledge of Python, SQL, and PySparkis required.  Experience working with data pipelines, architecture principles, batch and stream processing systems, and DataOps.  Experience working with large data sets, Azure cloud services including Azure Data Lake, Data factory, Databricks, Azure DevOps.   Background in programming in Python, Scala, C, C, Java is beneficial.  Agile experience   Experience working in AI startup environment or organisationswith an agile culture  Professional attitude and service orientation; superb team player  
Benefits
Note If interested please send your updated resume to ajith.anthonirajtwo95intl.com and include your rate requirement along with your contact details with a suitable time when we can reach you. If you know of anyone in your sphere of contacts, who would be a perfect match for this job then, we would appreciate if you can forward this posting to them with a copy to us.  We look forward hearing from you at the earliest!

"
https://startup.jobs/data-engineer-ii-analytics-vimeo-4311315,"Stats,Engineer","Data Engineer II, Analytics",Vimeo ,"Bengaluru, India",,"

Vimeo is looking for an experienced Data Engineer II, Analytics to join our Data Architecture and Analytics Engineering team and work closely with Data Analysts and Data Scientists to create and maintain robust, scalable, and sustainable data models that provide decision-making insights for senior leadership including executives. The ideal candidate is a self-starter with a bias for action and results, with experience in a fast-paced, data-driven environment.
 What youll do

Build data models that can support dynamic and efficient data analysis, and collaborate with other teams to maintain and evolve those models over time
Ensure alignment to coding standard methodologies and development of reusable code
Partner with Data Analysts to understand business processes and identify opportunities to build scalable and efficient data models that can be used for scalable, refreshable analysis and reporting
Create and enable the generation of ad-hoc  on-demand data sets for use by analysts and data scientists

Work with Engineering teams to set up monitoring and alerting systems for business and product KPIs 

Skills and knowledge you should possess

BSMS in Computer Science or a related technical field.
2 years working with Analytics and Data Engineering teams with a mixed data engineering and analytics background 
2 years of experience in scalable data architecture, fault-tolerant ETL, and monitoring of data quality in the cloud
Experience working on or leading initiatives around data governance, master data management, data catalogs, and enterprise data warehouse architecture
Strong analytical skills, data sensibility, and an able communicator
Open to working on multiple projects simultaneously
Proficiency in

SQL
Python 
Dimensional Modeling
Data pipeline development, workflow management, and orchestration tools
ETL optimization and best practices
Snowflake or other column-oriented and cloud-based databases
Looker or any similar Business Intelligence tool
Relational Databases


Bonus Points Nice Skills to Have, but Not Needed

DBT
Git  Github
LookerTableau is a big plus
large data sets terabyte scale
Apache Airflow


About Us
Vimeo NASDAQVMEO is the worlds most innovative video experience platform. We enable anyone to create high-quality video experiences to connect better and bring ideas to life. We proudly serve our growing community of nearly 300 million users  from creative storytellers to globally distributed teams at the worlds largest companies. Learn more at www.vimeo.com.
Vimeo is headquartered in New York City with offices around the world. At Vimeo, we believe our impact is greatest when our workforce of passionate, dedicated people, represents our diverse and global community. Were proud to be an equal opportunity employer where diversity, equity, and inclusion is championed in how we build our products, develop our leaders, and strengthen our culture.


"
https://startup.jobs/data-engineer-tripadvisor-4306699,Engineer,Data Engineer,TripAdvisor ,"Lisbon, Portugal",,"

We believe that we are better together, and at Tripadvisor we welcome you for who you are. Our workplace is for everyone, as is our people powered platform. At Tripadvisor, we want you to bring your unique perspective and experiences, so we can collectively revolutionize travel and together find the good out there.
Tripadvisor, the worlds largest travel site, operates at scale with over 500 million reviews, opinions, photos, and videos reaching over 390 million unique visitors each month.  We are a data driven company that leverages our data to empower our decisions.  Tripadvisor is extremely excited to play a pivotal role in supporting our travelers. With travel plans dashed in early 2020, many people have been dreaming about their next big holiday for more than a year, so its no surprise that travelers are extra conscious of getting it just right when they travel. Helping to play an important role in enabling travelers is an amazing opportunity over the next year and beyond.
Our data engineering team is focused on delivering Tripadvisors first-in-class data products that serve all data users across the organization. As a member of the Data Platform Enterprise Services Team, you will collaborate with engineering and business stakeholders to build, optimize, maintain, and secure the full data vertical. This includes tracking instrumentation, information architecture, ETL pipelines, and tooling that provide key analytics insights for business-critical decisions at the highest levels of Product, Finance, Sales, CRM, Marketing, Data Science, and more. All in a dynamic environment of a continuously modernizing tech stack including highly scalable architecture, cloud-based infrastructure, and real-time responsiveness. Tripadvisor provides a unique, global work environment that captures the speed, innovation and excitement of a startup, at a thriving, growing and well-established industry brand. We take pride in our data engineering and are looking for a talented and highly-motivated engineer with a passion for solving interesting problems to add to our high-performing team. 
What Youll Do

Providing the organizations data consumers high quality data sets by data curation, consolidation, and manipulation from  a wide variety of large scale terabyte and growing sources. 
Building data pipelines and ETL processes that interact with terabytes of data on leading platforms such as Snowflake and BigQuery.
Developing and improving our enterprise data by creating efficient and scalable data models to be used across the organization. 
Partnering with our analytics, data science, CRM, and machine learning teams.
Responsible for enterprise data integrity, validation, and documentation.
Solving data pipeline failure events and implementing sound anomaly detection 

Skills  Experience

BS or MS degree in Computer Science or a related technical discipline
4 years of data engineering or general software development experience 

Experience working with large datasets terabyte scale and growing and familiarity with various technologies and tooling associated with databases and big data

Relational DB PostgreSQLMySQL
Big Data i.e. Hadoop, Hive, BigQuery, Snowflake


Demonstrated proficiency in data design and data modeling 
Experience in developing complex ETL processes from concept to implementation; these should include defining SLA, performance measurements and monitoring.
Proficiency in query language and data exploration skills, proven record of writing complex SQL queries across large datasets. 
Systems performance and tuning experience, with an eye for how systems architecture and design impacts performance and scalability
Strong software engineering principles
Experience in functional programming in Python or in an equivalent language
Self-paced, organized, and detail-oriented person with a strong sense of ownership
Strong communication skills to effectively communicate with both business and technical teams. 
Ability to work in a fast-paced and dynamic environment
Ability to break down complex problems into simple solutions.
Strong interpersonal skills, intense curiosity, and an enthusiasm for solving difficult problems.

 
We strive to create an accessible and inclusive experience for all candidates. If you need a reasonable accommodation during the application or the recruiting process, please make sure to reach out to your individual recruiter or our team at greenhousetripadvisor.com. 
 
LI-JP
LI-Remote
 
 
 

"
https://startup.jobs/data-engineer-dun-bradstreet-4303999,Engineer,Data Engineer,Dun & Bradstreet ,"Warsaw, Poland",Full-Time,"

Why We Work at Dun  Bradstreet
Dun  Bradstreet unlocks the power of data through analytics, creating a better tomorrow. Each day, we are finding new ways to strengthen our award-winning culture and accelerate creativity, innovation and growth. Our 6,000 global team members are passionate about what we do. We are dedicated to helping clients turn uncertainty into confidence, risk into opportunity and potential into prosperity. Bold and diverse thinkers are always welcome. Come join us!

About the role
You will be part of the team that is responsible for the accuracy and integrity of our UK and Ireland Identity Data.  You will deliver and maintain data engineering solutions that ensure consistent operation of our UK and Ireland Data Management System.  You will develop new innovative processes to manage data quality and streamline existing processes to execute data improvement plans more efficiently and effectively.   
You will also be key technical support in conversations with our data partners to help understand their data and ensure we are maximizing value from all our data sources
Key Responsibilities 

Day-to-day operations of our Data Management System; ensuring process and procedures run as expected and any bugs are fixed in a timely manner with business-critical processes prioritised
Architecting solutions; Design, develop and test fit for purpose, resilient, scalable, and future-proof data services.
Recognise and exploit opportunities to ensure efficient and effective performance of Data processes. Explore new ways of conducting operational processes; developing ways of maximising the update cycle on data; achieving the best cost model whilst maintaining data quality
Write ETL scripts and code to make sure ETL processes perform optimally
Design, write and iterate code from development to production-ready. Understands security, accessibility, and version control. Can use a range of coding tools and languages.
Plan, design, manage, execute, and report tests, using appropriate tools and techniques, and works within internal policy and regulations. Ensuring risks associated with deployment are adequately understood and documented
Design and maintain appropriate metadata repositories to enable understanding of data assets and full auditability.
Partnering with cross-functional teams to understand how new sources will contribute towards the ongoing enhancement of data assets to achieve goals for database size, completeness, and other desirable improvements.
Creation of operational reports to measure usability and performance of data sources
Contribute to the vision and scope for the next generation of our data to drive revenue, performance quality and ensure operational efficiency, including new types of data and emerging capabilities for data collection.
Support operational plans that deliver business requirements through leading and coordinating their development, testing, and managing stabilisation activities.

What were looking for

3 years of proven in-depth knowledge and experience of SQL and database querying languages
Knowledge of other languages, like Python, would be advantageous
Ability to use PowerBI would be advantageous
Experience with ETL tools, in particular SSIS SQL Server Integration Services
Experience working with APIs and services
Has a demonstrable understanding of how to expose data from systems for example, through APIs, link data from multiple systems using different storage technologiesaccess methods and deliver streaming services. Creates repeatable and reusable procedures.
Ability to correctly execute test scripts under supervision. Understanding the role of testing and how it works.
Aware of the types of problems in databases, data processes, data products and services.
Ability to run development using Agile and Kanban methodologies
Dynamic and results-driven with the focus on facilitating action and effecting change
An innovative and inspirational approach
Self-motivation with the desire to learn new techniques  relentlessly curious
Demonstrable experience in Database design, modelling and best practice
Analytical, process and problem-solving skills in a highly complex environment. A clear thinker who can articulate database issues and solutions and gain support for implementation, whilst remaining focused on what is important. 
Ability to prioritise and multitask with flexible approach to changing deadlines and scope of project
Ability to work independently
A great team player


All Dun  Bradstreet job postings can be found at httpswww.dnb.comabout-uscareers-and-peoplejoblistings.html. Official communication from Dun  Bradstreet will come from an email address ending in dnb.com.

Global Recruitment Privacy Notice

"
https://startup.jobs/data-engineer-entry-senior-level-pikpok-4300349,"Engineer,Senior",Data Engineer - Entry/Senior level,PikPok ,"Wellington, New Zealand",Full-Time,"

Who we are
We love games, so we make games people love to play! Since 1997, PikPok has developed games for mobile, console, and PC that have engaged and delighted players with high quality art, immersive audio, intuitive gameplay, and rewarding in-game experiences. Our games have won industry awards, been given the Editors Choice distinction by the App Store and Google Play, and been downloaded half a billion times in more than 200 territories worldwide.
PikPok values the contributions of people with diverse backgrounds, experiences, skills, and perspectives. We want our games to reflect the diversity of the people playing them now and in the future. People with a strong sense of curiosity, ownership, and who love to collaborate will fit well with our values.
About the Role
The analytics department enables us to make great game experiences by providing analytics and business intelligence to the wider studio. Central to these operations is the Data Engineer, who supports the ongoing growth and development of our data pipelines and data products, encouraging an environment of effective decision-making based on sound hypotheses.
The Data Engineers responsibilities include
 Extract, load, and transform data from multiple sources including server logs, 3rd party SDKstools, AB tests, APIs. Building, maintaining, and improving both on-premise and cloud data pipelines and data storage. Building tools, APIs, functions, and services to get the maximum value from our data. Developing and upholding best practice development methodology. Producing technical documentation. 
Requirements
 Qualification in programming andor information technology andor computer science. Experience in an analytics developer, data engineer, or similar role. Proficient in Python and SQL. Understanding of good software design and architecture, version control systems SVN and GIT, and writing good readable code. Experience building and optimising data pipelines. Experience building andor accessing APIs. Experience working with cloud and big data technologies AWS, Redshift, Kinesis, S3, Celery an advantage, and CICD would be a plus. 
Benefits
PikPok offers a wide range of great benefits, outlined on our careers page
httpsapply.workable.compikpok

To Apply 
If youd like to know more about the role, further information can be found here
link.pikpok.com3Zsth88
link.pikpok.com3Zxuhb3
link.pikpok.com3M2Hyp7

To apply for this role please visit our careers page on httpsapply.workable.compikpok. Please include a covering letter with your application.

"
https://startup.jobs/data-engineer-op3n-world-4298392,Engineer,Data Engineer,OP3N WORLD ,"Los Angeles, United States",,"

OP3N was founded in 2021 as a subsidiary of EST Media Holdings, OP3N imagines a world where every human can create, own, and connect their ideas to community. Our mission is to be a launchpad for ideas and communities to create meaningful experiences together by consolidating the tools needed to mint, share and engage with NFTs and digital tokens into one vertical stack. 
OP3N leverages its cross-industry expertise from the entertainment, gaming and tech ecosystems to lay the foundations for a new era of community-driven, inclusive entertainment while bringing everyone together on a journey into Web3.
Were looking for an experienced Senior Data Engineer to own and scale our data infrastructure as we continue to build a first-of-its-kind web3 super app. This is a unique opportunity to join a small team of engineering and product leaders at the ground level, building scalable data solutions for what could be hundreds of millions of active users globally.
 
Key Responsibilities

Immediate

Audit current data infrastructure and availability in Cloud Firestore 
Spec and implement a new data warehouse I.e. Snowflake
Partner with Product to develop a scalable data infrastructure and analytics framework with ever increasing user data
Partner with engineering to ensure data flows into an easily queryable solution

Ongoing

Create and audit data infrastructure as the company, users, and product grows
Build well-designed scalable systems to extract, transform, and load data into warehouses from a variety of sources
Own data for new productsfeatures define data sources and architecture, set up data pipeline, design warehouse  reporting, etc. 
Explore available technologies and design solutions to continuously improve our data quality, workflow reliability, and scalability while reporting performance and capabilities


Ideal Background  Skillset

5 years of professional experience in data engineering

3-4 years of deep data pipeline experience including data warehouse setup

Proven builder - has created  maintained warehouses
Excellent written and verbal communication skills - able to clearly communicate needs and directions asynchronously across key stakeholders
Experience partnering with Product and Engineering teams on large scale data projects
Technical expertise 

Fluency in Python
Knowledge of Kafka  streaming technologies 
Machine learning  data modeling expertise a plus



"
https://startup.jobs/data-engineer-with-computer-vision-knowledge-phantom-ai-2-4290929,"Engineer,Computer Vision,Artificial Intelligence",Data Engineer with Computer Vision Knowledge,Phantom AI ,"Mountain View, United States",,"

About Us
At Phantom AI, weve built a team of incredibly talented and ambitious people challenging the norm in the automotive industry. We are building cost-effective L2L3 solutions to reduce the burden of everyday driving and make the roads safe for everyone. For instance, we believe democratizing technologies such as Automatic Emergency Braking and Emergency Lane Support is the first priority before tackling a fully self-driving vehicle. Our main customers are Tier 1 automotive manufacturers who are focused on delivering L2L3 solutions and in the future will deliver full autonomy.
We differentiate ourselves from other autonomous driving startups through a combination of state-of-the-art technological know-how and real automotive experiences of shipping ADAS systems at a volume production scale. If you feel that you have the passion, commitment, and drive to challenge the status quo within the automotive industry, we would love to hear from you.
 
Responsibilities

Develop a metadata mining system for deep learning datasets.
Develop an automatic QC system for human-labeled annotation.
Work with internal teams and external companies to coordinate data in database servers and from data providers.

 
 
Required Qualifications

3-5 years of relevant paid, professional, post-academic not including research, internships, co-ops, practica, etc. working experience with visual motion estimation.
Excellent Python programming and software design skills.
Excellent problem-solving capability.
Bachelors degree or equivalent experience

 
Desired Qualifications

Masters degree or equivalent experience

 
Benefits
We offer our employees a comprehensive benefits package including

Salary 130,000-180,000
Medical, dental, and vision coverage
Office snacks  reimbursable meals
Paid Time Off
FSA
401K

 
Work Type
Hybrid - Phantom AI follows this type of working experience to allow employees the flexibility to work weekly at the office 3x and from home 2x.
 
Equal Opportunity for Diversity  Inclusion
Phantom AI provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
 

"
https://startup.jobs/data-engineer-finops-atlassian-4290815,Engineer,Data Engineer - FinOps,Atlassian ,"Sydney, Australia",Full-Time,"

Working at Atlassian

Atlassian can hire people in any country where we have a legal entity. Assuming you have eligible working rights and a sufficient time zone overlap with your team, you can choose to work remotely or from an office unless its necessary for your role to be performed in the office. Interviews and onboarding are conducted virtually, a part of being a distributed-first company.

With a sufficient timezone overlap with the team, were able to hire eligible candidates for this role from any location in Australia and New Zealand.
Your future team
In this role as a Data Engineer, reporting into the Engineering Manager - FinOps, youll be an important part of our cost efficiency strategy across Atlassian. As a member of our Industry-leading Cloud FinOps team, part of our Engineering Excellence department, youll own the data model and pipelines related to our infrastructure spend.
Its your job to ensure our cost data is available, complete, accurate, and able to be trusted by teams to make high quality data driven decisions.
What youll do

Contribute to our data model for Atlassian costs.
Build and run our data pipelines to ingest our vendor costs with a focus on solid engineering principals.
Ensure our data is of high quality and well understood.
Work with large datasets.
Partner with analysts to ensure our data meets downstream requirements.
Work as part of a fast-growing, multi-disciplinary, high impact, industry recognised FinOps team.
Get exposure, impact and visibility across Atlassian engineering teams and senior execs.

Your background

2 years of technical experience as a data or software engineer.
1 years experience with AWS.
Demonstrative ability to build data pipelines with solid software engineering principals in mind.
Strong SQL and Python skills.

Its great but not required if you have

A background in FinOps or Finance.
Experience working with Databricks




Our perks  benefits

To support you at work and play, our perks and benefits include ample time off, an annual education budget, paid volunteer days, and so much more.

About Atlassian

The worlds best teams work better together with Atlassian. From medicine and space travel, to disaster response and pizza deliveries, Atlassian software products help teams all over the planet. At Atlassian, were motivated by a common goal to unleash the potential of every team.

We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyones perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.

To learn more about our culture and hiring process, explore our Candidate Resource Hub.

"
https://startup.jobs/data-engineer-bangalore-c3ai-4289911,Engineer,Data Engineer - Bangalore,C3.ai ,"Bengaluru, India",,"

C3.ai, Inc. NYSEAI is a leading provider of Enterprise AI software for accelerating digital transformation. The proven C3 AI Platform provides comprehensive services to build enterprise-scale AI applications more efficiently and cost-effectively than alternative approaches. The core of the C3 AI offering is an open, data-driven AI architecture that dramatically simplifies data science and application development. Learn more at www.c3.ai
C3 AI has an opening for a Data Engineer. You will be required to create advanced application integration solutions and configure, deploy and enhance enterprise cloud applications. C3 AI product suite is entirely data-driven, so a great candidate will have a passion for acquiring, analyzing, and transforming data to generate insight. This role is very hands-on and requires a structured mindset and solid implementation skills.
Qualified candidates will have a solid knowledge of integration and data manipulation technologies.
Responsibilities

Engage directly with customers to participate in design and development of data integrationtransformation solution according to functional requirements
Perform debugging, troubleshooting, modifications and unit testing of integration solutions
Support, monitor, execute production application jobs and processes
Participate in the development of documentation, technical procedures and user support guides

Qualifications

2 years of experience with systemdata integration, development or implementation of enterprise andor cloud software
Eligible to work in Singapore without sponsorship
Proficiency in data integrationEAI technologies, such as Tibco
Demonstrated proficiency with JavaScript
Experience with Unix-based operating systems
Familiarity with version controlSCM is a must experience with git is a plus
Experience with relational databases any vendor
Solid understanding of concepts of cloud computing
Engineering degree in Computer Science or Electrical Engineering
Working knowledge of Agile Software development methodology
Strong organizational and troubleshooting skills with attention to detail
Strong analytical ability, judgment and problem analysis techniques
Interpersonal skills with the ability to work effectively in a cross functional team

C3 AI provides a competitive compensation package and excellent benefits.
C3 AI is proud to be an Equal Opportunity and Affirmative Action Employer. We do not discriminate on the basis of any legally protected characteristics, including disabled and veteran status. 

"
https://startup.jobs/data-engineer-hatch-it-4278004,Engineer,Data Engineer,Hatch IT ,"Falls Church, United States",,"

Hatch I.T. is partnering with JDSAT to find a Data Engineer. See details below

About the Role
Their team is looking for an experienced data engineer to contribute to the design and buildout of a data architecture to support advanced analytics services for federal clients. Applicants should have experience building enterprise-grade data ingestion solutions and, ideally, on cloud platforms like Amazon Web Services

About the Company
JDSAT is a solutions company before anything else, and they believe that data, mathematics, and software are the most effective path to finding a solution thats tailored to organizations needs.

Candidates must be able to obtain a secret level clearance
What does a typical day look like?

Assess, analyze, and organize raw data
Prepare data for use in simulation, optimization, or data science tools 
Combine information from many disparate sources, conduct data validationverification, and store the data in a manner that enables database users to intuitively join information across multiple domains 
Leverage industry best practices across all relevant levels of the technology stack to include the version control system, ETL processes, and the use of AWS cloud products 
Identify opportunities for new data acquisition 
Interface with source data in a variety of different forms to include APIs, flat file extracts, direct database querying, etc. 
Apply strong attention to detail to identify errors or data quality concerns and communicate with customers in a professional manner

What qualifications are they looking for?

Bachelors degree in Analytics, Data Analysis, Statistics, Finance, Economics, Computer Science, or related field with minimum 8 years experience working or a combination of postgraduate studies and working in a technical field as described above. 
Ability to assess and profile raw data and reassemble raw data from multiple sources into a single, enterprise model 
Ability to work in a consulting role, building technology and communicating with end-users and customers of varying levels of technical capability 
Ability to produce high-quality, professional documentation and communication materials 
Ability to build and deploy team-friendly code in SQL

Theyre extra impressed with folks who have

Masters degree in Analytics, Data Analysis, Statistics, Finance, Economics, Computer Science, or related field 
Ability to build and deploy team-friendly code written in Python and R 
Experience with Amazon Web Services AWS 
Experience leveraging cloud platforms to build secure and scalable data infrastructure 
Experience building batch or streaming data ingestion pipelines
Experience working with Medallion Architectures and Master Data Management


Dont think youre 100 qualified for this position? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every single qualification. At hatch I.T., were dedicated to helping companies build diverse, inclusive and authentic workplaces, so if your experience doesnt perfectly align with every qualification in the job description, we encourage you to apply anyway. You may just be the right candidate for this or other roles.


If you are interested in learning more about this company or any StartupsSmall Businesses in the area, please contact us and check us out here!! 
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.


"
https://startup.jobs/data-engineer-foursquare26-4275045,Engineer,Data Engineer,Foursquare ,,,"

Foursquare is the leading independent location technology and data cloud platform, dedicated to building meaningful bridges between digital spaces and physical places. Our proprietary technology unlocks the most accurate, trustworthy location data in the world, empowering businesses to answer key questions, uncover hidden insights, improve customer experiences, and achieve better business outcomes. A pioneer of the geo-location space, Foursquares location tech stack is being utilized by our mobile apps CityGuide and Swarm, as well as the worlds largest enterprises and most recognizable brands, like Amazon, Microsoft, Samsung, Spotify, Uber, Airbnb and others.
Foursquares flexible building blocks include technology to maximize marketing impact and drive incremental real-world engagement Attribution, Audience, Proximity, SDK; data to deeply understand points of interest and real-world behavior patterns Places and Visits, and tools to conduct advanced analysis, data enrichment, unification and visualization Studio.
About the team
Foursquares Marketers Engineering team writes and operates the software which produces core data sets for our Marketers suite of products. These petabyte-scale pipelines process geospatial data for the purposes of marketing use cases, such as ad targeting and attribution. Its critical to this teams success that we have rich data sets to build our applications on top of, and this data is kept fresh, easy to explore, and simple to make changes. The engineers on this team work closely with application engineers to prove out variant approaches and introduce new functionality.
About the Position
In the Data Software Engineer role, you will ship products with high visibility and strategic importance to Foursquare and contribute directly to the revenue. Our pipelines are written in a variety of programming languages and deployed to multiple orchestration platforms. The main technologies we work with are Spark, Amazon EMR, Ruby, Java, and Apache Airflow.
In this role, youll

Write and operate the data pipelines which produce Foursquares core data sets for our Marketers suite of products  Attribution and Targeting.
Document the expected and actual behavior of these pipelines, along with expectations for inputs and outputs.
Monitor data quality and freshness, with a focus on proactively evaluating the business impact of changes. Report regularly on the state of the data sets and the software which produces them.
Maintain a prioritized list of data questions and bugs which require further investigation. Escalate as needed to call attention to problems with the input data.
Evaluate new sources of data, and build new pipelines that combine our data in creative ways that drive customer value.
Participate in on-call rotation duties to ensure that data is correct and produced on-time, and to restore service when the pipelines are experiencing an outage.

What youll need

2-4 years of software development experience.
Professional experience with at least one of Hadoop MapReduce andor Spark data processing pipelines.
Strong algorithms and data structures knowledge.
Professional experience scripting with the UnixLinux command line or Python.
Experience with cloud computing service providers, such as AWS.
Experience with containerization technologies, such as Docker, Mesos or Kubernetes.

Excellent written communication skills.

Your own unique talents! If you dont meet 100 of the qualifications outlined above, we encourage and welcome you to still apply!



Nice to have

Experience with CICD systems such as Jenkins, Travis, TeamCity, and CircleCI.
Experience at marketing or ad-tech data companies RTB  real-time bidding. DSP  demand-side platform.
Experience with geospatial data processing.

Benefits and Perks

We are a fully flexible company that allows you to work from any location in Serbia or from our Belgrade office
25 days of paid vacation
Private medical insurance
Education stipend
Home Office Setup, you get all the necessary hardware
Summer Fridays, two non-working Fridays each July and August
Learning and development programs 
Professional coaching
Restricted Stock Units

Foursquare is proud to foster an inclusive environment that is free from discrimination. We strongly believe in order to build the best products, we need a diversity of perspectives and backgrounds. This leads to a more delightful experience for our users and team members. We value listening to every voice and we encourage everyone to come be a part of building a company and products we love.
Foursquare is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected Veteran status, or any other characteristic protected by law.
Foursquare Privacy Policy
LI-MT1

"
https://startup.jobs/data-engineer-azure-scala-kafka-tiger-analytics-4269233,"Engineer,Scala",Data Engineer - Azure (Scala/Kafka),Tiger Analytics ,,Full-Time,"

Description
Tiger Analytics is pioneering what AI and analytics can do to solve some of the toughest problems faced by organizations globally. We develop bespoke solutions powered by data and technology for several Fortune 100 companies. We have offices in multiple cities across the US, UK, India, and Singapore, and a substantial remote global workforce. We are expanding our Data Engineering practice and looking for Sr. Azure Data Engineers to join our growing team of analytics experts. The right candidate will have strong analytical skills and the ability to combine data from different sources and will strive for efficiency by aligning data systems with business goals. This is a remote role for applicants based in USA.
Requirements
 Bachelors degree in Computer Science or similar field 8 years experience in Data Engineering  several years in Analytics space Strong Proficiency in Scala - coding experience a must Strong Proficiency in Kafka and ADF for data pipelines migration experience a must Azure Synapses  Experience with real time streaming, Kafka, and API Integration  Experience in PySpark Strong Proficiency in Python programming. Strong Proficiency in SQL queries Experience building data pipelines using Azure stack Experience using Apache spark Good working experience on Delta Lake and ETL processing Prior experience of working in a Unix environment Experience in harmonizing raw data into a consumer-friendly format using Azure Databricks Experience extractingqueryingjoining large data sets at scale Experience building data ingestion pipelines using Azure Data Factory to ingest structured and unstructured data Experience in data wrangling, advanced analytic modeling is preferred Strong communication and organizational skills 
Benefits
This position offers an excellent opportunity for significant career development in a fast-growing and challenging entrepreneurial environment with a high degree of individual responsibility.

"
https://startup.jobs/data-engineer-gcp-capco-4268808,Engineer,Data Engineer (GCP),Capco ,,,"

CAPCO POLAND
Capco Poland is a global technology and management consultancy specializing in driving digital transformation across the financial services industry. We are passionate about helping our clients succeed in an ever-changing industry.
We also are experts in          focused on development, automation, innovation, and long-term projects in financial services. In Capco, you can code, write, create, and live at your maximum capabilities without getting dull, tired, or foggy.
We are looking for 3 Data Engineers to support one of our best-in-class clients from banking. Our clients Big Data Lake is the largest aggregation of data ever within financial services with over 300 sources and a rapidly growing book of work. The main aim is to build new analysis approaches and cases based on existing data sources and create a Big Data ecosystem. Sometimes it is migration from on-premise to cloud, sometimes establishment of on-premises solutions, and sometimes cloud solutions separately.
 
THINGS YOU WILL DO

Deliver an ecosystem of curated, enriched, and protected sets of data  created from global, raw, structured, and unstructured sources
Collect, store, analyze, and leverage data
Integrate data with the architecture used across the company
Data Engineering and Management
Data development process design, build and test data products that are complex or large-scale
Promote development standards, code reviews, mentoring, testing, scrum story writing
Cooperate with customersstakeholders

 
TECH STACK ETL, Hadoop-Based Analytics Hbase, Hive, mapReduce, Kafka, Spark, BI, ETL, Database, etc, Python, Spark, GCP cloud storage, Big Query, PubSub, Data Flow, Jenkins, GitHub 
 
SKILLS  EXPERIENCES YOU NEED TO GET THE JOB DONE

Experience working with data pipeline building technologies Python, Spark
Experience with Hadoop eco-system and data management frameworks
Good knowledge of ETL and Data warehouse concepts
Good knowledge of SQL and relational database design
Knowledge of GCP cloud storage, Big Query, PubSub, Data Flow
Knowledge of CI CD, Agile, DevOps, Software Development Life Cycle SDLC
Understanding users requirements and functional specification
Understanding of tools and components of Data Architecture
Excellent communication, interpersonal, and decision-making skills
Good English knowledge

WHY JOIN CAPCO?

Employment contract andor Business to Business - whichever you prefer
Possibility to work remotely
Speaking English on daily basis, mainly in contact with foreign stakeholders and peers
Multiple employee benefits packages MyBenefit Cafeteria, private medical care, life-insurance
Access to 3.000 Business Courses Platform Udemy
Access to required IT equipment
Paid Referral Program
Participation in charity events e.g. Szlachetna Paczka
Ongoing learning opportunities to help you acquire new skills or deepen existing expertise
Being part of the core squad focused on the growth of the Polish business unit
A flat, non-hierarchical structure that will enable you to work with senior partners and directly with clients
A work culture focused on innovation and creating lasting value for our clients and employees

ONLINE RECRUITMENT PROCESS STEPS

Screening call with the Recruiter
TechnicalCompetencies interview with Capco Hiring Manager
Culture fit with Head of Engineering
FeedbackOffer

 
Follow us here    or contact us directly  recruiting.polandcapco.com 

"
https://startup.jobs/data-engineer-parker-4263587,Engineer,Data Engineer,Parker ,"New York, United States",,"

About the role


Parkers mission is to increase the number of financially independent people. We believe we can achieve this goal by building tools that enable independent business owners to scale their businesses profitably. Our first product combines a virtual credit card system with dynamic spending limits and software tooling to help merchants grow and optimize their profitability.
We are growing very fast -- in less than five months, we grew to millions in card volume. We have a significant waitlist of customers waiting to use our product. We are looking to expand our headcount quickly to support the demand. Our investors include Solomon Hykes founder of Docker, Paul Buchheit founder of Gmail, Paul Graham founder of Y Combinator, Robert Leshner founder of compound.finance, and many more. We have raised over 30M from top-tier fintech investors.
Basic Qualifications

Minimum of three years of experience with modern data engineering technologies such as S3, Redshift, SQL, DBT, Python, etc.
Interest in building robust pipelines that will be mission critical for our business
Proven abilities in developing your own scalable, productsapplications from scratch

Who you are

A passionate, determined, and curious thinker who thinks about problems from an engineering perspective
Desire to work in a fast-paced environment, continuously grow and master your craft
Strong communicator who can articulate your ideas clearly and participate in making difficult decisions
A hacker with general interest in learning about new technologies and coming up with solutions to unsolved problems

Nice to haves

Experience working within payments or at a fintech companyon a fintech project
Experience and subject matter expertise in the e-commerce industry
Interestcontributions to open source software

Compensation

The annual salary for this role in NYC is 150,000 - 204,000. We also offer start-up equity along with health, dental, and vision benefits.




"
https://startup.jobs/data-engineer-off-cycle-zelus-analytics-4260324,Engineer,Data Engineer (Off-cycle),Zelus Analytics ,,,"

We are not actively hiring. This job post is to submit your resume off-cycle.
Thank you for your interest in Zelus Analytics. We are always inspired to hear of individuals who are as passionate about sports analytics as we are! Even when we are not in a specific hiring cycle, we welcome folks who are pursuing a data engineering career in sports analytics to submit their resume for future consideration!
We seek data engineers with a passion for sports to develop cloud-based data pipelines and automated data processing for our world-class sports intelligence platforms in baseball, basketball, cricket, football American, hockey, soccer, and tennis. Through your work, you can support the professional teams in our exclusive partner network in their efforts to compete and win championships. We often have both entry-level and senior positions available, allowing us to consider qualified candidates with a wide range of experience levels.
Zelus Analytics unites a fast-growing startup environment with a research-focused culture that embraces our core values of integrity, innovation, and inclusion. We pride ourselves on providing meaningful mentorship that offers our team the opportunity to develop and expand their skill sets while also engaging with the broader analytics community. In doing so, we hope to create a new path for a more diverse group of highly talented people to push the cutting edge of sports analytics.
We believe that a diverse team is vital to building the worlds best sports intelligence platform. Thus, we strongly encourage you to apply if you identify with any marginalized community across race, ethnicity, gender, sexual orientation, veteran status, or disability. At Zelus, we are committed to creating an inclusive environment where all of our employees are enabled and empowered to succeed and thrive.
As a Zelus Data Engineer, you will be expected to

Design, develop, document, and maintain the schemas and ETL pipelines for our internal sports databases and data warehouses
Implement and test collection, mapping, and storage procedures for secure access to team, league, and third-party data sources
Develop algorithms for quality assurance and imputation to prepare data for exploratory analysis and quantitative modeling
Profile and optimize automated data processing tasks
Coordinate with data providers around planned changes to raw data feeds
Deploy and maintain system and database monitoring tools
Collaborate and communicate effectively in a distributed work environment
Fulfill other related duties and responsibilities, including rotating platform support

In addition to the above, a Senior Data Engineer will be expected to

Break down complex data engineering projects into actionable work plans including proposed task assignments for one to four engineers
Identify and recommend new ETL tools and novel data sources to push the cutting edge of our sports intelligence platforms
Provide guidance and technical mentorship for junior engineers 
Assist with recruiting and outreach for the engineering team, including building a diverse network of future candidates

A qualified entry-level candidate will be able to demonstrate several of the following and will be excited to learn the rest through the mentorship provided at Zelus

Academic andor industry experience in back-end software design and development
Experience with ETL architecture and development in a cloud-based environment
Fluency in SQL development and an understanding of database and data warehousing technologies
Proficiency with Python preferred, Scala, andor other data-oriented programming languages
Experience with automated data quality validation across large data sets
Familiarity working with Linux servers in a virtualizeddistributed environment
Strong software-engineering and problem-solving skills

A qualified senior candidate will be able to demonstrate all of the above at a higher level of competency plus the following

Expertise developing complex databases and data warehouses for large-scale, cloud-based analytics systems
Experience with task orchestration and workflow automation tools Airflow preferred
Experience building and overseeing team-wide data quality initiatives
Experience adapting, retraining, and retooling in a rapidly changing technology environment
Desire and ability to successfully mentor junior engineers

Zelus has a fully distributed workforce, spanning thirteen states and seven countries as of the end of 2022. In addition to competitive salaries, our compensation packages include equity and benefits, such as an annual incentive bonus plan and flexible PTO, that allow us to attract and retain a world-class team.
As an equal opportunity employer, Zelus does not discriminate on the basis of race, ethnicity, color, religion, creed, gender, gender expression or identification, sexual orientation, marital status, age, national origin, disability, genetic information, military status, or any other characteristic protected by law. It is our policy to provide reasonable accommodations for applicants and employees with disabilities. Please let us know if reasonable accommodation is needed to participate in the job application or interview process.
Zelus is an at-will employer; employment at Zelus is for an indefinite period of time and is subject to termination by the employer or the employee at any time, with or without cause or notice.
Pay 75,000.00 - 150,000.00 per year

"
https://startup.jobs/data-engineer-mid-to-staff-commercehub-4259834,Engineer,"Data Engineer, Mid to Staff",CommerceHub ,,,"

Our mission is to connect and optimize the worlds commerce. That means the whole world. So were determined to nurture our culture of meritocracy where everyone can thrive, no matter what we look like, where were from, how we grew up, whom we love, the nature of our faith, or how our bodies or minds work. Were committed to achieving equity in treatment and opportunity for everyone, where people are judged on the merits and quality of their work. 
It all starts with people. Inside every company, behind every brand - while business success is often measured in profit, it has always been powered by people. We firmly believe people are the heart of any organization - including our own. Thats why a career here provides much more than simple pay and perks. Were dedicated to empowering people, solving tough problems, and helping careers flourish inside and out.  
  
Position Summary 
The ideal candidate is an experienced data pipeline builder who enjoys optimizing data systems and building them from the ground up. This role requires someone who is comfortable working across the organization to guide and support our engineers, data scientists, data analysts and product teams. The right candidate will be excited by the prospect of designing our companys long term cloud data architecture from the ground up to support our products and data initiatives. 


Implement and maintain a data architecture built around automated ingestion, data security, compliance, and governance. 


Implement the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS data primitives. 


Work with stakeholders including the Product, BI and Design teams to assist with data- related technical challenges and support their data infrastructure needs. 


Develop both functional and non-functional requirements. 


Support analytics through tools, processes, and pipelines. 


Build and maintain the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of sources 


Participate in full development life cycle including requirements development, implementation, peer review, source control, automated testing, deployment, and operations 


Drive corporate policies around the evolution and enforcement of industry data standards and best practices 


Mentor other data engineers as well as engineers and business leaders in all aspects of data management, lifecycle, security and compliance. 


  
Requirements 
Minimum Qualifications 


Bachelors degree or higher in Computer Science andor equivalent work experience. 


nalytics skills related to working with both structured and non-structured datasets. 


5 years experience with AWS cloud services such as SQS, Kinesis, S3, Athena, Redshift. 


5 years experience with object-oriented scripting languages such as Python, Scala. 


10 years in leadership roles. 


Preferred Qualifications 


5 years working at a SAAS company 


Strong working knowledge of message queuing, stream processing and highly scalable data stores such as Amazon Redshift. 


Exceptional written and verbal communication skills. 


Comfortable communicating across all levels of management. 


Ability to prioritize tasks and work independently. 


Excellent analytical, decision-making, and problem-solving skills. 


Proven ability to work in a rapidly changing environment with keen attention to detail. 


  
What its like to work at ChannelAdvisor, a CommerceHub Company 
We take a whole-person approach to engage and support our global team. We believe the diversity of our global team is an advantage. If youre curious, innovative, determined, and customer-focused, then youll love the challenge and rewards of collaborating as a team to help our customers win. We offer competitive compensation programs that recognize your hard work and results. Because when our customers win, we win.  And when we win, you win. 
We work to create an environment where everyone who is committed, works hard, and delivers results can thrive and grow. You can connect with one of our employee resource groups and support our diversity, equity and inclusion task force, network with like-minded team members, and showcase your leadership skills.  
  
Benefits  

Medical coverage provided through Irish Life Health; premiums paid by the company
Competitive time off package with 23 Days of PTO, 2 Wellness days and 1 Give Back Day
PTO raises to 25 Days after 2 years
Flexibility to choose where you work - at home, in the office, or both! 
Access to tools to support your wellbeing such as the Calm App, MoveSpring and an Employee Assistance Program
Professional development stipend and learning and development offerings to help you build the skills and connections you need to move forward in your career.
Charitable contribution match per team member

  
ChannelAdvisor, a CommerceHub Company, is an Equal Employment Opportunity Employer. We celebrate diversity and are committed to providing an environment of mutual respect where equal employment opportunities are available to all applicants and teammates without regard to race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment is decided on the basis of qualifications, merit, and business need. 

"
https://startup.jobs/data-engineer-cross-river-4256857,Engineer,Data Engineer,Cross River ,"Fort Lee, United States",,"

Who We Are

Cross River is a highly profitable, fast-growing financial technology company powering the future of financial services. Our comprehensive suite of innovative and scalable embedded payments, cards, and lending products deliver financial services for millions of businesses and consumers around the globe. Cross River is backed by leading investors and serves the worlds most essential fintech and technology companies. Together with its partners, Cross River is reshaping global finance and financial inclusion. 
We are on a mission to build the infrastructure that propels access, inclusion, and the democratization of financial services. While our company has tripled in size over the last three years, our strong sense of purpose led Cross River to be named to American Bankers list of Best Places to Work in Fintech for the last 6 years. The reason for this success is simple  our nimble, and collaborative family culture lives in every member of our growing team. Together we are at the forefront of technology and innovation, and we invite passionate, collaborative, and motivated, high performers to join our expanding team.
What Were Looking For
The Data and Analytics team builds and operates systems to centralize all of the organizations internal and third-party data, making it easy for teams across the company to access, process, and transform that data for analytics, and powering end-user experiences. As an engineer on the team you will contribute to the full spectrum of our systems, from managing foundational processing and data storage, to building and maintaining scalable pipelines, to developing frameworks, tools, and internal applications to make that data easily and efficiently available to other teams and systems. As such, you will play a key role in orchestrating and automating the data ingestion pipeline to make it more flexible while maintaining a high level of quality. The ideal candidate will have worked in a data engineering or science role, preferably at an investment bank or at a fintech and will have a basic understanding of consumer credit and related securitized products. You must have a strong sense of ownership, passion for technology and automation, self-starter, and thrive in a fast paced, unstructured startup environment.
Responsibilities

Design, build, and operate our foundational data infrastructure storage cloud data warehouse, S3 data lake, orchestration Airflow, processing Spark, Flink, streaming services Kafka, BI tools Tableau, graph databases, and real-time event aggregation stores.
Design and build services for end-to-end data security and data governance managing access controls across multiple storage and access layers, tracking data quality, cataloging datasets and their lineage, usage auditing
Implement new features in our high-volume, in-house analytics service, and work closely with our customers to understand and solve their various use
Manage RedshiftEMR infrastructure, and drive architectural plans and implementation for future data storage, reporting, and analytic solutions
Write, test and review microbatch or streaming ETL powered by Kafka or AWS Kinesis
Leverage your Python, Airflow, and AWS expertise to scale our Airflow data pipelines to the next 10x level.
Lead, mentor, and coach junior data engineers

Qualifications

8 years of experience working as a Data Engineer role supporting ETL data pipelines and workflow orchestration tools eg. Airflow
5 years experience working on batch and streaming data applications Spark, Hadoop, Kafka, Kinesis, and Flink
You enjoy working with dynamic programming languages, relational databases, and distributed systems. Our platform is ever-evolving, but is a combination of Python, Scala, Postgres, Kubernetes, Spark, Cassandra, Kafka, and MongoDB
Strong analytical and technical skills to troubleshoot issues, analyze the cause, quickly come-up with the possible solutions, document the changes, and communicate organizational impact
Demonstrate sense of urgency in maintaining data integrity and experience automating recurring analyses
Proactively seek to automate data operations and collaborate with cross functional teams to improve processes
Excellent verbal communication skills and attention to detail
Excited to learn new technologies and data transformationanalysis in general

Bonus

Interested in learning more about consumer lending analytics and the general finance domain
Working knowledge of corporate banking and capital market products

 
LI-JO1
Salary Range 140,000.00 - 160,000.00

Cross River is an Equal Opportunity Employer. Cross River does not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment is decided on the basis of qualifications, merit, and business need.
By submitting your application, you give Cross River permission to email, call, or text you using the contact details provided. We will only contact you with job related information.


"
https://startup.jobs/data-engineer-aqemia-4252366,Engineer,Data Engineer,Aqemia ,"Paris, France",Full-Time,"

At Aqemia you will work in a multi-disciplinary team of passionate drug hunters, AI engineers and developers who are committed to our mission of finding many drugs, at high pace, to cure diseases.

As part of our growing team, you will enjoy a fast-paced, challenging, science-driven and creative environment, working at the very forefront of AI  Deep physics-powered drug discovery. This is a tremendous opportunity to bring your own impact on changing the way medicines are discovered and be involved in shaping the direction of our fast growing business and team.

We are looking for highly skilled and collaborative individuals who are naturally curious, have a passion for learning and solving complex problems with a can-do mindset.

If this sounds exciting to you, come and join us!

The difference youll make

As a Data Engineer, you will join the Data Engineering Team and contribute to design and build a modern, reliable and scalable Data Platform for Aqemias engineering teams. You will also support engineering Teams to build their Data pipeline and assets. This way, you will be instrumental in all engineering teams success.
What youll do

Contribute in defining the relevant Data Architecture and stack for Aqemia
Contribute to build the relevant Data infrastructure for Aqemia in AWS
In a data mesh oriented organization provide engineering teams with the right tools and practices to build their own pipelines and data assets
Support engineering team in designing their Data pipelines and assets
Bring the Data Engineering expertise in engineering projects from design to delivery

Your profile

4 years experience as a Data or Software engineer in an engineering team of 4 engineers
Knowledge of Cloud infrastructure and products AWS, other cloud experience is a plus
Good knowledge of Data Engineering building blocks storage, orchestrator
Fluent in object-oriented language development ideally python
Experience in delivering technical projects from start to finish

Preferred skills
Proficient in SQL
Experience in backend engineering
Experience in infrastructure-as-code techniques ideally Terraform
Knowledge in ML Ops and DevOps
Knowledge of Kubernetes, K8s administration
You know how to interact with technical stakeholders

Who you are

You are eager to play an active role in contributing to Aqemias strategy to develop drugs for patients.
You are anxious to bring your wealth of knowledge and skills to the table to inspire and coach brilliant people from diverse backgrounds.
You are keen to solve tough problems on issues that truly matter.
You are inquisitive, and proactive with a can-do attitude.
You are excited to join a small team and make your mark on drug discovery.
You thrive on working collaboratively in a fast-paced, interdisciplinary environment that keeps everyone on track.


Our Workplace Environment
- Fast-paced, intellectually and scientifically demanding, results-driven.
- Our Founders boast  10 years experience in research at Ecole Normale Suprieure in Paris, not to mention a stint in Oxford and Cambridge  10 years experience in strategy consulting at BCG.
- Aqemia has a rapidly growing team of 50 people from world-class institutions AstraZeneca, GSK, Sanofi, Harvard, Ecole Normale Suprieure, Ecole Polytechnique, BCG
- Our premises are conveniently located in the center of Paris 1 Bd Pasteur, with a possibility of up to 2 days of remote work.
- We are part of the French Tech 2030 program httpslafrenchtech.comfrla-france-aide-les-startupfrench-tech-2030.
- Working language English

We are growing fast, if you feel that you dont fit this job description but youre still excited to join, then please get in touch! 

Aqemia is an Equal Employment Opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion or belief, sex, sexual orientation, gender perception or identity, national origin, age, marital status, disability status or any other basis under applicable law.

"
https://startup.jobs/data-engineer-wildlife-studios-4249660,Engineer,Data Engineer,Wildlife Studios ,"São Paulo, Brazil",,"

Were looking for a talented and passionate Data Engineer, to join Wildlifes BI for Live Games team.
As a Data Engineer, youll play a key role in enabling all of the Analytics for the Live Games, ensuring our systems, services, and pipelines are running efficiently and effectively. You will work directly with the Analytics teams to understand their data requirements, propose technical solutions and implement end-to-end pipelines to deliver the best results.
We know that the work we do has a high impact on our companys success and culture. The right person for this position is curious by nature, comfortable in a ""take the initiative"" environment, love solving problems, and can thrive in a fast and growing business. 
What youll do

Be part of a cross-functional and highly skilled team, directly contributing to feature development and impactful results;
Partner with colleagues to build, maintain and improve data pipelines;
Actively participate in decision-making within the team and with internal clients, proposing ideas and solutions then implementing them from start to end;
Have a direct contribution to the Live Games success, providing quality data that can be used for fast decision-making and can influence the direction a feature will take;
Collaborate with amazing individuals and professionals, in an area with good support for development and growth.

What youll need

Spark PySpark and SparkSQL. SQL knowledge is critical to the success of the role. 
ETL Pipelines, Airflow. We orchestrate all our jobs, and ensuring they are well defined, structured and organized is super important.
Data architecture, AWS or similar cloud provider S3 storage, working with clusters, etc. The role requires working knowledge of how the data will move around different structures until it reaches the final output.
Analytical skills are great to have for this role, as they will help you interact better with the Business Analytics teams.
University degree complete related to computing such as Computer Engineering, Computer Science, Information Systems, and Systems Analysis and Developmentor or related experience.

More about you

You are motivated by technical challenges. You thrive in situations where you need to go deep in a subject and get your hands dirty. You love working with data, creating pipelines and delivering results that directly contribute to the success of the company.
You are curious and want to grow in your role. You work well with others as well as on your own.

About Wildlife
Wildlife is one of the leading mobile game developers and publishers in the world. We have released more than 60 titles, reaching billions of people around the globe. Today, we have offices in Brazil, Argentina, Ireland, and the United States. Here, we create games that will excite, intrigue, and engage our players for years to come!
Equal Opportunity
Wildlife is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, colour, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state, and local law.
Were committed to providing accommodations for candidates with disabilities in our recruiting process. 

"
https://startup.jobs/data-engineer-yakoa-4248074,Engineer,Data Engineer,Yakoa ,"San Francisco, United States",Full-Time,"

Were looking for a forward-thinking, structured problem solver, and technical specialist passionate about building systems at scale. You will be among the first to tap into massive blockchain datasets, to construct data infrastructure that makes possible analytics, data science, machine learning, and AI workloads.
As the data domain specialist, you will partner with a cross-functional team of product engineers, analytics specialists, and machine learning engineers to unify data infrastructure across Yakoas product suite. Requirements may be vague, but the iterations will be rapid, and you must take thoughtful and calculated risks. Your work will take place at the interface of the AI, blockchain, and intellectual property domains, so you must be a quick learner with a thirst for many types of knowledge.
Responsibilities
 Design, build, test, and maintain scalable data pipelines and microservices sourcing both first-party and third-party datasets and deploying distributed cloud structures and other applicable storage forms such as vector databases and relational databases. Index multiple blockchain data standards into responsive data environments, and tune those environments to power real-time query infrastructure. Design and optimize data storage schemas to make terabytes of data readily accessible to our API. Build utilities, user-defined functions, libraries, and frameworks to better enable data flow patterns. Utilize and advance continuous integration and deployment frameworks. Research, evaluate and utilize new technologiestoolsframeworks centered around high-volume data processing. Mentor other engineers while serving as technical lead, contributing to and directing the execution of complex projects. 
Requirements
 4 years working as a data engineer. Proficient in database schema design, and analytical and operational data modeling. Proven experience working with large datasets and big data ecosystems for computing spark, Kafka, Hive, or similar, orchestration tools dagster, airflow, oozie, luigi, and storageS3, Hadoop, DBFS. Experience with modern databases PostgreSQL, Redshift, Dynamo DB, Mongo DB, or similar. Proficient in one or more programming languages such as Python, Java, Scala, etc., and rock-solid SQL skills. Experience building CICD pipelines with services like Bitbucket Pipelines or GitHub Actions. Proven analytical, communication, and organizational skills and the ability to prioritize multiple tasks at a given time. An open mind to try solutions that may seem astonishing at first. An MS in Computer Science or equivalent experience. 

Exceptional candidates also have
 Experience with Web3 tooling. Experience with artificial intelligence, machine learning, and other big data techniques. B2B software design experience. 

No crypto or Web3 experience? No problem! Well help coach you and cover any costs for educational materials for your growth.
Benefits
 Unlimited PTO.  Competitive compensation packages.  Remote friendly  flexible hours.  Wellness packages for mental and physical health.  

"
https://startup.jobs/data-engineer-ml-ops-emplifi-external-posting-4237224,"Engineer,Machine Learning",Data Engineer - ML Ops,Emplifi (external posting) ,"Praha 2, Czechia",,"

As a Data Engineer - MLOps at Emplifi you will work on both Data Engineering and MLOps projects and will be responsible for several batch and streaming data pipelines. You will be working on our ML serving platform built on top of MLflow and you will make sure that our models run reliably and scale well. You are a software engineer at heart!
 
What Youll Do Here
Team

Participate in the productionalization of ML models, e.g. implement streaming pipeline which detects sentiment in various languages and processes large amounts of data on a daily basis
Take ownership of ML tooling
Go the extra mile to deliver code of exceptional quality, i.e. make sure the code is tested, scalable and maintainable according to best practices while also reasonably optimized
Willingness and drive to learn all technologies required for the delivery of our service ScalaPythonAkkaSpark - curious personality is a must!
Be a good communicator and team player, help other team members when appropriate, seek support first rather than try to solve problems on your own

Stakeholders

Proactively communicate with stakeholders, announce changes and give status updates
Make sure we deliver products and data of high quality that are robust, fast and that satisfy our stakeholders needs
Take responsibility for applications that other teams are relying on

What Youll Bring to Us

Solid professional experience in software engineering and an interest in ML
Languages excellent knowledge of OOP andor FP in Scala, Python or Java
Very good understanding of the SQL syntax, databases, distributed systems and streaming PubSub
Knowledge and experience in various levels of software testing
Knowledge of NLP and computer vision is nice to have
Technologies not all required! MLflow, Apache Spark, Akka Streams, Rabbit MQ, AWS Kinesis, AWS S3, ElasticSearch
Technical academic background statisticalmathematicalcomputer science
Proactive approach in solving problems
English language around B2 level

What We Offer

International, fast paced and rapidly growing environment
Opportunity for professional growth and development 
Possibility to learn new and cutting edge technologies, in an environment that encourages new ideas
Work in an international environment in our new modern offices in Karln, with our big terrace and our own grill 
Unlimited PTO
Multisport card
Home office working
Theres more as well! Speak with us to find out all details!


"
https://startup.jobs/data-engineer-arabic-computer-systems-4234399,Engineer,Data Engineer,Arabic Computer Systems ,"Riyadh, Saudi Arabia",,"

We are looking for a data engineer to join our team. You will use various methods to transform raw data into useful data systems. Overall, youll strive for efficiency by aligning data systems with business goals. To succeed in this data engineering position, you should have strong analytical skills and the ability to combine data from different sources. Data engineer skills also include familiarity with several programming languages and knowledge of learning machine methods. If you are detail-oriented, with excellent organizational skills, wed like to hear from you. 

Responsibilities
 Analyze and organize raw data  Build data systems and pipelines Evaluate business needs and objectives Interpret trends and patterns Prepare data for prescriptive and predictive modeling Combine raw information from different sources Explore ways to enhance data quality and reliability Identify opportunities for data acquisition Collaborate with data scientists and architects on several projects 
Requirements
 Technical expertise with data models, data mining, and segmentation techniques Knowledge of programming languages e.g. Java, Python Hands-on experience with SQL database design Great numerical and analytical skills Degree in Computer Science, IT, or similar field 
Benefits
 Private Health Insurance Training  Development Paid Time Off 

"
https://startup.jobs/data-engineer-biofourmis-2-4231103,Engineer,Data Engineer,Biofourmis ,,,"

Biofourmis brings the right care to every person, no matter where they are. The companys AI-driven solution collects and analyzes patient data in real time and identifies shifts that require proactive interventions. This vital innovation provides people everywhere with connected access to hospital-level services, virtual provider networks for remote care, and life-changing clinical trialsall without leaving their homes. Trusted by leading health systems, payers, biopharma companies and patients alike, Biofourmis connected platform improves patient outcomes, prevents hospital readmissions, accelerates drug development, and closes critical gaps in careultimately making science smarter, healthcare simpler, and patients healthier. Biofourmis is a global technology company enabling care delivery, with headquarters in Boston and key offices in Singapore and India.
Job Summary
Biofourmis is a digital therapeutics company that pioneered and is the leader in Personalized Predictive Care. Our disruptive turnkey technology uses advanced clinical grade wearable sensors to continuously monitor bio vitals and process them using our patented and FDA approved AIML algorithms to predict changes in physiology that are co-related to medical and disease events in the cardiac, oncology, respiratory, and other therapeutic areas.
Biofourmis technology is applicable to a multitude of different therapeutics areas. We are building out a dramatically expanded of solutions to address heart failure, oncology, and infectious diseases. The Data engineer position will be responsible for handing the existing cloud based Data lake and build next version of analytics Datalake with advanced technologies.
Responsibilities

Creation and maintenance of optimal Data lake pipeline architectures.
Stay abreast of industry trends and enable successful data solutions by leveraging best practices.
Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.
Partnering effectively with inhouse Products, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources and AWS big data technologies.
Assemble large, complex data sets that meet functional  non-functional business requirements.
Keep our data separated and secure within national boundaries through multiple AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.


 

Education

BachelorMasterEngineering in ITComputer sciencesoftware engineering or relevant experience.

Experience  Training
 

3 years of experience in AWS cloud AWS solution architect  AWS Certified Data analytics certification
will be preferable
5 years of experience in software engineering and Big Data Analytics.
Prior experience on AWS cloud services EC2, Glue, Athena, S3, EKS, RDS, Redshift, Data pipeline, EMR, DynamoDB, cloud watch.
Experience in creating and maintain Data lake on AWS cloud.
Experience in Big Data analytics tools like Hadoop, Spark, Kafka etc...
Strong experience in collecting data from different source systems and create ETL pipelines to handle complex data sets  uncertain schema changes in data.
Strong experience in Python programming and analytics libraries like Pandas, NumPy etc...
Strong experience on Analytics skills and complex SQL based queries implementation.
Data engineer also need to very passionate about efficientaccurate code development, optimising performance of organization Data lake.
Good experience in UNIX based shell scripting.
Support to Data scientist team for data availability, extract  provide required data sets.
Coordinating with various teams and clients to provide data based on specific requirements.


"
https://startup.jobs/data-engineer-turtl-4224631,Engineer,Data Engineer,Turtl ,"Ljubljana, Slovenia",Full-Time,"

Turtl is an exciting software company with more than 300 customers around the world and 120 employees based in London and Boston. Our software enables anyone to create, personalize, publish and track digital documents - with no need for specialist design or coding skills. Turtl is used by businesses of all sizes, from small organizations to big names, such as Cisco, Nestle and Lexus. We think theres huge potential for growth given the wide applicability of our software and the clear benefits were hearing from customers. Its a fast-paced work environment, so were looking for talented people who want to continuously learn and actively embrace challenges. Youll find Turtl a straightforward and open place to work, where colleagues can be relied on to help. If youre ready to take that next step in your career, then its a great time to be joining the team!
THE ROLE
Building efficient systems to collect, manage and process raw data into high quality and reliable data sets while helping evolve our modern data stack to meet the requirements of the business. Working together with the Head of Data and rest of the team you will help define and deliver the data roadmap for Turtl, the business who is putting Digital Document Performance at the heart of every business.
WHAT YOULL DO
 Evolve existing data ingestion pipelines, including ensuring data quality and monitoring of services to serve our SaaS product. Develop existing capabilities for ingesting and processing data from multiple external services our business is using in different teams.  Centralise all sources, including OLTP databases, into our data warehouse on Databricks and ensure it evolves into a clean and usable data set, maintaining historical transaction logs. Develop a semantic layer over the data sets. Prepare the necessary data sets for machine learning and insight generation. Develop a stable and scalable reverse ETL solution. Develop the necessary systems to evolve our customer facing analytics product. 
Requirements
 You are an experienced Data engineer who has experience in building pipelines to ingest, process and store data in data warehouses and lakes in continuous, reliable and optimized manner for fast querying and easy discoverability for the rest of the data team.  You have strong foundation in using Spark using either PySpark or Scala, SQL and preferably have Databricks experience. Proven experience working in a multi-functional team with practices such as automated testing, code review and QA. Ability to quickly absorb business context and connect the dots across the organization, understanding the business vision and how this translates into our roadmap. A passion for defining and scoping projects before jumping into rabbit holes. Understanding of your own rhythm of work and what influences it. Clear communication and appreciation for receiving and offering constructive feedback. 
Benefits
 A competitive base salary between 45,000 - 70,000 euros depending on experience. Stock Options Mature team with well defined and simple processes meant to empower our work Appreciation of engineering practices such as automated testing, infrastructure as code, CICD 

EQUAL OPPORTUNITIES STATEMENT
Turtl is an equal opportunity employer and are committed to growing a diverse workforce that represents all people regardless of race, ethnicity, religion, age, gender identity or expression, sexual orientation, disability or neurodiversity. We encourage applications from all backgrounds and will make any recruitment or interview adjustments that will ensure a comfortable candidate experience.

"
https://startup.jobs/data-engineer-business-systems-notion-4222613,Engineer,"Data Engineer, Business Systems",Notion ,"San Francisco, United States",,"

About Us
Were on a mission to make it possible for every person, team, and company to be able to tailor their software to solve any problem and take on any challenge. Computers may be our most powerful tools, but most of us cant build or modify the software we use on them every day. At Notion, we want to change this with focus, design, and craft.
Weve been working on this together since 2016, and have customers like Pixar, Mitsubishi, Figma, Plaid, Match Group, and thousands more on this journey with us. Today, were growing fast and excited for new teammates to join us who are the best at what they do. Were passionate about building a company as diverse and creative as the millions of people Notion reaches worldwide.
Notion is an in person company, and currently requires its employees to come to the office for two Anchor Days Mondays  Thursdays and requests that employees spend the majority of their week in the office including a third day.
About The Role
As Notion continues to grow quickly, there is a unique opportunity to build the foundations of data and help the product and company reach their full potential. The Data Engineering team is responsible for designing and building reliable, trusted and timely datasets that accelerate the decision-making process of key product and business functions.
As a Data Engineer focusing on our Business Systems, you will serve as a strategic thought-partner to drive Notions critical go-to-market strategy. You will design Notions integrated system architecture to ensure it aligns well with the business organizational structure and processes. You will be a critical part of building the foundation of Notions integrated tech stack, bringing your expertise and best practices to design and develop solutions that will allow us to automate critical data hand offs and enable us for hyper-growth.
You will work closely with Business Technology and Data Science teams, as well as other business stakeholders, to build bridges across data silos, and implement innovative solutions to unlock our business goals. As an early member of the team, your career growth will be boundless!
What Youll Achieve

Enable workflows across siloed systems by integrating elements of application layers and build a longer term strategy for data governance across Notions internal tools
Understand Notions data across applications to design and build technical solutions for complex problems that require an in-depth understanding of integrated business processes
Partner with Notions Business Technology Systems Managers and Data Scientists to build pipelines from our data warehouse to power crucial customer and product data requirements in our internal tools
Define processes and ETL infrastructure to transform and make data readily available across the company
Implement automated workflows that lower manualoperational cost for stakeholders, define and uphold SLAs for timely delivery of data, and move the company closer to data democratization

Skills Youll Need to Bring

3 years of experience in hands-on development, focused on integration of data models across a variety of sales, marketing and finance enterprise SaaS tools
Ability to analyze technical business requirements, including functional and non-functional requirements, to architect and develop integrations across critical applications
In-depth knowledge of SQL and object-oriented programming paradigms e.g Python, Java, Scala to efficiently manipulate large volumes of data
Creative and analytical thinker who can facilitate resolution to key design decisions in complex, integrated systems projects
Strong communication and collaboration while working in cross-functional environments

Nice to Haves

Experience integrating data with Salesforce, Marketo, Stripe, andor Netsuite
Worked at a fast-growing start-up, a SaaS company or are eager to contribute in such an environment being a current Notion user would also be great!
Hands-on experience in designing and building highly scalable and reliable data pipelines using BigData stack e.g Airflow, DBT, Spark, Hive, ParquetORC, ProtobufThrift, etc

We hire talented and passionate people from a variety of backgrounds because we want our global employee base to represent the wide diversity of our customers. If youre excited about a role but your past experience doesnt align perfectly with every bullet point listed in the job description, we still encourage you to apply. If youre a builder at heart, share our company values, and enthusiastic about making software toolmaking ubiquitous, we want to hear from you.
Notion is proud to be an equal opportunity employer. We do not discriminate in hiring or any employment decision based on race, color, religion, national origin, age, sex including pregnancy, childbirth, or related medical conditions, marital status, ancestry, physical or mental disability, genetic information, veteran status, gender identity or expression, sexual orientation, or other applicable legally protected characteristic. Notion considers qualified applicants with criminal histories, consistent with applicable federal, state and local law. Notion is also committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, please let your recruiter know.
Notion is committed to providing highly competitive cash compensation, equity, and benefits. The compensation offered for this role will be based on multiple factors such as location, the roles scope and complexity, and the candidates experience and expertise, and may vary from the range provided below. For roles based in San Francisco or New York City, the estimated base salary range for this role is 130,000 - 230,000 per year.
LI-Onsite

"
https://startup.jobs/data-engineer-securrency-4219606,Engineer,Data Engineer,Securrency ,"Raleigh, United States",Full-Time,"

Securrency is a financial markets infrastructure technology company focused on enhancing capital formation and stimulating global liquidity. Securrency is driving change at the core of financial services via a patent-pending distributed identity and compliance framework and a state-of-the-art infrastructure designed to bridge legacy financial platforms to blockchain networks. One of the industrys most advanced regulatory technology providers, we have developed compliance tools that automate enforcement of the multi-jurisdictional regulatory policy. These tools provide transparency and consistency to strengthen investor confidence and provide regulators with increased oversight of the market activity. Securrency provides software-as-a-service SaaS and platform-as-a-service PaaS delivery models to offer blockchain-based financial services infrastructure to banks and other financial services providers.

Our proprietary, patent-pending Compliance Aware Token technology provides multi-jurisdictional compliance and unprecedented convenience to financial services providers and market participants to facilitate the issuance, trading, and servicing of digital securities and other digital assets. Securrencys technology is blockchain-agnostic, and its compliance and policy-enforcement tools support ledger-to-ledger transactions across multiple blockchains.

We have built a state-of-the-art blockchain-based financial service and compliance platform that will serve as the global rails along which all future value moves in a transparent and interoperable manner. Well, on its way to being a technology unicorn, but while we are growing rapidly, we still retain the spirit and camaraderie of a dynamic start-up.

Job Purpose
As a data engineer, your primary responsibility is to design, develop, maintain, and test data pipelines and infrastructure that enable efficient, secure, and scalable data processing and analysis. You will work closely with data scientists, analysts, and stakeholders to understand their data requirements and design and implement data solutions that meet those requirements. Additionally, you will develop and implement data architecture strategies and best practices, manage and maintain large and complex data sets and databases, and identify and resolve data pipeline and infrastructure issues. You will also stay up-to-date with emerging technologies and trends in data engineering and provide technical guidance and mentorship to junior data engineers and data analysts. Finally, you will participate in data governance initiatives, ensure compliance with relevant data regulations and standards, and collaborate with cross-functional teams to ensure alignment with broader organizational goals and initiatives.

Responsibilities
 Work with large, complex data sets and high throughput data pipelines that meet business requirements. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources. Build data and analytics tools that utilize the data pipeline to provide actionable insights to operational efficiency and other key business performance metrics. Work with internal and external stakeholders to assist with data-related technical issues and support data infrastructure needs. Collaborate with data scientists and architects on several projects. Solve various complex problems. 
Requirements
 Previous experience as a data engineer or in a similar role 5 years of Python development experience is necessary. Hands-on experience with database technologies e.g. SQL and MongoDB Technical expertise with distributed Spark or other distributed data processing technologies Experience with machine learning techniques Great numerical and analytical skills Ability to write reusable code components. Degree in Computer Science, IT, or similar field. Open-minded to the new technologies, frameworks Thorough business analysis skills 
Would be a plus
Understanding Blockchain system mechanism
Benefits
 Amazing and accessible office locations in UAE and USA Competitive compensation package World-class benefits package Global company events Flexible working hours Employees may work remotely for a maximum of 40 days a year Eligible to work from alternative Securrency locations 65 days a year Exposure to industry thought leaders 

"
https://startup.jobs/data-engineer-global-revenue-informatics-verkadahq-4218179,Engineer,"Data Engineer, Global Revenue Informatics",Verkada ,"San Mateo, United States",Full-Time,"

Who We Are
At Verkada, were building the operating system for the physical world. 

We started in 2016 with video security cameras and an intuitive cloud-based platform, and in just six years, weve expanded to include five additional product lines access control, environmental sensors and alarms, as well as visitor and mailroom management that help enterprises to protect and manage their people and assets. 

Today more than 14,000 organizations across 63 countries worldwide trust Verkada as their physical security layer for easier management, intelligent control, and scalable deployments. We are valued at 3.2 billion, with 360 million raised in venture capital from investors including Linse Capital, MSD Partners, L.P., Felicis Ventures, Next47, Sequoia Capital, Meritech Capital, and First Round Capital. 

This is just the beginning. The need for businesses to secure and manage their people and physical assets with scale and speed is mission critical and continues to increase. Our teams are energized by the complex challenges we work on and the platform we are building to make the schools, hospitals, businesses and communities that we live and work in safer.

About Verkada
Verkada is building a one-of-a-kind B2B enterprise sales machine. With a sales team of over 900 around the world, we are changing the game in the 100B physical security market. We hire the best, from new grads to top performing veterans, and it shows in our results. That caliber of team requires accurate, complete and insightful data to allow them to perform their best. As we scale, we need more and better data to inform how we segment our customers, prioritize our efforts, decide where to expand and how to grow our organization. We want to invest to turn data into a core competitive advantage, and this is an amazing opportunity to build out a best-in-class data function.

Data Engineer, Sales Strategy and Operations
You will be joining the Sales Strategy and Operations team as a high impact contributor, gaining exposure to a wide variety of challenges, processes, and data that makes the engine of this Sales organization hum. As a Data Engineer on the team, you will play an integral role in leveraging our data and infrastructure to help our global sales team run better, faster, and smarter.
Why Data? 

Data is the foundation of our go-to-market strategy. It supports all parts of our sales motion, including 
Developing customer profiles to identify  target high value buyers 
Identifying  facilitating key growth opportunities, e.g., entering markets in previously untapped geographies 
Segmenting our sales team and defining sales representative patches
Populating and maintaining an accurate universe of buyers 
We recognize the importance of data to our scalability and success, and we aim to make it a core competency of our business. 

What Youll Do


Improve data quality constantly iterate on ways to improve the quality of our data, including but not limited to
Building automated tools to monitor, validate, and scrub our existing data
Ensuring consistency across systems
Suppressing andor mitigating duplicate records
Evaluating and acquiring new data worldwide
Mapping complex real world organizational structures such as conglomerates and multinational corporations in our systems

Build scalable infrastructure build reliable, efficient, and scalable infrastructure for data storage, as well as optimal extraction, transformation, and loading of data from a variety of sources and systems

Improve and automate processes identify, design and implement internal process improvements - automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability

Requirements

Bachelors or masters degree in a technical discipline from a top school
3 years of professional experience, ideally in very analytical or technical roles
Advanced working SQL knowledge and experience working with relational databases, query authoring as well as working familiarity with a variety of databases
Experience building and optimizing big data data pipelines, architectures, and data sets
Experience with Python, AWS EC2, RDS, etc., Google Cloud, etc.; working knowledge of Salesforce and Looker is a plus
Ability to work with cross-functional teams in a dynamic and fast-paced environment


US Employee Benefits
Verkada is committed to investing in the holistic health and wellbeing of all employees and their families. Our benefits and perks programs include, but are not limited to 

 Healthcare programs that can be tailored to meet the personal health and financial well-being needs - Premiums are 100 covered for the employee under most plans and 80 for family premiums
 Nationwide medical, vision and dental coverage
 Health Saving Account HSA and Flexible Spending Account FSA with tax saving options
 Expanded mental health support
 Paid parental leave policy  fertility benefits
 Time off to relax and recharge through our paid holidays, firmwide extended holidays, flexible PTO and personal sick time
 Professional development stipend
 Wellnessfitness benefits
 Healthy lunches and dinners provided daily

Verkada Is An Equal Opportunity Employer
As an equal opportunity employer, Verkada is committed to providing employment opportunities to all individuals. All applicants for positions at Verkada will be treated without regard to race, color, ethnicity, religion, sex, gender, gender identity and expression, sexual orientation, national origin, disability, age, marital status, veteran status, pregnancy, or any other basis prohibited by applicable law. 

"
https://startup.jobs/data-engineer-enterprise-data-warehouse-disco-4217550,Engineer,"Data Engineer, Enterprise Data Warehouse",DISCO ,"Gurugram, India",,"

Your Impact
As a Data Engineer on the Enterprise Data Warehouse platform, you will build data pipelines that enable critical visibility for our business. This insight is key in our mission to conquer the Legal Tech industry. We hire the best and brightest engineers that approach design from a systems perspective and have the aspirational goal of everything well-crafted. A DISCO candidate can deliver real customer value while pursuing high marks on these architecture quality attributes Availability, Scalability, Interoperability, Modifiability, Performance, Security, Testability.
 
What Youll Work On

Highly scalable and finely crafted data pipelines
Business Intelligence and Data Analytics capabilities to drive low-latency Key Performance Indicator generation;
Custom connectors to extract critical data from third party systems for ingestion into the Enterprise Data Warehouse;
Event bus and event sourcing capabilities that provide business and engineering leverage and efficiencies;
Transactional or eventually consistent stores that provide well-encapsulated domain object semantics;
Orchestrated scale-out data pipelines that can leverage serverless and containerized compute that balance cost, latency, and duration;
Algorithmically intensive data engines that operate on streaming, large, or multi-tenant datasets;
Innovative data validation frameworks to enhance data quality and reliability.

 
Who You Are

5 years experience as a data engineer or in a similar role
Strong SQL skills and knowledge of modern enterprise programming languages like Python, Java, or C
Excellent with data modeling, database design and data analysis, including how to scale out, make highly available, or map to storage systems
Strong experience with Data InfrastructureDatabase technologies such as Snowflake, Redshift, Vertica, Teradata, Oracle RAC, PostgreSQL, Informatica
Able to employ modern Continuous Integration and Continuous Deployment CICD tools with an emphasis on a well-maintained testing pyramid
Experience with Business Intelligence technologies such as QuickSight, Tableau, Qlik, or Looker
Experience with Big Data technologies such as Kafka, DataFlow, and Spark and data processing and orchestration systems such as DBT, NiFi, and Airflow, 
Extensive experience designing and operating software in a cloud provider such as AWS or GCP

 
Even Better If You Are...

Experienced translating requirements into a robust, scalable data pipelines to support dynamic workloads and world-class analytics
Skilled taking technical or business context to guide the team by turning ambiguity into clarity
Experienced with persistence mechanisms  storage technologies such as; Relational Databases, NoSql Stores, data caches, etc.




DISCOs Technology Stack
Enterprise Reporting Snowflake, QuickSight, Fivetran, DBT
Cloud Provider AWS - ECS, EC2, Lambda, Aurora MySQL, Redshift, DynamoDB, SQS, SNS, Kinesis, S3, CloudFront, CloudFormation, SageMaker, KMS, CodePipeline, etc.
Event Bus Kafka and Schema Registry
CICD Terraform, Docker via ECS, Jenkins, CodeDeploy, GitHub, Artifactory, Consul for App Config, Service Discovery, Shared Secrets
Visibility ELK Stack for logging, Data Dog, New Relic, Sentry.io
Programming Languages JavaKotlin, PythonFlask, C.NET, JavaScriptReact
Transport Mechanisms Protobuf, Avro, HTTP RestJSON

About DISCO
DISCO provides a cloud-native, artificial intelligence-powered legal solution that simplifies ediscovery, legal document review and case management for enterprises, law firms, legal services providers and governments. Our scalable, integrated solution enables legal departments to easily collect, process and review enterprise data that is relevant or potentially relevant to legal matters. 
Are you ready to help us fulfill our mission to use technology to strengthen the rule of law? Join us! 
We are an equal opportunity employer and value diversity. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Please note that DISCO has a mandatory COVID vaccination policy which requires all employees in the U.S. to be fully vaccinated, subject to applicable legal exemptions.
 


"
https://startup.jobs/data-engineer-apac-cadmus_io-4217527,Engineer,Data Engineer (APAC),Cadmus ,"Jakarta, Indonesia",Full-Time,"


Equal Access to High-Quality Education Moves Our World Forward
Meet Cadmus!
At Cadmus, we believe every student should have equal opportunity to achieve academic excellence; thats why were changing how the world learns!
Cadmus is a global EdTech company purposefully built for the higher education sector to break down global learning barriers by providing educators and diverse student cohorts with access to high-quality learning and assessment. Built on rich, student-centric values, Cadmus empowers individuals to achieve their highest potential and graduate with the critical skills and knowledge to profoundly impact our workforces, communities, and evolving world.
A Bold Plan for Global Impact
At Cadmus, were working hard in pursuit of an ambitious, world-shaping goal to provide 1 billion students with access to high-quality education by 2050.
Thats why we need a Data Engineering superstar to join our team and help us maintain a high-quality experience for our users worldwide  ensuring our cloud platform is reliable, performant and set up to scale. We want to do everything we can to ensure our users enjoy their experience and love our platform. With countless opportunities to make an impact, youll help us solve exciting engineering challenges and deliver improved functionality to take Cadmus even further.

About the role
As a Cadmus Data Engineer, youll be working across multiple engineering disciplines to build and scale our data pipeline, as well as develop our data strategy and approach across teams. You will set up ETLELT pipelines within AWS and write data processing systems collecting and delivering data to all parts of the Cadmus platform. You will set up modern data tools and services within our infrastructure. You will develop a deep insight into the use of data across the platform and have strong ownership of the data thinking within the company. You will empower non-engineering and engineering teams to derive valuable insights from data.
This is a truly unique opportunity to help shape our technology stack, infrastructure, and processes and contribute to the culture of data engineering at Cadmus.

About our current tech
 AWS cloud infrastructure with data pipelines, Kinesis streaming, S3 lake, Redshift warehouse, etc. AWS Lambda ELT functions in Typescript and Rust. Elixir backend and React Typescript front-end applications. Various Elixir, Typescript NodeJS, Rust, and Haskell services provide critical and non-critical functionality to the product and data pipelines. 
We also use and love to experiment with other languages for various tooling to ensure we find the best tool for the job. If you want to know more about what its like to be a developer at Cadmus, check out our blog post.
Requirements
We want to hear from engineers passionate about building cloud data pipelines with modern data tools. We are looking for engineers who take a principled approach to data and its use within the Product and the Company. You are able to break down data silos and democratise access to the data in a way that helps everyone around you make smarter decisions. At the same time, your technical understanding and skills in the data engineering world are immaculate and stem from empirical experience.
You thrive working in a team, love end-to-end ownership, obsess over details, and use data to help make the best choices for the wider business. You are an outstanding problem solver; you have the ability to take inputs from different stakeholders and come up with outstanding solutions. You are highly organised and can easily and joyfully manage large projects, and large volumes of data points.
In this role youll
 Architect and develop cloud-based AWS data systems that ingest data from across the Cadmus Platform and have ELT, warehouse and lake analysis capabilities. Work on and expand the Cadmus Learning Analytics product offering. Shape how every team gets access to the data and empower them to make data-based decisions with the best tools in hand. Progressively develop our data strategy with Higher-Ed University Stakeholders, Company value proposition, and internal teams in mind. Write SQL-like warehouse transformations via DBT, and other ETL scripts. Play an active role in designing and improving the overall data architecture to support massive growth in users and products across the globe. 
Wed love to hear from you if you have
 4 years of industry experience in building data engineering pipelines and teams at the growth stage. A solid understanding of the AWS offerings in the data space, as well as open-source and other proprietary product offerings for a modern data stack. Solid knowledge of SQL, and great experience with multiple languages in your tool belt. Knowledge of computer science fundamentals, data structures and design patterns, and an ability to use them practically when implementing solutions. Excellent communication skills and soft skills which bridge technical and strategic verticals. You can easily and effectively communicate data to different stakeholders across the business. High levels of team empathy. You love working within a focused team and can bring everyone along in the data journey. 
Wed really love to hear from you if you have
 Startup DNA; youre flexible, comfortable with ambiguity, and have experience scaling systems to support rapid growth. Experience with Redshift and DBT, and various dash-boarding products. Experience with Functional Programming languages. 
Benefits
Whats great about working at Cadmus?
 A remote-friendly, flexible working culture; where you can work from any global location Learning allowances; because we dont just have words on a website, we genuinely do what we say and provide educational opportunities to all including the Cadmus team A diverse and inclusive workplace where there are no barriers to anyone succeeding A surrounding team of mission-driven individuals who genuinely love what they do Equity incentives; that way, we all share in the success of Cadmus Mentoring and succession planning for your career Pats from Homer, our resident and super cute dog! 

Were a remote-friendly company, and this role is open to candidates in Jakarta or remote from anywhere in Indonesia as long as you have a good internet connection.


Hiring Process
Apply online with your resume only.
Instead of a cover letter, wed love for you to answer a few questions alongside your application.
Our interview process is normally a recruiter interview, a hiring manager interview, a programming task, and 2-3 hours of onsite interviews that will be held via Zoom.
While we review your application, get to know us by visiting cadmus.io or following our social channels.

Inclusivity at Cadmus
At Cadmus, we hire great people from a wide variety of backgrounds because it makes our company stronger. We never discriminate on the basis of race, religion, national origin, gender identity or expression, sexual orientation, age, marital, or disability status. If you share our values and enthusiasm for education, you will find a home at Cadmus.
If you need assistance or accommodations made due to a disability, please let us know.

"
https://startup.jobs/data-engineer-aus-cadmus_io-4217523,Engineer,Data Engineer (AUS),Cadmus ,"Melbourne, Australia",Full-Time,"


Equal Access to High-Quality Education Moves Our World Forward
Meet Cadmus!
At Cadmus, we believe every student should have equal opportunity to achieve academic excellence; thats why were changing how the world learns!
Cadmus is a global EdTech company purposefully built for the higher education sector to break down global learning barriers by providing educators and diverse student cohorts with access to high-quality learning and assessment. Built on rich, student-centric values, Cadmus empowers individuals to achieve their highest potential and graduate with the critical skills and knowledge to profoundly impact our workforces, communities, and evolving world.
A Bold Plan for Global Impact
At Cadmus, were working hard in pursuit of an ambitious, world-shaping goal to provide 1 billion students with access to high-quality education by 2050.
Thats why we need a Data Engineering superstar to join our team and help us maintain a high-quality experience for our users worldwide  ensuring our cloud platform is reliable, performant and set up to scale. We want to do everything we can to ensure our users enjoy their experience and love our platform. With countless opportunities to make an impact, youll help us solve exciting engineering challenges and deliver improved functionality to take Cadmus even further.

About the role
As a Cadmus Data Engineer, youll be working across multiple engineering disciplines to build and scale our data pipeline, as well as develop our data strategy and approach across teams. You will set up ETLELT pipelines within AWS and write data processing systems collecting and delivering data to all parts of the Cadmus platform. You will set up modern data tools and services within our infrastructure. You will develop a deep insight into the use of data across the platform and have strong ownership of the data thinking within the company. You will empower non-engineering and engineering teams to derive valuable insights from data.
This is a truly unique opportunity to help shape our technology stack, infrastructure, and processes and contribute to the culture of data engineering at Cadmus.

About our current tech
 AWS cloud infrastructure with data pipelines, Kinesis streaming, S3 lake, Redshift warehouse, etc. AWS Lambda ELT functions in Typescript and Rust. Elixir backend and React Typescript front-end applications. Various Elixir, Typescript NodeJS, Rust, and Haskell services provide critical and non-critical functionality to the product and data pipelines. 
We also use and love to experiment with other languages for various tooling to ensure we find the best tool for the job. If you want to know more about what its like to be a developer at Cadmus, check out our blog post.
Requirements
We want to hear from engineers passionate about building cloud data pipelines with modern data tools. We are looking for engineers who take a principled approach to data and its use within the Product and the Company. You are able to break down data silos and democratise access to the data in a way that helps everyone around you make smarter decisions. At the same time, your technical understanding and skills in the data engineering world are immaculate and stem from empirical experience.
You thrive working in a team, love end-to-end ownership, obsess over details, and use data to help make the best choices for the wider business. You are an outstanding problem solver; you have the ability to take inputs from different stakeholders and come up with outstanding solutions. You are highly organised and can easily and joyfully manage large projects, and large volumes of data points.
In this role youll
 Architect and develop cloud-based AWS data systems that ingest data from across the Cadmus Platform and have ELT, warehouse and lake analysis capabilities. Work on and expand the Cadmus Learning Analytics product offering. Shape how every team gets access to the data and empower them to make data-based decisions with the best tools in hand. Progressively develop our data strategy with Higher-Ed University Stakeholders, Company value proposition, and internal teams in mind. Write SQL-like warehouse transformations via DBT, and other ETL scripts. Play an active role in designing and improving the overall data architecture to support massive growth in users and products across the globe. 
Wed love to hear from you if you have
 4 years of industry experience in building data engineering pipelines and teams at the growth stage. A solid understanding of the AWS offerings in the data space, as well as open-source and other proprietary product offerings for a modern data stack. Solid knowledge of SQL, and great experience with multiple languages in your tool belt. Knowledge of computer science fundamentals, data structures and design patterns, and an ability to use them practically when implementing solutions. Excellent communication skills and soft skills which bridge technical and strategic verticals. You can easily and effectively communicate data to different stakeholders across the business. High levels of team empathy. You love working within a focused team and can bring everyone along in the data journey. 
Wed really love to hear from you if you have
 Startup DNA; youre flexible, comfortable with ambiguity, and have experience scaling systems to support rapid growth. Experience with Redshift and DBT, and various dash-boarding products. Experience with Functional Programming languages. 
Benefits
Whats great about working at Cadmus?
 A remote-friendly, flexible working culture; where you can work from any global location Learning allowances; because we dont just have words on a website, we genuinely do what we say and provide educational opportunities to all including the Cadmus team A diverse and inclusive workplace where there are no barriers to anyone succeeding A surrounding team of mission-driven individuals who genuinely love what they do Equity incentives; that way, we all share in the success of Cadmus Mentoring and succession planning for your career Pats from Homer, our resident and super cute dog! 

Were a remote-friendly company, and this role is open to candidates in Melbourne or remote from anywhere in Australia as long as you have a good internet connection and are willing to travel to Melbourne throughout the year.


Hiring Process
Apply online with your resume only.
Instead of a cover letter, wed love for you to answer a few questions alongside your application.
Our interview process is normally a recruiter interview, a hiring manager interview, a programming task, and 2-3 hours of onsite interviews that will be held via Zoom.
While we review your application, get to know us by visiting cadmus.io or following our social channels.

Inclusivity at Cadmus
At Cadmus, we hire great people from a wide variety of backgrounds because it makes our company stronger. We never discriminate on the basis of race, religion, national origin, gender identity or expression, sexual orientation, age, marital, or disability status. If you share our values and enthusiasm for education, you will find a home at Cadmus.
If you need assistance or accommodations made due to a disability, please let us know.

"
https://startup.jobs/data-engineer-two95-international-inc-4196714,Engineer,Data Engineer,Two95 International Inc. ,"Hyderabad, India",Full-Time,"

Title Data Engineer Type Full Time Location Hyderabad Salary Open INR
Requirements
Job Description
Apply strong expertise in developing, maintaining, and testing infrastructures, transforming data into a format that can be easily analyzed, and architecting solutions for data scientists that enable them to do their jobs.  Develop, construct, test and maintain architectures, such as databases and large-scale processing systems Ensure architecture will support the requirements of the data scientists, the stakeholders, and the business. Discover opportunities to acquire new data from other systems Develop and improve data set processes for data modeling, mining, and production. Employ a variety of languages and tools to marry systems together Recommend and implement ways to improve data reliability, efficiency, and quality. Collaborate with stakeholders including the Product owner, data science, and design teams to assist with data-related technical issues and support their data infrastructure needs. Create data tools for analytics and data scientist team members that assist them in building and optimizing the products that help business achieving their goals. Work with data and analytics experts to strive for greater functionality in data systems.  Technical expertise BScMSc in Computer Science, Computer Science, Information Systems or related Technical Discipline 1-4 years experience in Data Engineer role 5-10 years for senior data engineer Deep knowledge of Python, SQL, and PySparkis required. Experience working with data pipelines, architecture principles, batch and stream processing systems, and DataOps. Experience working with large data sets, Azure cloud services including Azure Data Lake, Data factory, Databricks, Azure DevOps. Background in programming in Python, Scala, C, C, Java is beneficial.  Agile experience Experience working in AI startup environment or organisationswith an agile culture Professional attitude and service orientation; superb team player  Individual skills Good written and verbal communication skillsalong with strong desire to work in cross-functional teams  Mindset  behaviours Able to build a sense of trust and rapport that creates a comfortable  effective workplace; collaborative Attitude to thrive in a fun, fast-paced, startup-like environment Open minded to new approaches and learning  Key Skills
- Deep knowledge of Python, SQL, and PySparkis required. - Azure, ADB, ADF
Benefits
Note If interested please send your updated resume to rehana.jtwo95intl.com and include your salary requirement along with your contact details with a suitable time when we can reach you. If you know of anyone in your sphere of contacts, who would be a perfect match for this job then, we would appreciate if you can forward this posting to them with a copy to us.  We look forward to hearing from you at the earliest!

"
https://startup.jobs/data-engineer-financials-remote-kuali-4194261,Engineer,Data Engineer - Financials (Remote),Kuali ,"Lehi, United States",Full-Time,"

Who are we? Kuali builds software solutions for higher education. We help our customers  colleges  universities  focus on providing a fantastic education to students by decreasing their administrative costs. We work in a competitive space, ripe for innovation, with users ready to be delighted.
Our Culture As a company, we are guided by our cultural values
 Iterate to evolve Cultivate openness Act with accountability Assume the best Practice humility Deliver amazing experiences 
As Kuali engineers, we learn from and teach each other, we practice transparency and empathy, and we delight in delivering value to our customers.
We work remotely, and have for years. Distributed work is in our bones, with a history of institutions working across state lines on open-source software for more than ten years. Our employees each work in the environment where theyre happiest, from Pennsylvania to Hawaii. We work consciously to create a collaborative and healthy remote work culture, and we travel to meet in person a few times each year.
Everyone should love their work. Kuali has been voted a top place to work for 5 years in a row by the Salt Lake Tribune. We also made Forbes list of Americas Best Startup Employers for 2020. Not too shabby.
Your product team You will work closely with product, design, and our customers on the Kuali Financials product. Customers use our Financials product to efficiently and effectively manage the complex accounting needs of higher education.
Requirements
Who are you?
Were looking for curious, enthusiastic, empathetic engineers to solve problems, execute on ideas, advocate for the customer, and contribute to a team culture built on trust and mutual respect. As a data engineer here, youll have a significant impact on what we do and how we do it. We build and support data pipelines and reporting solutions for a range of business needs, including end user consumption. We are focusing heavily on greenfield development you will build new analytics and data services including data pipelines, data warehousing solutions, ETL processes, end user reports, as well as the deployment mechanisms and the platforms environments. As a Data Engineer you will have the unique opportunity to influence major decisions from how our data platforms cloud-based infrastructure is architected, to what modern data-engineering frameworks and tooling are used, to how our CICD will operate. While building out the new platform, you will also maintain a light-weight, legacy solution, which will be deprecated and replaced. We believe that great developers can always learn new tools. Above any specific tech stack, were looking for versatile developers  those who know when to think big and when to act small, and who are comfortable in both greenfield and refactoring projects.
We believe the best products are created by teams who represent a broad range of ideas and perspectives. We value employees with diverse backgrounds and experiences.
You...
 Have 3 years of Data Engineering experience or equivalent. Architecture-level experience conceptualizing and building infrastructure that processes, stores, and vends large data sets for analytics purposes at an enterprise scale in the cloud. Advanced SQL skills including database design best practices. CTEs, functions, stored procedures dont intimidate you. Have hands-on experience with ETL-as-code frameworks such as Airflow, Luigi, or Prefect or experience building ETL processesservices from scratch with generic languages and libraries. Experience developing and supporting reports with BI and reporting tools such as Tableau, Domo, Looker, or Sisense. Understand the software development lifecycle and are able to work alongside development teams. Youre excited to collaborate closely with Application Engineers, Product Managers, and Customer Success and use real-time feedback to solve problems iteratively. Are ready to help reformulate existing frameworks to improve and expand current offerings. Arent afraid to get your hands dirty on devops work. 
Wed be delighted if you bring experience with
 Shipping Software as a Service SaaS solutions One or more of these technologies Java, Python, Node.js, AWS One or more relational databases MySQL, Oracle or Postgres One or more analytics databases Redshift, Snowflake, or Teradata. Report and dashboard requirements analysis and design Front end development with React or Angular The Higher Education community 

Other things you should know
This team is and has always been fully remote. Youd be expected to have a suitable home working environment or alternative. We try to get together in person as a team or company 2-4 times a year.
Benefits
 Top-of-the-line equipment of your choice to get your job done A truly exceptional benefits package including full premium coverage for employee and dependent medical and dental care 401k matching Paid MaternityParental leave All the paid time off you need just work it out with your manager Allowance for continuing education, conferences, andor training Space to work on self-driven projects during hack time Employee resource groups and community events 

"
https://startup.jobs/data-engineer-truecaller-4183952,Engineer,Data Engineer,Truecaller ,"Stockholm, Sweden",,"

Hej, Truecaller is calling you from Stockholm, Sweden! Ready to pick up?
Truecaller transformed how we communicate when we launched in 2009 in Stockholm, Sweden.
Our mission is to build trust everywhere by making tomorrows communication smarter, safer and more efficient. We focus on bringing smart services with big social impacts, like protecting people from fraud, harassment, and scam calls or messages. We identify new numbers from anywhere in the world and build a space for trustworthy conversations, for those that matter. Truecaller is loved by over 340 million people around the world and is growing very rapidly across regions. 
Who we are

The worlds 1 caller ID and spam blocking service for Android and iOS.

A team of 400 people from 35 different nationalities, spread across our headquarters in Stockholm and offices in Bangalore, Mumbai, Gurgaon, Tel Aviv and Nairobi.

Listed on NasdaQ OMX Stockholm, Large Cap

We at Data Platform work on a vision to empower our product-oriented teams with a complete self-service analytics platform to make data-driven decisions possible in Truecaller, guaranteeing the stability, velocity, and delivery of the mainstream events pipeline. 
As a Data Engineer, you will play an important role in the Data Platform team who are responsible for building and maintaining a scalable, robust, self-service analytics platform.
What do we expect from you

Experience working with orchestration tool ex Airflow
Experience working with a large data set
Experience building complex ETL pipelines
Experience working with cloud providers such as GCP, AWS or Azure
Strong programming skills in Spark with Scala and Python
Experience working with CICD tools like Jenkins and Git
Strong understanding of Software Engineering practices and principles
Excellent problem solving and communication skills

Self-motivated and have a proven ability to take initiative to own the problems that come up and solve them


What will you work on

Maintaining and improving the ingestion pipeline to reliably deliver billions of events daily in defined SLA
Provide support for all teams in building and optimizing their complex pipelines
Work closely with other teams to identify pain points and problems around the platform
Develop new tools and frameworks to improve the data platform
Facilitate company wide to be data-driven
Work in close collaboration with data scientists and data analysts to help support their work go to production
Work with vast projects such as building an ML platform and streaming use cases

Setup best practices and processes around software and data development


It would be great if you also have

Working with messaging system like Kafka
Knowledge in Kubernetes
Hands on experience with any streaming platform
Experience managing data warehouse in BigQuery or Redshift

Life at Truecaller - Behind the code  httpswww.instagram.comlifeattruecaller
Sounds like your dream job?
We will fill the position as soon as we find the right candidate, so please send your application as soon as possible. As part of the recruitment process, we will conduct a background check.
This position is based in Stockholm, Sweden. 
We only accept applications in English. 
What we offer 


A smart, talented and agile team An international team where  35 nationalities are working together in several locations and time zones with a learning, sharing and fun environment. 


A great compensation package Competitive salary, 30 days of paid vacation, flexible working hours, private health insurance and pension contribution, Udemy membership to keep learning and improving, gym membership. 


Great tech tools Pick the computer and phone that you fancy the most within our budget ranges. 


Do it your way We work in-office on Tuesdays, Wednesdays and Thursdays with flexibility on the other days. You can enjoy 3 weeks working remotely from anywhere you want per year.


Office life Enjoy your days with a wide range of yummy snacks and beverages, and have fun at our playroom! As well, exciting company parties and team activities such as Lab days, Running team, Geek lunch!


Come as you are
Truecaller is diverse, equal and inclusive. We need a wide variety of backgrounds, perspectives, beliefs and experiences in order to keep building our great products. No matter where you are based, which language you speak, your accent, race, religion, color, nationality, gender, sexual orientation, age, marital status, etc. All those things make you who you are, and thats why we would love to meet you.

"
https://startup.jobs/data-engineer-angle-health-4171191,Engineer,Data Engineer,Angle Health ,,Full-Time,"

Changing Healthcare For Good
At Angle Health, we believe the healthcare system should be accessible, transparent, and easy to navigate. As a digital-first, data-driven health plan, we are replacing legacy systems with modern infrastructure to deliver our members the care they need when they need it. If you want to build the future of healthcare, wed love for you to join us.

The Role
As a Data Engineer on the Strategy team at Angle Health, you will be part of an elite team of problem solvers tackling some of the hardest operational challenges across the business. You will be working closely with Technical Product Managers and other engineers in small teams embedded within functions across the companyfrom sales to operations to finance and more. 

In partnership with operational stakeholders, your job will be to quickly understand business workflows, and design and implement technical, data-driven solutions to achieve the intended business outcomes. That may include architecting backend micro-services, building and maintaining robust data pipelines, standing up health metrics and automated alerts, and developing scalable technical solutions to support Angle Healths day-to-day operations.

Every day will be a new learning experience in this role and may span discussing architecture with fellow engineers, wrangling large-scale data, coding a custom web app or micro-service, or speaking with customers and executives.

This role is ideal for entrepreneurial engineers, applied data scientists, and creative, intellectually curious thinkers that want to solve high-value problems in healthcare. 

This position may be based in San Francisco, New York City, Salt Lake City, or Remote.

We are currently trialing various titles for the same role. Please consider the following posted roles as the same position Deployed Engineer, Software Engineer, Business Operations, and Data Engineer. Please note these are the same, so if you have already applied for one position, there is no need to reapply for the others.
What We Value

A strong engineering background in computer science, software engineering, data science, mathematics, or similar technical field is required for this role
Proficiency in programming languages e.g. Python, SQL, Java, TypeScriptJavaScript, or similar and data engineering frameworks
A highly analytical mindset and an eagerness to build technical solutions to complex business problems
High attention to detail and intellectual curiosityyoure not satisfied with surface-level answers. You want to dive into the data, the ""how,"" and the ""why"" because ""the way its always been done"" is not always the way it should be done
Low egothe outcome matters more than who gets the credit
Demonstrated ability to collaborate effectively in teams of technical and non-technical individuals
Highly organized with an ability to multitask, problem solve, and balance competing priorities in a rapidly changing environment


Because We Value You

      Competitive compensation and stock options
      100 company paid comprehensive health, vision  dental insurance for you and your dependents
      Supplemental Life, ADD and Short Term Disability coverage options 
      Discretionary time off
      Opportunity for rapid career progression
      Relocation assistance if relocation is required
      3 months of paid parental leave and flexible return to work policy after 10 months of employment
      Work-from-home stipend for remote employees 
      Company provided lunch for in-office employees 
      401k account 
      Other benefits coming soon!

Backed by a team of world class investors, we are a healthcare startup on a mission to make our health system more effective, accessible, and affordable to everyone. From running large hospitals and health plans to serving on federal healthcare advisory boards to solving the worlds hardest problems at Palantir, our team has done it all. As part of this core group at Angle Health, you will have the right balance of support and autonomy to grow both personally and professionally and the opportunity to own large parts of the business and scale with the company.

Angle Health is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, gender including pregnancy, childbirth, or related medical conditions, sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. Angle Health is committed to working with and providing reasonable accommodations to applicants with physical and mental disabilities.

"
https://startup.jobs/data-engineer-python-ctw-4153993,"Engineer,Python",Data Engineer - Python,CTW ,"Minato City, Japan",Full-Time,"

Job overview
As a Data Engineer Python, you will be responsible for designing and implementing a robust data flow on our G123 platform to ensure optimal timeliness, data quality and improved user experience. You will work closely with advertising and operations to directly address the data needs as our G123 platform grows its portfolio of games and expands globally. Main responsibilities include

 Investigating suitable data solutions, evaluating and using reasonable costs  Mainstreaming technologies to solve data problems Maintaining the data warehouse of the G123 platform 
Requirements
Bachelors degree or above in Computer Science or other related fields
 3 years of relevant experience as a data engineer, etc. Proficiency in bash, Python, Scala, JavaScript Maintained and operated open source data-related suites HDFS, YARN, Hive, Presto, Spark, Kafka, HBase, Elasticsearch, TiDB, Flink, Airflow, etc. Familiar with AWS data-related services DynamoDB, QuickSight, Lambda, S3, EMR, Athena, Redshift, Kinesis, Glue, RDS, DMS, etc. Familiar with Unix  Linux systems Familiar with Docker and other containerization concepts  Must be residing in Japan regardless of the visa status  
Good to have
 Experience in operation and maintenance of production environment in public cloud AWS, Ali cloud, Tencent cloud, etc. Experience in operation and maintenance of large data stream production environments data volume of petabytes or more, processing data streams of more than 100,000 strokes per second Experience in open source community contribution Passionate about games, able to actively dig and understand the data requirements of games  
Language Requirements
 Can work in Chinese OR Japanese OR English Can read and understand technical documents in English 
Benefits
 Visa support Free lunch catering after 3 months employment and early-bird breakfast Free in-office Starbucks, coffee, tea, beverages, snacks, sweets,  vitaminssupplements Modern office space in the heart of Tokyo, with views of Tokyo Tower and Mount Fuji Industry-leading work-life harmony overtime is rare and discouraged Commute expenses covered 30,000 yen rent support if you live 2 stops from Roppongi 1-chome or Kamiyacho stations Bonus for continuous employment 50,000 yenmonth added to base salary after 5 years of continued employment Health insurance Diverse  international environment Working hours 1000  1900, Monday to Friday in-office   About CTW.inc Founded in 2013, CTW is Japans No.1 browser game company. Our primary service is the G123 IP game platform  which publishes video games based on famous Japanese IP. Weve been experiencing steady growth in revenue in the past few years as our player base has exploded to over 50 million users worldwide. We must grow quickly to keep up with market demand. Our CEO founded and named this company with the aspiration to ""Change the World."" As a fast-growing startup, we hope our team will drive themselves to try new things and accelerate their careers to match the rapid expansion of the company. Our core values are- ambition- drive- simplicity With global hits like Vivid Army, Queens Blade, and many more, were looking to go from a market leader in Japan to an industry leader globally!

"
https://startup.jobs/data-engineer-us-remote-actian-corporation-4150155,Engineer,Data Engineer - US Remote,Actian Corporation ,,Full-Time,"

Our Vision is to be the Most Trusted, Flexible and Easy to Use Hybrid Cloud Data Platform.  Actian is transforming industries by empowering companies to accelerate application modernization and simplify the Cloud journey.  Our customers use the Actian Data Platform to unify their siloed data, explore and securely exchange data to run a variety of analytic workloads that provide real time business insights at a fraction of the cost.  We have 24 of the Fortune 100 companies using Actian technology in some of the most mission critical applications that impact your daily life.

 As an experienced Data Engineer, you know what it takes to deliver high quality data solutions to an enterprise.  Skilled in sourcing, extracting, transforming, and loading data, you can translate business designs into database models.  You understand the value and benefit of solid data practices.  You enjoy exploring and get excited about learning new technologies and learning in a collaborative environment. You implement best practices when writing code and scripts for automation.
Responsibilities

Develop a variety of data workflows, pipelines, and ETL processes using cloud platform products and internal integration tools, including Actian Avalanche
Develop data quality and governance automations to ensure the accuracy and quality of the data through inspection, validation, processing, anomaly detection and auto-corrections
Translate design specification into working modules with a hands-on approach in the development of prototypes through iterative design
Write and maintain efficient and reliable code
Write technical documentation

Skills

At least 5years of experience in Data management
Advanced SQL scripting
Knowledge of data transformation tools
Knowledge of algorithms and data structures
Knowledge of any object-oriented programming language
Strong communication and collaboration skills
Experience with Agile methodologies
A degree in computer science or similar background


We value diversity at our company. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or any other applicable legally protected characteristics in the location in which the candidate is applying. 


ATTENTION JOB SEEKERS Please be aware of potential scams related to Actian job postings. We have been made aware of individuals impersonating Actian recruiters and asking for sensitive information, including banking details. Please note that any communications from Actian will come through our ATS, Lever, or from an email address with an Actian domain. If you are approached by actianinterviewsessions.com or an gmail address, this is not legitimate. We encourage you to apply for roles only through httpswww.actian.comcompanycareers. If you have any concerns or questions, please contact us at talentacquisitionactian.com. 

"
https://startup.jobs/data-engineer-l5-growth-netflix-4107826,"Engineer,Marketing",Data Engineer (L5) - Growth,Netflix ,"Mexico City, Mexico",Full-Time,"

At Netflix, our mission is to entertain the world. With 200 million paid members in over 190 countries on millions of devices; enjoying TV series, documentaries, and feature films across a wide variety of genres and languages - Netflix is reinventing entertainment from end to end. We are revolutionizing how shows and movies are produced, pushing technological boundaries to efficiently deliver streaming video at a massive scale over the internet, and continuously improving the end-to-end user experience with Netflix across their member journey. 

We pride ourselves on using data to inform our decision-making as we work toward our mission. This requires curating data across various domains such as Growth, Finance, Product, Content, and Studio. All of this data collection and curation is made possible thanks to the amazing Data Engineers of Netflix who bring this data to life.

Data Engineering at Netflix is a role that requires building systems to process data efficiently and modeling the data to power analytics. These solutions can range from batch data pipelines that bring to life business metrics to real-time processing services that integrate with our core product features. In addition, we require our Data Engineers to have a rich understanding of large distributed systems on which our data solutions rely. Candidates should have knowledge across several of these skill sets and usually need to be deep in at least one. As a Data Engineer, you also need to have strong communication skills since you will need to collaborate with business, engineering, and data science teams to enable a culture of learning. Learn more about the work of data engineers at Netflix.

Location of work We are considering candidates who are in Mexico City, as well as fully-remote candidates within Mexico
Who are you?

You strive to write elegant code, and youre comfortable with picking up new technologies independently
You are proficient in at least one major programming language e.g. Java, Scala, Python and comfortable working with SQL
You enjoy helping teams push the boundaries of analytical insights, creating new product features using data, and powering machine learning models
You have a strong background in at least one of the following distributed data processing or software engineering of data services, or data modeling
You are familiar with big data technologies like Spark or Flink and comfortable working with web-scale datasets
You have an eye for detail, good data intuition, and a passion for data quality
You appreciate the importance of great documentation and data debugging skills
You relate to and embody many of the aspects of the Netflix Culture. You love working independently while also collaborating and givingreceiving candid feedback
You are comfortable working in a rapidly changing environment with ambiguous requirements. You are nimble and take intelligent risks


At Netflix, we carefully consider a wide range of compensation factors to determine your personal top of market. We rely on market indicators to determine compensation and consider your specific job family, background, skills, and experience to get it right. These considerations can cause your compensation to vary and will also be dependent on your location. 

For more information about our compensation philosophy, feel free to check here. 

"
https://startup.jobs/data-engineer-third-party-integration-knorex-4105886,Engineer,Data Engineer (Third Party & Integration),KNOREX ,"Pune, India",Full-Time,"

About Knorex
Founded in 2009, Knorex httpsknorex.com is a technology company that provides programmatic advertising products and solutions to marketers to connect in real-time to their desired audience worldwide. Through its flagship universal marketing platform, Knorex XPO unifies the connectivity to each of the major marketing channels including social media, search, connected TVsOTTs, video, audio, displaynative, and email, in one place, while simplifying execution and optimization across these channels through AI automation. Knorex operates across the US and APAC.
Why Knorex?
We are constantly on the lookout to recruit the best and the brightest - from engineering to sales to account management to operations and HR.
Knorex offers you many different opportunities to scale your ambition and creativity far and beyond. We embrace a dynamic and pragmatic way of doing things, setting ourselves up for long term achievement yet relentlessly focused on delivering the short term goals. If you love the joy of building stuffs and seeing them grow, growing yourself and others in the process, and challenging yourself to do stuffs that you once thought impossible, we invite you to explore a career with us.
What Knorex offers
 Competitive remuneration package including quarterly bonus pay out.  Comprehensive benefits scheme such as W3F Work, Wellbeing, Welfare Fund for courses, materials, personal health and wellbeing. Work from home benefit. Quarterly recognition program and long service reward. Career growth and personal development scheme. You will have the advance to influence and drive the changes, at Knorex we welcome good ideas and accept any possibility to make our company great. We offer great learning opportunities, you will have exposure in cutting edge performance marketing and advertising technology. Opportunity to work cross-country and with variety of projects of a different nature. 
Position DescriptionYou will work closely with our cross-country teams located regionally to learn about the business and technical analytics requirements and translate them into production system. Owing to the large data and real-time stream of data, coming up with efficient and pragmatic solutions and algorithms to the challenging problems will be come imperative. You will work with other team members to ensure the timely delivery of the systems and solutions and critically assess and monitor the efficiency andor effectiveness of the developed solution.

Key Responsibilities
 Develop clever algorithms and pragmatic solutions to our data analytics problems. Develop metrics to measure the outcomeimpact of your introduced solutions. Develop and maintain API to support other teams in retrieving the metrics Work with other members to implement and integrate into our existing systems. Document and improve the solutions over time. Evaluate and identify new technologies for implementation. Communicate with our business and technical teams to understand the analytics requirements. Respond and follow up to incorporate feedback and draw new insights. Prioritize tasks to meet multiple deadlines. 

Requirements
 Minimum 2 years of experience as a Data Engineer. Must be proficient in Python or Node.js , Mongo Db and Cloud AWSGCP.  Experience of building Data pipelines, good understanding of database design, Data quality and Data architecture.  Experience in SparkScala is a good plus with streaming data. Proficiency in coding  writing scalable codes and pay attention to performance with good knowledge of algorithms and data structures. Experience working with APIs, building Internal APIs and consuming external APIs. Experience with ad serving, ad tracking and optimisation is a plus Strong in analytics and problem solving technique. Keen to understand the pipelinesproduct and troubleshoot issues. Able to work independently as well as in collaborative mode with minimum supervision; Work productively even under pressure; Possess good work ethic, attitude with good follow-through; Excellent communication in written and spoken English. 

Benefits
 Ample opportunities to grow. You get to propose your own ideas and see it through Work with passionate, talented and driven colleagues who get things done!  Opportunity to work cross-country and with variety of projects of different nature  Challenging and exciting problems that await you to solve Comprehensive Health Insurance Coverage Personal Development Fund for courses and materials 

"
https://startup.jobs/data-engineer-smithrx-4099696,Engineer,Data Engineer,SmithRx ,"San Francisco, United States",,"

Job Summary
The Engineering team at SmithRx is developing the next-generation modern pharmacy benefits management PBM platform to change how companies administer and manage pharmacy benefits. Our unified technology platform provides real-time actionable insights that drive cost savings, power clinical services, and result in a brilliant customer experience. A unified technology platform exists nowhere in the pharmacy benefit ecosystem to programmatically solve widespread deficiencies. The result is a PBM delivering unmatched service quality and operational efficiencies that exceeds all industry standards.
As a Data Engineer, you will be responsible for building and maintaining a single source of truth data ecosystem to meet SmithRxs fast-growing needs. We set up data infrastructure and data pipelines, develop dimensional models, and integrate data from multiple sources in a near real-time fashion that would allow us to uncover new insights and serve our internal and external customers. You will advocate and bring best practicesmethodologies, coding standards, and large-scale data warehouse design perspectives to our team.  
What Youll Do

Design, build, and maintain highly scalable and reliable data platforms, including data pipelines, data warehouses, and data lakes.
Collaborate with cross-functional teams to understand their data requirements and design data solutions that meet their needs.
Implement and optimize data governance and security policies to ensure data quality and compliance.
Develop and maintain near real-time data processing workflows using ETLELT tools and scripting languages.
Follow Kimballs methodology to design a dimensional data warehouse. Migrate tablesviews from analytics databases and automate manual reports to an enterprise data warehouse.
Identify and troubleshoot performance issues in real-time data pipelines and data warehouses.
Keep up-to-date with emerging trends and technologies in data engineering and recommend best practices to continue improving our data platforms and enterprise data warehousedata lake.

What will you bring

BS or advanced degree in Computer Science, Information Systems, or relevant experience.
Minimum of 3 years of experience in data engineering, data warehousing, or related field.
Strong understanding of data modeling, dimensional data warehouse design, and SQL, SparkSQL
Hands-on experience with at least one cloud-based data platform, preferably AWS, as well as columnar data warehouse solutions eg, Redshift, Snowflake, Big Query, etc
Proficiency in at least one scripting language, such as Python, Java, or Scala.
Experience with data integration tools or near real-time streaming such as Apache Kafka, Apache Flink, AWS Glue, Airflow, or other stream processing frameworks.
Strong analytical and problem-solving skills.
Excellent communication and collaboration skills to work effectively with cross-functional teams.

Good to have

Data visualization tools like Looker
Healthcare industry domain knowledge


"
https://startup.jobs/data-engineer-qcells-4092677,Engineer,Data Engineer,Qcells ,"San Francisco, United States",Full-Time,"

This is a San Francisco based position that is currently remote and will have a hybrid schedule once we return to office. We are open to candidates willing to relocate to the San Francisco Bay Area.

ABOUT GELI
Geli Growing Energy Labs, Inc. provides software and business solutions to design, connect, and operate energy storage systems ranging in size from residential to utility-scale, as well as grid-tied, microgrid, and off-grid systems. Gelis suite of products creates an ecosystem where project developers, OEMs, financiers, and project operators can deploy advanced energy projects using a seamless hardware-agnostic software platform.

Geli is a subsidiary of Hanwha Q CELLS, one of the worlds largest photovoltaic manufacturers most recognized for its high-performance, high-quality solar cells and modules.

OUR VISION
Geli is committed to helping make the planet a cleaner, better place to live, both with our software products and through our everyday actions.

Imagine a world where there is less reliance on non-renewable power, where you source your electricity from your neighbors rather than from power stations hundreds of miles away and software makes the best possible use of the solar, wind, and battery storage available. This is our vision.

We are looking for enthusiastic colleagues that are not only fluent in technology, but also share our vision of a world running on 100 renewable energy.

JOB SUMMARY
Geli is looking for an enthusiastic Data Engineer, who is eager to work at the forefront of the rapidly expanding energy storage industry. As part of our Data Science team, you will develop data workflows to support both our production systems and design tools. Data is at the core of all Gelis products, so this position will give you plenty of opportunities to contribute to our platform.
Responsibilities

Build and maintain robust data pipelines.
Collaborate with data scientists to understand data requirements of the machine learning and optimization models.
Collaborate with software engineers and DevOps to develop appropriate technical solutions.
Design and implement best practice approaches to data infrastructure, management and governance.

Required Experience and Skills

A solid foundation in computer science and software engineering principles, including object-oriented programming.
At least 2 years of work experience as Data Engineer
Experience writing clean, maintainable, efficient and thoroughly tested python 3.6 code.
Experience developing data pipelines and implementing data engineering best practices.
Ability to work collaboratively with all levels and teams at Geli.
Self-sufficient and proactive approach.
Willingness to learn and adapt in the rapidly growing energy industry.

Desired Experience and Skills

Knowledge of energy storage applications and renewable energy
Familiarity with machine learning and optimization algorithms and concepts
Docker, Kubernetes, AWS
Airflow
RabbitMQ, AMQP, Kafka
Data Lakes and Data Warehouses
PostgreSQL, Django ORM, Cassandra, Timescale DB, Redis, S3
RestAPIs




BENEFITS OF WORKING AT GELI
Competitive salary commensurate with experience
Competitive benefits offerings 
Conveniently accessible location in downtown San Francisco
Flexible work-from-home-office opportunities, as determined by the position and job duties
Cigna and Kaiser options - available by region
Cigna Dental including orthodontic coverage and vision plans, both have options with a 0 paycheck contribution
Healthcare and Dependent Care Flexible Spending Accounts FSA
Company Paid Health Savings Account HSA Contribution when enrolled in the High Deductible Cigna medical plan
Company-paid Basic Life, ADD, short-term, and long-term disability insurance
Voluntary benefits include critical illness, hospital indemnity, accident insurance
401k with a 4 employer match
3 weeks of paid Parental Leave
Sick time- 72 hours frontloaded per calendar year
Vacation time Flex time, and 13 Paid Holidays
Health Advocate wellness and concierge services
Wellness programs with our benefits providers
Commuter benefits
EAP
Bereavement leave- 5 paid days

WORK ENVIRONMENT
Make a difference join a group of people who are passionate about renewable energy
Have an impact the company is still small enough that everyones contribution has a significant impact on the success of the company
Many opportunities to lead teams, and projects, and contribute to development
Casual professional working environment theres no need to dress up, just present your best self
Work collaboratively in a diverse environment- we commit to reaching better decisions by respecting opinions and working through disagreements

DIVERSITY
We value the insights that a diverse team can bring. We encourage applications from members of groups that have been traditionally underrepresented in tech. 

Growing Energy Labs, Inc. provides equal employment opportunities EEO to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, or genetics.

"
https://startup.jobs/data-engineer-arabic-computer-systems-4090435,Engineer,Data engineer,Arabic Computer Systems ,"Al-Kharj, Saudi Arabia",Contractor,"

Responsibilities 
 Create and maintain an optimal data structure.  Compile large and complex datasets that meet functionalnon-functional business requirements. Define, design and implement internal process improvements automate manual processes, improve data delivery, and redesign infrastructure to increase scalability.  Build the infrastructure required to optimally extract, transform, and load data from a variety of data sources. Build analytics tools that use data to provide actionable insights into operational efficiency and business KPIs. Work with stakeholders and implementation teams to assist with data-related technical issues and support data infrastructure needs.  Identify ways to improve data reliability, efficiency and quality.  Ensure that data is available and accessed by the right parties. Ensure data entries adhere to data management practices. 
Requirements
  Must have a Bachelors degree in StatisticsComputer Science or equivalent.  At least 5 years of working experience in data warehouses and big data.  Advanced SQL working experience working with Relational databases and writing SQL queries as well as familiarity with working with a variety of databases.  Experience building and optimizing big data.  Strong analytical skills related to working with unstructured data sets.  Build processes that support data transformation, data structures, metadata, and dependency. Manipulating, Processing, and Extracting Value from Large Disconnected .datasets  Knowledge of Message Queuing, Stream Processing and scalable ""Big Data"" data stores. Experience with big data tools Kafka, Spark, Hadoop, etc.  Experience in object-orientedobject function scripting languages Relational SQL and NoSQL Databases Python, Java,   Experience in programming languages, Scala, etc.  Excellent verbal and written communication skills in Arabic and English.  Professional problem solving ability. 

"
https://startup.jobs/data-engineer-lynx-analytics-4090155,Engineer,Data Engineer,Lynx Analytics ,"San Francisco, United States",,"

COMPANY OVERVIEW
Founded in 2010, Lynx Analytics is a predictive analytics company run by world-class quantitative marketing scientists and industry-experienced data scientists. Our focus is to become a leading analytics solution provider in our chosen fields of expertise telecom, retail, life sciences, and financial services while advancing graph analytics technology.
Lynx is headquartered in Singapore with operations in Hong Kong, Germany, USA, Hungary, South Africa, Indonesia, and several other Southeast Asian countries. We work with some of the worlds largest companies and are constantly looking to expand our knowledge base and geographical footprint. Lynx Analytics technology is deployed with various Clients across Asia and has significant growth potential.
We have a diverse and inclusive global team comprising Professors, PhDs, MScs, and MBAs from Ivy Leagues, INSEAD and NUS with a broad spectrum of experience in start-ups and blue-chip companies Google, SAP, Vodafone, GE, Morgan Stanley, Barclays, HSBC to name but a few. It is the combination of our industry insight and experience, scalable proprietary technology, and highly qualified people that drives our compelling value proposition.
We are looking for ambitious, innovative, empathetic and relentless team players to explore the career opportunities that we offer as we continue to scale our operations.

We are looking for a Data Engineer to work on automating and productizing advanced big data transformation and analytics pipelines. You would be working with standard big data technologies Hadoop, Spark, etc,, as well as our proprietary big graph analysis framework.
KEY RESPONSIBILITIES
A Data Engineers responsibility is to implement and deploy data analysis pipelines at various clients of Lynx Analytics. This includes participating in the activities below

Understand deeply the business problem that we are trying to solve by our analytical solution
Through continuous consultations with employees of our client, discover the clients existing data sources that are relevant to the problem we try to solve. This includes discussions with client IT, data owners, future business users, etc.
Working together with the IT teams of the client, define the technical architecture for the analytical solution that we are to deploy for the client.
Implement the data ingestion subsystem this is the system responsible for moving all the necessary data sources to a single location where the actual analysis will happen.
Implement the data analysis pipelines. 
Integrate the results into business UIs developed by Lynx or pre-existing client software systems

REQUIREMENTS

Relevant tertiary qualification, preferably at Masters level or above, in Engineering or another relevant discipline with strong academic results
Strong programming skills
Experience in GCP, Airflow and Spark
Solid experience in Python and SQL
Good problem-solving skills
Good communication skills

DESIRABLE

Experience in Big Data
A minimum of 3 years of experience in Data Science or Analytics
Industry experience in working for a big enterprise like our clients

WHAT WE OFFER

Opportunity to work on creating innovative, leading-edge data science pipelines using our state of the art, in-house built big graph tool
Work closely with the developers of the big graph tool you will be building upon
Be a member of a very strong team with mathematicians, ex-Googlers, Ivy League  professors, MBA alumni and telecommunications industry experts
Startup atmosphere



"
https://startup.jobs/data-engineer-azure-tiger-analytics-4090010,Engineer,Data Engineer - Azure,Tiger Analytics ,"Toronto, Canada",Full-Time,"

Tiger Analytics is a fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning and AI. We are the trusted analytics partner for several Fortune 100 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent in various US locations as we continue to build the best analytics global consulting team in the world.
Key Responsibilities
-Define process to analyze and map specific data hierarchies across functions, customers, and systems.
-Collaborate with Data Management Leads to ensure the model and lexicon of the data assets meets users needs.
-Define sources and calculations for metrics and KPIs. Lead metrics standardization in functions, reconciling data as necessary.
-Align hierarchy with corresponding business rules, requirements, and data transformations.
-Define and manage an exception process to manage hierarchies and cross functional metrics.
-Develop and maintain data models.
-Build technical specs and map documentation.
-Partner with IT to develop data architecture standards and guidelines for processing and persisting data on prim and cloud.
-Partner with IT to ensure appropriate standardization, categorization, and hierarchy is followed.
Requirements
-Undergraduate Degree in Computer Science, Engineering, Mathematics, Statistics, or similar field.
-Minimum 5 years of experience in data architecture.
-Strong SQL abilities.
-Deep understanding of data structure and cross-functional hierarchies to address business needs.
-Ability to translate business requirements into technical requirements and reverse engineering.
-Ability to communicate data hierarchy management procedures to non-technical stakeholders.
-Ability to gather requirements to create conceptual, logical, and physical models.
-Ability to perform data profiling to create business mapping and technical design.
-Knowledge of Cloud tools and systems; Azure preferred.
-Knowledge of data storage systems, data factory, and data lake.
-Knowledge of cleansing data and building data pipelines.

"
https://startup.jobs/data-engineer-ii-berkadia-4086061,Engineer,Data Engineer II,Berkadia ,"Ambler, United States",,"

The Opportunity
Berkadia Commercial Mortgage, LLC, seeks a Data Engineer II in Ambler, PA to build data pipeline to merge tested design of data warehousing and the latest big data technologies.  Design and build data projects while securing core data elements. Develop ETL pipelines, in the cloud, to import, process and store data to be used by product teams. Support advanced data processing patterns used and deployed in this framework. Support the legacy data warehouse systems that leverage Microsoft SQL Server stack SQL, SSIS, SSAS, SSRS. Create and modify data warehouse jobs using Kimball Data Warehouse methodology. Design and build data projects while securing core data elements. Research new technology and share knowledge with the team; implement and release data applications in AWS using big data technologies. Assist development efforts across the organization. Understand and adhere to development standards and processes. Use the latest technology to develop ETL pipelines in the cloud to import, process and store data to be used by product teams. Support the legacy data systems that leverage the Microsoft SQL Server stack. Use SQL Language to interact with databases. Perform Process Troubleshooting to understand, identify, and resolve data issues. Perform data transformation for different tasks like data cleansing, aggregation, and formatting. Perform Data Modeling activities to store data in databases and files.
 
REQUIREMENTS    Bachelors degree or foreign equivalent in Computer Science, Computer Engineering, or related field. 5 years of experience using SQL Language to interact with databases. 5 years of experience Process Troubleshooting to understand, identify, and resolve data issues. 5 years of experience performing data transformation for different tasks like data cleansing, aggregation, and formatting. 1 year of experience performing Data Modeling activities to store data in databases and files. Up to 5 national travel required. Remote work available at employer discretion.
LI-DNI 

Berkadia, a joint venture of Berkshire Hathaway and Jefferies Financial Group, is a leader in the commercial real estate industry, offering a robust suite of services to our multifamily and commercial property clients. Powered by deep relationships and industry-changing technology, our people sell, finance, and service commercial real estate, providing support for the entire life cycle of our clients assets. Our unique ownership structure allows us to put the clients interests first and creates a marketplace that delivers a superior experience.
Berkadia complies with the law regarding reasonable accommodations for disabled applicants.  Applicants who require reasonable accommodations to participate in the interview process should contact Talentacquisitionberkadia.com to arrange for such accommodations.
Berkadia is an Equal Opportunity employer and complies with all applicable Federal, State and local laws concerning discrimination in employment.  No question in this Application is intended to elicit information in violation of any such law, nor will any information obtained in response to any question be used in violation of any such law.
By applying to this job opportunity you are acknowledging Berkadias Application Policy and Berkadias Privacy Policy. Applicants have rights under Federal Employment Laws. Please click the following links for more information EEOC, Employee Rights under the FMLA, EPPA.


"
https://startup.jobs/data-engineer-bangalore-demystdata-4085755,Engineer,Data Engineer (Bangalore),DemystData ,"Bengaluru, India",Full-Time,"

Our Solution
Demyst unlocks innovation with the power of data. Our platform helps enterprises solve strategic use cases, including lending, risk, digital origination, and automation, by harnessing the power and agility of the external data universe. We are known for harnessing rich, relevant, integrated, linked data to deliver real value in production. We operate as a distributed team across the globe and serve over 50 clients as a strategic external data partner. Frictionless external data adoption within digitally advancing enterprises is unlocking market growth and allowing solutions to finally get out of the lab. If you like actually to get things done and deployed, Demyst is your new home.
 The Opportunity
As a Data Engineer at Demyst, you will be powering the latest technology at leading financial institutions around the world. You may be solving a fintechs fraud problems or crafting a Fortune 500 insurers marketing campaigns. Using innovative data sets and Demysts software architecture, you will use your expertise and creativity to build best-in-class solutions. You will see projects through from start to finish, assisting in every stage from testing to integration.
To meet these challenges, you will access data using Demysts proprietary Python library via our JupyterHub servers, and utilize our cloud infrastructure built on AWS, including Athena, Lambda, EMR, EC2, S3, and other products. For analysis, you will leverage AutoML tools, and for enterprise data delivery, youll work with our clients data warehouse solutions like Snowflake, DataBricks, and more.
While this is a fully-remote role, successful candidates are expected to be based to Bangalore where Demyst is building its next hub. As you will be supporting clients globally, you will be expected to align your working hours to the US.
 Responsibilities
 Collaborate with internal project managers, sales directors, account managers, and clients stakeholders to identify requirements and build external data-driven solutions Perform data appends, extracts, and analyses to deliver curated datasets and insights to clients to help achieve their business objectives Understand and keep current with external data landscapes such as consumer, business, and property data. Engage in projects involving entity detection, record linking, and data modelling projects Design scalable code blocks using Demysts APIsSDKs that can be leveraged across production projects Govern releases, change management and maintenance of production solutions in close coordination with clients IT teams 
Requirements
 Bachelors in Computer Science, Data Science, Engineering or similar technical discipline or commensurate work experience; Masters degree preferred 1-3 years of Python programming with Pandas experience Experience with CSV, JSON, parquet, Avro, and other common formats Data cleaning and structuring ETL experience Knowledge of API REST and SOAP, HTTP protocols, API Security and best practices Experience with SQL, Git, and Airflow Strong written and oral communication skills Excellent attention to detail Ability to learn and adapt quickly Willingness to align to US working hours Answering the screening questions all applicants who do not provide thoughtful and detailed answers to the two questions provided in the application form will be automatically disqualified 
Benefits
 Distributed working team and culture Generous benefits and competitive compensation Collaborative, inclusive work culture all-company offsites and local get togethers in Bangalore Annual learning allowance Office setup allowance Generous paid parental leave Be a part of the exploding external data ecosystem Join an established fast growth data technology business Work with the largest consumer and business external data market in an emerging industry that is fueling AI globally Outsized impact in a small but rapidly growing team offering real autonomy and responsibility for client outcomes Stretch yourself to help define and support something entirely new that will impact billions Work within a strong, tight-knit team of subject matter experts Small enough where you matter, big enough to have the support to deliver what you promise Transfer to US and Australia offices possible for top performers after two years 
Demyst is committed to creating a diverse, rewarding career environment and is proud to be an equal opportunity employer. We strongly encourage individuals from all walks of life to apply.

"
https://startup.jobs/data-engineer-komodo-health-4081258,Engineer,Data Engineer,Komodo Health ,"New York, United States",,"


We Breathe Life Into Data
At Komodo Health, our mission is to reduce the global burden of disease. And we believe that smarter use of data is essential to this mission. Thats why we built the Healthcare Map  the industrys largest, most complete, precise view of the U.S. healthcare system  by combining de-identified, real-world patient data with innovative algorithms and decades of clinical experience. The Healthcare Map serves as our foundation for a powerful suite of software applications, helping us answer healthcares most complex questions for our partners. Across the healthcare ecosystem, were helping our clients unlock critical insights to track detailed patient behaviors and treatment patterns, identify gaps in care, address unmet patient needs, and reduce the global burden of disease. 
As we pursue these goals, it remains essential to us that we stay grounded in our values be awesome, seek growth, deliver wow, and enjoy the ride. At Komodo, you will be joining a team of ambitious, supportive Dragons with diverse backgrounds but a shared passion to deliver on our mission to reduce the burden of disease  and enjoy the journey along the way. 

The Opportunity at Komodo Health
Komodo Health leverages the latest data engineering technology such as Spark, Airflow, and Snowflake to tackle some of healthcares biggest challenges by transforming extraordinary amounts of data into rich and meaningful insights. As a Data Engineer on the Platform team, you will help lead the development of Komodo Healths platform-enabled workflow tools.  The Komodo platform powers all of Komodos current and future workflow analytical applications and enables 3rd party builders to integrate with, extend, customize, or build on the platform.
Reporting directly to the Engineering Manager, you will be solving complex data challenges while designing and implementing data processing and transformation at a scale that powers state-of-the-art interactive product experiences. You will enable smarter, more innovative uses of healthcare data by building robust data pipelines and implementing data best practices.
Looking back on your first 12 months at Komodo Health, you will have

Worked on foundational pieces of our data platform architecture, pipelines, analytics, and services underlying our platform 
Partnered with Engineering team members, Product Managers, Data Scientists, and customer-facing teams to understand and deliver python packages as well as web-based python services
Designed and developed reliable data pipelines that transform data at scale, orchestrated jobs via Airflow, using SQL and Python in Snowflake andor Spark
Helped implement continuous improvements to our data governance practices and implemented data quality improvements
Implemented technical enhancements to our CICD processes andor built tooling to ensure data consistency and quality
Gained an understanding of the broader Komodo Health data landscape and being part of architectural decisions for the Healthcare Analytics and Platform as a Service offerings

What you bring to Komodo

Expertise in writing enterprise-level code and contributing to large data pipelining and API processing with Python
Experience with SQL and query design on large, complex datasets 
Ability to use a variety of relational, NoSQL, Postgres, andor MPP databases ideally Snowflake on AWS and leading data modeling, schema design, and data storage best practices
Demonstrated proficiency in designing and developing with distributed data processing platforms like Spark and pipeline orchestration tools like Airflow 
A thirst for knowledge, willingness to learn, and a growth-oriented mindset
Committed to fostering an inclusive environment where your teammates feel motivated to succeed
Experience enhancing CICD build tooling in a containerized environment, from deployment pipelines Jenkins, etc, infrastructure as code Terraform, Cloudformation, and configuration management via Docker and Kubernetes
Excellent cross-team communication and collaboration skills

Compensation at Komodo Health
We are committed to providing competitive compensation for all roles at Komodo Health. We carefully consider multiple factors when determining compensation, including your skills, experience, and location while balancing internal equity relative to peers at the company. The targeted base salary range for the Data Engineer role is 123,900 to 186,300 plus a competitive bonus and equity package.
LI-JK1

Where Youll Work
Komodo Health has a hybrid work model; we recognize the power of choice and importance of flexibility for the well-being of both our company and our individual Dragons. Roles may be completely remote based anywhere in the country listed, remote but based in a specific region, or local commuting distance to one of our hubs in San Francisco, New York City, or Chicago with remote work options. 
What We Offer
On top of our commitment to providing competitive, fair pay for all roles at Komodo Health, were proud to offer robust and inclusive benefits to all Dragons at Komodo Health. We offer global time off programs, extensive internal and external career development and learning opportunities, multiple affinity groups celebrating our teams diversity, and an annual wellness and productivity stipend to support you in being your healthiest, best self. 
Equal Opportunity Statement
Komodo Health provides equal employment opportunities to all applicants and employees. We prohibit discrimination and harassment of any type with regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws. 


"
https://startup.jobs/data-engineer-remote-carvana-4075301,Engineer,Data Engineer (Remote),Carvana ,,,"

About ADESA...
ADESA, a Carvana-owned company, currently operates 56 locations throughout the US. Our Vehicle Service  Logistics Centers, some up to 200 acres, provide a wide array of vehicle services including repair  reconditioning, and auction remarketing. Many of our sites serve as market hub distribution centers. Our inventory comprises hundreds of thousands of vehicles across North America from retail to commercial, OEM  more. 
Were excited about the future! As an industry leader, ADESA is poised for a multi-year expansion including huge investments in facilities, massive sales growth, and an ever-increasing inventory of vehicles! We are looking for great people who want to take this journey with us!  
About the position...
We are looking for a data engineer with cloud data warehouse experience and a passion for turning data into information. You will develop data pipelines and data models that underpin our KPIs and business processes. This exciting, fast-paced role requires excellent organizational skills, critical thinking, problem-solving, and teamwork to enable business partners to make informed decisions.
What youll be doing...

Implement data models and data engineering solutions for our cloud data warehouse
Work with business stakeholders to discover ROI and key success metrics to guide our investments in technology and improve our online operations
Analyze data to ensure quality and help assess whether business value can be achieved
Develop SQL to prototype concepts and troubleshoot issues
Create visual solutions to deliver to stakeholders

What you should have

Experience engineering data ingestion and transformation solutions
Demonstrated ability to understand and implement data models, particularly Data Warehouse use cases
SQL and RDBMS experience ex. Snowflake, RedShift, Oracle, SQL Server, MySQL, etc.
Experience with cloud warehousing and analytics ex. Snowflake, BigQuery, etc.
OOP or functional programming experience ex. Python, JavaScript, etc.
Excellent analytical and problem-solving skills with the ability to analyze and break down problems
Drive to match data solutions to business goals
Ability to visualize data Tableau experience preferred
Strong oral and written communication skills
Motivated self-starter, ability to initiate and drive work to completion

Legal stuff
Hiring is contingent on passing a complete background check. This role is not eligible for visa sponsorship.
Carvana is an equal employment opportunity employer.  All applicants receive consideration for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, marital status, national origin, age, mental or physical disability, protected veteran status, or genetic information, or any other basis protected by applicable law.  Carvana also prohibits harassment of applicants or employees based on any of these protected categories.
Please note this job description is not designed to contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.

"
https://startup.jobs/data-engineer-bjss-4065396,Engineer,Data Engineer,BJSS ,"York, United Kingdom",,"

About Us
Were an award-winning innovative tech consultancy - a team of creative problem solvers. Since 1993 weve been finding better, more sustainable ways to solve complex technology problems for some of the worlds leading organisations and delivered solutions that millions of people use every day.
In the last 30 years we won several awards, including a prestigious Queens Award for Enterprise in the Innovation category for our Enterprise Agile delivery approach.
Operating from 26 locations across the world, we bring together teams of creative experts with diverse backgrounds and experiences, who enjoy working and learning in our collaborative and open culture and are committed to world-class delivery.
We want to continue to grow our team with people just like you!
We are DataOps advocates and use software engineering best practices to build scalable and re-usable data solutions to help clients use their data to gain insights, drive decisions and deliver business value. Clients dont engage BJSS to do the straightforward things, they ask us to help on their biggest challenges which means we get to work with a wide range of tools and technologies and there are always new things to learn.
About the Role
BJSS data engineers are specialist software engineers that build, optimise and maintain data applications, systems and services.  This role combines the discipline of software engineering with the knowledge and experience of building data solutions in order to deliver business value.
As a BJSS data engineer youll help our clients deploy data pipelines and processes in a production-safe manner, using the latest technologies and with a DataOps culture.
Youll work in a fast moving, agile environment, within multi-disciplinary teams of highly skilled consultants, delivering modern data platforms into large organisations.You can expect to get involved in variety of projects in the cloud AWS, Azure, GCP, learning about and using data services such as Databricks, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion etc.
About You

Youre an engineer at heart and enjoy the challenge of building reliable, efficient data applications systems, services and platforms.
You have a good understanding of coding best practices and design patterns and experience with code and data versioning, dependency management, code quality and optimisation, error handling, logging, monitoring, validation and alerting.
You have experience in writing well tested object-oriented Python.
You have experience with using CICD tooling to analyse, build, test and deploy your code.
You have a good understanding of design choices for data storage and data processing, with a particular focus on cloud data services.
You have experience in using parallel computing to process large datasets and to optimise computationally intensive tasks.
You have experience in programmatically deploying, scheduling and monitoring components in a workflow.
You have experience in writing complex queries against relational and non-relational data stores.

Some of the Perks

Flexible benefits allowance  you choose how to spend your allowance additional pension contributions, healthcare, dental and more
Industry leading health and wellbeing plan - we partner with several wellbeing support functions to cater to each individuals need, including 247 GP services, mental health support, and other
Life Assurance 4 x annual salary
25 days annual leave plus bank holidays
Hybrid working - Our roles are not fully remote as we take pride in the tight knit communities we have created at our local offices. But we offer plenty of flexibility and you can split your time between the office, client site and WFH
Discounts  we have preferred rates from dozens of retail, lifestyle, and utility brands
An industry-leading referral scheme with no limits on the number of referrals
Flexible holiday buysell option
Electric vehicle scheme
Training opportunities and incentives  we support professional certifications across engineering and non-engineering roles, including unlimited access to OReilly
Giving back  the ability to get involved nationally and regionally with partnerships to get people from diverse backgrounds into tech
You will become part of a squad with people from different areas within the business who will help you grow at BJSS
We have a busy social calendar that you can chose to join quarterly town hallssquad nights outweekends away with families includedoffice get togethers
GymFlex gym membership programme


"
https://startup.jobs/data-engineer-climact-4063185,Engineer,Data engineer,- - CLIMACT ,"Ottignies-Louvain-la-Neuve, Belgium",,"


Use your data skills to work on the most important challenge of our time supporting the world to a net zero carbon society.

Climact is an independent energy  climate company, using models and data to drive down carbon emissions for both private and public organisations. We develop sophisticated, user-friendly and open-source models on the energy and climate systems at the global, national and city levels - and use our findings to improve energy and climate policy. Were looking for enthusiastic new colleagues to join us at a period of rapid growth, to be part of an innovative team thats having a global impact in tackling climate change with our asset-based consulting. Models can truly support the energy transition. The whole Climact team is focused on leveraging them towards climate action.
See examples of our work at www.climact.com and LinkedIn.

Climact is recruiting a talented data engineer who will support the development of our existing tools with the rest of the technical team. Climacts vision is to be able to provide top notch modelling of the energy system based on open-source models and data, and to be able to leverage these assets for our consulting practices, supporting both public and private actors towards a carbon-free energy system. The technical team is growing as we expand our current models to more countries to cover all major economies, and also expanding the type of analysis and services we can provide e.g., the impact on power and gas networks at the European level, minerals, air quality, water use. Your role will be to support the increasing role and quantity of data in our models.
The current Climact technical team will give you valuable support to complement your technical competency with programming and data, leveraging your ability to impact. The role will report to this technical team and the wider management.
Role and responsibilities
The new position, within the technical team, includes the following responsibilities

Contribute to the development of existing tools e.g. Pathways Explorer, EUCityCalc, EUCalc, My2050
Orchestrate and automate existing ETL pipelines
Distribute data through API
Manage related infrastructure including cloud infrastructure and database
Automate release process of existing tools
Contribute to the data science and some architectural design on various programming tools, in co-creation with the energy and climate consultants
Contribute to developing analytics and data science expertise on Climact consulting engagements


"
https://startup.jobs/data-engineer-intern-fall-2023-ripple-4055260,Engineer,Data Engineer Intern (Fall 2023),Ripple ,"Toronto, Canada",Internship,"


At Ripple, were building a world where value moves like information does today. Its big, its bold, and were already doing it. Through our crypto solutions for financial institutions, businesses, governments and developers, we are improving the global financial system and creating greater economic fairness and opportunity for more people, in more places around the world. And we get to do the best work of our career and grow our skills surrounded by colleagues who have our backs. 
If youre ready to see your impact and unlock incredible career growth opportunities, join us, and build real world value.


Data Engineer Intern
Ripple is the worlds only enterprise blockchain solution for global payments. Today the world sends more than 155 trillion across borders, however, the underlying infrastructure is dated and flawed. Ripple solves this problem. We connect banks, payment providers, corporations, and digital asset exchanges via RippleNet to provide one frictionless experience to send money globally. Ripples distributed financial technology outperforms todays banking infrastructure by driving down costs, increasing processing speeds and delivering end-to-end visibility into payment fees, timing and delivery.
Ripple is growing rapidly and we are looking for a results-oriented and passionate Software Engineer to help build our data platform. The platform enables everyone in the company to make data driven strategic decisions. You have a passion for the craft of software engineering and take pride in applying those skills to data challenges.
WHAT YOULL DO
Youll collaborate with partner engineering teams and business stakeholders to build new capabilities to the Data platform to use data to drive Ripples strategy and success.The entire Data Platform team works collaboratively and is a strong partner for teams across the company. You will get to meet and learn from diverse and talented colleagues.
Data Engineer candidates will work to help build, improve, maintain data pipelines and foundational frameworks for data ingestion and data transformation, and develop creative ways to design and implement scalable and reliable backend infrastructure of the Data applications.
WHAT WERE LOOKING FOR



Currently enrolled in an undergraduate, graduate or PHd program, preferably in a science or quantitative field
Coursework  previous intern experience with software engineering, ideally involving data oriented applications Python, Java or other programming languages
Experience in building ETLELT data pipelines. Real time pipelines are a huge plus.
Experience writing SQL queries in data warehouses such as Redshift, BigQuery
Knowledge of building REST API endpoints 
Exposure to Hadoop and NoSQL databases like hbase, cassandra etc is a plus
Exposure to CICD via Airflow or similar tool a plus
Experience with Terraform or similar tools a huge plus
Excellent written and verbal communication skills
Attention to detail and a commitment to excellence



WHAT WE OFFER INTERNS
Were looking to increase what we already offer, but you can expect



Competitive regional hourly pay
11 Mentoring
Dedicated ""buddy""- well partner you with someone at Ripple who will act as friend, advocate, liason, guide and so much more!
Social and professional development events In-person and virtual
Ripple offers lunches twice a week for non-remote employees
On that note, Ripple has a very flexible remote policy - You get the choice of where you work from.
The chance to work in a fast-paced start-up environment with experienced industry leaders
A learning environment where you can dive deep into the latest technologies and make an impact
Weekly company meeting - ask me anything style discussion with our Leadership Team
Design laptop, software and headphones on loan






 
 
 
LI-DNI




WHO WE ARE
Do Your Best Work

The opportunity to build in a fast-paced start-up environment with experienced industry leaders
A learning environment where you can dive deep into the latest technologies and make an impact.  A professional development budget to support other modes of learning.
Thrive in an environment where no matter what race, ethnicity, gender, origin, or culture they identify with, every employee is a respected, valued, and empowered part of the team.
Ripple is Flexible First you have the option to work from home, from our offices, or a combination of the two around our centers of gravity 15 global offices.
Weekly all-company meeting - business updates and ask me anything style discussion with our Leadership Team
We come together for moments that matter which include team offsites, team bonding activities, happy hours and more!

Take Control of Your Finances

Competitive salary, bonuses, and equity
Competitive benefits that cover physical and mental healthcare, retirement, family forming, and family support
Employee giving match
Mobile phone stipend

Take Care of Yourself

Twice a quarter RR days so you can rest and recharge
Generous wellness reimbursement and weekly onsite  virtual programming
Generous vacation policy - work with your manager to take time off when you need it
Industry-leading parental leave policies. Family planning benefits.
Catered lunches, fully-stocked kitchens with premium snacksbeverages, and plenty of fun events

Benefits listed above are for full-time employees.

Ripple is an Equal Opportunity Employer. Were committed to building a diverse and inclusive team. We do not discriminate against qualified employees or applicants because of race, color, religion, gender identity, sex, sexual identity, pregnancy, national origin, ancestry, citizenship, age, marital status, physical disability, mental disability, medical condition, military status, or any other characteristic protected by local law or ordinance.
 
Please find our UKEU Applicant Privacy Notice and our California Applicant Privacy Notice for reference.


"
https://startup.jobs/data-engineer-viator-tripadvisor-4050363,Engineer,Data Engineer - (Viator),TripAdvisor ,"Lisbon, Portugal",,"

The Viator Engineering team is distributed across Europe and is responsible for Viator.comhttpswww.viator.com
Were looking for a Data Engineer to join our fast-growing team in a hybrid-working setup based out of the Lisbon TripAdvisor office. 
Viator is a Tripadvisor company that makes it easy to find and book something youll love to do. With an industry leading selection of high-quality experiences, Viator gives millions of travellers a month something new to discover, both near and far from home. We believe that we are better together, and at Viator we welcome you for who you are. Our workplace is for everyone, as is our people powered platform. At Viator, we want you to bring your unique identities, abilities and experiences, so we can collectively revolutionise travel and together find the good out there.
Within Viator we have a fun and friendly environment where the key objective is getting things done. Working closely alongside product managers and designers, our engineers are part of the full process from software design, to code, to test, to deployment and back again. Most of our data engineers release code to production every few days and we have a diverse data platform.  Viators data stack includes Java, SQL, Kubernetes, Kafka, Spark, Terraform, and runs on AWS. You dont need to have experience with everything listed but you do need to be hungry to learn. 
What youll get
Highly competitive salary along with the following
-   Annual performance related bonus-   Generous stock RSU award upon joining, with additional awards annually-   Regular salary reviews and excellent career growth opportunities-   Very flexible working hours-   Free meals in the office-   Starting from 1104 annual stipend for costs outside the office such as gym membership or home office set up-   Full family private healthcare and dental-   Excellent contributory pension-   Critical illness and full life cover-   Standard 22 days paid holiday not including bank holidays-   Holiday break during the last week of the year-   ""Summer Fridays"" scheme allowing extra days off during the summer-   Daily meal allowance of 8.80 per working day-   Access to LinkedIn Learning platform
Application process
-   30 minute call with a recruiter to learn more about the role-   30 minute technical interview with someone from the Viator Data Engineering team-   Four one-hour interviews with members of the team, covering technical topics - including some coding - and what you would bring to Viator
We strive to create an accessible and inclusive experience for all candidates. If you need a reasonable accommodation during the application or the recruiting process, please make sure to reach out to your individual recruiter or our team at greenhousetripadvisor.com. 
Viator
 
 

"
https://startup.jobs/data-engineer-gcp-66degrees-4038711,Engineer,"Data Engineer, GCP",66degrees ,,,"

Overview of 66degrees
66degrees is a leading Google Cloud Premier Partner. We believe that engineering takes heart. Focusing exclusively on Google Cloud, we help our clients achieve the most innovative and disruptive transformations in their industries.
66degrees is dedicated to providing our employees with a challenging and exciting work environment without forgetting to have some fun along the way. Lets get you here.
Overview of Role
66degrees is looking for smart, energetic, and personable technology business professionals for a Data Engineer position.  As a Cloud Data Engineer you will be a part of data management and engineering projects within GCP environments, including migrating, analyzing, and managing data structures within GCP. Assist in the design, definition, development, and testing of GCP data solution components. Capable of quickly adapting and building plans around migrating data from legacy applications, you will work closely with the GCP Security Team and other SME roles to ensure datastores are protected. 
Responsibilities

As a Data Engineer, youll guide customers on how to ingest, store, process, analyze, and explorevisualize data on the Google Cloud Platform.
Work with technical and business leads to transfer global business requirements into sound solutions and implementation.
Lead data management within a GCP environment, including migrating, analyzing, and managing data structures within GCP.
Work with data scientists and software engineers to support data acquisition activities, data solution ideation, and implementation.
Share support responsibilities for implemented components.
Provide extensive technical, strategic advice and guidance to key stakeholders around the data transformation effort.
Capable of quickly adapting and building plans around migrating data from legacy applications.
Work closely with the GCP Security Engineer and SME role to ensure datastores are protected.
Liaise between clients and developers to ensure that all data requirements are met.

Qualifications

5 years of experience with data engineering, cloud architecture, or working with data infrastructure.
3 years of experience with working within Google Cloud Platform GCP or a platform including Azure or AWS.
Experience with the primary managed data services within GCP, including Cloud Bigtable, Cloud Spanner, Cloud SQL, or BigQuery.
Experience with relational databases.
Experience in migrating legacy databases and applications.
Google Cloud Data Engineer certification is a plus. 

 66degrees is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to actual or perceived race, color, religion, sex, gender, gender identity, national origin, age, weight, height, marital status, sexual orientation, veteran status, disability status or other legally protected class.
 

"
https://startup.jobs/data-engineer-dbt-developer-remotebase-4023442,"Developer,Engineer",Data Engineer (DBT Developer),Remotebase ,,Full-Time,"

We are seeking a Data Engineer with 4 years of experience to join our team. The ideal candidate will have a strong background in working with online advertising data and experience with marketing attribution cases.
In this role, you will be responsible for developing and maintaining data pipelines using DBT, BigQuery, and other tools. You will work closely with cross-functional teams to ensure that data is accurate, complete, and accessible to all stakeholders.Key responsibilities
 Develop and maintain data pipelines using DBT and other tools Use BigQuery to perform data analysis and support data-driven decision making Collaborate with cross-functional teams to identify and prioritize data needs Utilize Metabase and Snowplow Analytics to create dashboards and reports for stakeholders Work with Kafka and Neo4J to support real-time data processing and analysis 
Requirements
 3 years of experience as a Data Engineer or similar role GCP, AWS and Cloud Services as preferred requirement Strong experience with DBT and Neo4J  BigQuery Familiarity with Metabase, Snowplow Analytics, Kafka Experience with any programming language for example Python Experience working with online advertising data and marketing attribution cases Ability to work independently and handle multiple projects simultaneously Excellent problem-solving skills and attention to detail Strong communication and collaboration skills 
Benefits
 Fully remote with office optional. You decide when you would like to work from home and when from the office. Flexible timings. You decide your work scheduled. Medical Insurance Company laptop Paid leaves Gym allowance Market competitive salaries Insane learning and growth Annual company retreat 

"
https://startup.jobs/data-engineer-proprietary-research-point72-3990640,Engineer,"Data Engineer, Proprietary Research",Point72 ,"New York, United States",,"

ABOUT PROPRIETARY RESEARCH
On our proprietary research teamMarket Intelligenceyoull partner with our investment professionals and Compliance team to uncover insights about companies, industries, and the broader economy through deep fundamental research and applying data science and engineering techniques to alternative data sets. Youll work alongside a talented team with diverse skills, backgrounds, and perspectives. Our researchers, product managers, and data scientists and engineers work together to build compliant research products that answer the questions posed by our investment professionals. We look for other bright, motivated, and collaborative people to join our team and grow with usmore than 90 of the leaders in our group were promoted from within.
 
ROLE SUMMARY
Data Engineers on the Proprietary Research team design and build solutions that enable investment professionals to effortlessly, extract insights from large, complex compliance approved structured and unstructured data sets. As a member of our Data Engineering team, you will work closely with investment professionals, researchers, and data scientists to design, build, and launch robust end-to-end data pipelines and tools that that help extract the most value out of Point72s data assets. In this role, you will

Develop and support big data processing pipelines including but not limited to ingestion, transformation, and end-customer delivery
Build out cloud-based infrastructure using distributed techniques for other data engineers  data scientists  researchers
Be at the forefront of new technology developments with respect to the handling and processing of big data and conduct proof of concepts evaluations of new technologies
Build and support visualization and exploration capabilities around our big data sets

 
WHAT EXCITES YOU

Transforming large unstructured, data sets into valuable investment research inputs
Learning and working with industry-leading cloud-computing technologies
Speaking with experts both inside and outside the Firm to understand data needs and offerings
Fast-paced work environments that at times require switching gears to address the needs of the business
Working as part of a cross-functional team made up of investment, research, and compliance professionals

 
WHAT EXCITES US

Excellent attention to detail, organization, and project management skills
Superb business intuition and a solution orientated, methodological approach to problem solving
Ability to collaborate and build relationships across multiple business units within the Firm
People who elevate the room through their work ethic, curiosity, and solution-orientated attitude
Adherence to the highest ethical standards working closely with the Firms Compliance team

 
WHATS REQUIRED  

Experience or demonstrated interest in big data technologies
1-2 years of experience in Data Engineering or related field 
Strong experience in Python Development
Experience with Spark or Scala
Ability to devise novel and innovative solutions to challenges
Knowledge ofexperience with graph databases is a plus

 
WHAT SUCCESS LOOKS LIKE

Integrity  You demonstrate 100 commitment to the highest ethical standards
Ownership  You take charge of your work, uphold your commitments, and always do your best
Commerciality  You focus on what matters, ask necessary questions, and are diligent about not wasting time
Humility  You welcome and are receptive to feedback and learn from past mistakes
Adaptability  You can triage business needs and context switch quickly and efficiently
Admirability  You elevate the room through your work ethic, domain knowledge, and work product

 
WE TAKE CARE OF OUR PEOPLE
We invest in our people, their careers, their health, and their well-being. When you work here, we provide

Fully paid health care benefits
Generous parental and family leave policies
Mental and physical wellness programs
Volunteer opportunities
Non-profit matching gift program
Support for employee-led affinity groups representing women, minorities and the LGBTQ community
Tuition assistance
A 401k savings program with an employer match and more

 
ABOUT POINT72
Point72 Asset Management is a global firm led by Steven Cohen that invests in multiple asset classes and strategies worldwide. Resting on more than a quarter-century of investing experience, we seek to be the industrys premier asset manager through delivering superior risk-adjusted returns, adhering to the highest ethical standards, and offering the greatest opportunities to the industrys brightest talent. Were inventing the future of finance by revolutionizing how we develop our people and how we use data to shape our thinking. For more information, visit www.Point72.comworking-here.
The annual base salary range for this role is 130,000-155,000, which does not include discretionary bonus compensation or our comprehensive benefits package. Actual compensation offered to the successful candidate may vary from posted hiring range based upon geographic location, work experience, education, andor skill level, among other things.  

"
https://startup.jobs/data-engineer-scalapay-3985384,Engineer,Data Engineer,Scalapay ,"Barcelona, Spain",Full-Time,"

Thanks to our innovative BuyNow PayLater payment solution, Scalapay is transforming the way more than 3.5 million customers buy online and in-store, empowering 5,000 merchants to give their customers magical shopping experiences. 

Being only 3 years old didnt stop us from becoming a unicorn  We have raised over 700mln and we did this thanks to a team built around our 4 core values createmagic staycurious beimpactful playasateam. 

This is where your magic happens. If you love it, Scalapay it 

 
THE MISSION
 
Data-driven decisions are the centre of our efforts to automate processes and support decisions across the business. 

As Data Engineer, your role will focus on developing distributed software for large-scale heterogeneous data processing, the development of ETL and reverse ETL pipeline.
 
You will be responsible for
- Data modelling
- Data lineage
- Data quality  Data observability
- SLA, performance and scalability 
 
Youll be required to discuss requirements for data access and integrations with colleagues across the business, design solutions to the companys problems and implement these using modern software processes in the cloud.
The role gives a large amount of autonomy and the chance to work with a top-notch team of experienced data experts.
 
Who we are looking for 
- Strong Python and SQL experience
- Experience building and deploying ETL pipelines 
- Experience with API integration
- Ability to scope projects and decompose work into tasks
- Familiarity with Linux-like development stacks, including Git, Kubernetes, etc
- Familiarity with AWS infrastructure such as S3, Athena, Glue, etc
- Knowledge of Data Modeling  Data Observability is a plus
- Ability to engage with colleagues on business processes
- A Bachelors degree in computer science or a related field
 
Why you should join Scalapay
- Attractive packages based on skills and experience
- International environment with significant challenges to be met every day
- Lots of opportunities to work with a team of industry tech leaders who are focused on delivering products that offer exceptional user experiences
- Personalised support to accelerate your professional growth and take ownership of the products you deliver we want to help you grow!
- Latest technologies and being encouraged to bring your flair to the role
 
Recruitment Process
1 A quick chat with one of our Talent Acquisition team members
2 The first interview with the Hiring Manager to deep dive into your experiences and better understand your motivation
3 A case study to test your hard skills
4 A Meet The Team session where youll get to meet different people at Scalapay and see if youll fit into our culture


Want to learn more? Dont hesitate to explore our Careers website and Careers website, and our LinkedIn, WTTJ, Meritocracy and Glassdoor pages. 
 

Pro tip send your CV in English 

Super Pro tip we know that application processes can be scary and frustrating but we look for talent, not people that tick all our boxes.
 
We believe in the power of diversity Scalapay is an Equal Opportunity Employer for any minority, disability, gender identity or sexual orientation.


"
https://startup.jobs/data-engineer-xor-security-3984687,Engineer,Data Engineer,XOR Security ,"Reston, United States",,"

XOR Security is currently seeking a Data Engineer to support an Advanced Cyber Analytics team. This program provides engineering and operational support for targeted threat monitoring and response capabilities requiring analysts to have advanced levels of experience in security event monitoring, incident response, malware analysis and reverse engineering, cyber intelligence, insider threat, and penetration testing.  This position will focus on expanding and optimizing our data, data pipeline architecture and data analytics capabilities. The selected applicant will support our software developers and analysts to define problems, build models, and perform analysis to identify alternatives and recommendations to maximize operational efficiency. The applicant should also be able to create visualizations, find trends in data sets and develop algorithms for actionable reporting. The applicant will need to effectively communicate the results of analysis and describe complex mathematical methods, applications, and results to the client and teammates ranging from technical and non-technical disciplines. 
Required Qualifications

TSSCI
BS or MS degree in IT, Engineering, Mathematics, or related field.
Individuals must have a minimum of five 5 years experience building, optimizing and analyzing cyber security datasets.
Experience designing and deploying and integrating with Big Data systems, ELK, Hadoop, Splunk or similar.
Experience deploying and managing large-scale Elasticsearch clusters which are scalable and reliable including performing Elasticsearch performance and configuration tuning.
Monitoring solutions for the Elastic Stack Including building and deploying visualizations in Kibana.
Assisting with designing and developing highly scalable Data Pipelines that incorporate complex transformations and efficient code.
Assisting with ingestion Pipelines, integration APIs, and provide Elasticsearch tuningoptimizing based on application needs.
Experience with machine learning, applied probability, and statistical methods and experience with Neural Networks.
Individuals must have experience in pattern recognition and the ability to identify relationships between features.
Experience scripting with Python, Bash or PowerShell.
Familiarity with streaming analytics.
Ability to identify and determine software, services, and process necessary to exact into sustainable cyber solutions.
Analyzes operational requirements, business needs, and operational data to support the development of applications and high-performance data processing systems.
Strong logicalcritical thinking abilities, especially analyzing existing application architectures, and developing a good understanding of data models.
Strong desire to learn new and emerging techniques and technologies to solve complex cybersecurity related tasks.
Experience working within an Agile andor DevOps development framework.
Ability to collaborate and share initiatives in developing  innovating solutions.
Must be inquisitive and do what if analysis on data, questioning existing assumptions and processes.
Have the ability to handle multiple competing priorities in a fast-paced environment where priorities change rapidly.
Excellent written and verbal communication skills.
Security certification or have the ability to obtain this certification within the 120 days of employment.

Desired Qualifications

Practical cyber security experience.
Experience in Data Science relative to data processing, data analysis, and data interpretation.
Proficient in basic software development using Java and or Python.
Experience with data mining, descriptive and predictive modeling, risk analysis, streaming analytics, anomaly detection, exploratory data analysis, ETL, event log processing, mathematical modeling, graphnetwork analysis, data visualization, text mining, data access, storage and retrieval.
Experience in DevOps and DevSecOps - Docker, Vagrant, Jenkins, Puppet, Chef 
Relevant Security Certifications Security , CISSP, SANS Certifications, Network , OSCP etc.
Support the operational planning and development of cyber threat emulation, cyber hunt, and tactical operations.

Closing Statement
XOR Security offers a very competitive benefits package including health insurance coverage from the first day of employment, 401k with a vested company match, vacation and supplemental insurance benefits.
XOR Security is an Equal Opportunity Employer EOE. MFDV.
Citizenship Clearance RequirementApplicants selected may be subject to a government security investigation and must meet eligibility requirements - US CITIZENSHIP and Top SECRET CLEARANCE REQUIRED.
 

"
https://startup.jobs/data-engineer-ii-goguardian-2-3966973,Engineer,Data Engineer II,GoGuardian ,,,"


What We Do
 
At GoGuardian, were helping build a future where all learners are ready and inspired to solve the worlds greatest challenges. Our award-winning system of learning solutions is purpose-built for K-12 and trusted by school leaders to promote effective teaching and equitable engagement while helping empower educators to keep students safe. 
 
What Its Like to Work at GoGuardian
 
We are a fast-growing learning company that thrives on making bold moves and setting high standards. Working with us means joining a mostly! remote team of diverse, passionate, mission-driven employees who are inspired by our vision, dedicated to our customers, and comfortable with rapid change. We balance our rocketship pace with a work culture that offers unbridled support, allyship, and inclusivityto say nothing of fun! Join our Neurodiversity ERG, attend an online magic show and bring your kids, head to a DEI Community of Practice session, share a pet photo with other dog lovers, or relax in an online meditation class. Our culture has earned us accolades! We have been named one of Built Ins 2022 Best Places to Work and are certified as a Great Place to Work.
 

What Youll Do

Develop new ETL processes to fulfill the data needs of data science and other departments
Utilize expertise in data modeling, ETL architecture, and report design for department initiatives
Produce detailed documentation including data flow diagrams, logical diagrams, and physical diagrams as needed
Ensure the data collection pipeline and data analysis infrastructure meet the needs of the business
Acquire strong knowledge of data structures, analysis, replication, and distributed relational data  database mapping
Assist with the code review process for purposes of learning, asking new questions, and finding errors
Assist with the development of new scripts, KPIs, and dashboards
All other duties as assigned

Who You Are

Bachelors degree or equivalent experience
4 years of experience buildingoperating systems for data extraction, ingestion, and processing of large data sets
Experience with modern data warehousing tools such as Redshift, Big Query, Snowflake, and Azure Synapse along with some visualization experience using Tableau, Google Analytics, Amplitude, etc.
Hands-on experience with Python, SQL, and NoSQL query language along with data warehouse design and maintenance
Experience working with data orchestration tools such as Airflow, Luigi, AWS Step Functions, Segment
Strong attention to detail, analytical mindset, and highly organized
Ability to work independently within the policies, processes, and procedures defined for the team
Desire to work in a fast-paced startup atmosphere requiring constant learning
Strong technical aptitude and demonstrated ability to quickly evaluate and learn new technologies
Strong interpersonal skills, with the ability to work independently and within a team environment

 


 
Please share this with your friends or co-workers who may be interested in working at GoGuardian! We have multiple openings and are always looking for talented people. 

 
GoGuardian is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. GoGuardian does not discriminate against employees, applicants, interns or volunteers on the basis of race, religion, color, national origin, ancestry, physical disability, mental disability, medical condition, pregnancy, marital status, sex, age, sexual orientation, military and veteran status, registered domestic partner status, genetic information, gender, gender identity, gender expression, or any other characteristic protected by applicable law.
 
GoGuardians Job Applicant Privacy Policy is located here. 
 
BI-Remote


"
https://startup.jobs/data-engineer-breathe-battery-technologies-2-3961415,Engineer,Data Engineer,Breathe Battery Technologies ,"London, United Kingdom",Full-Time,"

Our mission is to create technologies that enable people to breathe cleaner air. Through continuous research and development, we are pioneering new ways to utilise battery systems and enhance their health, charging, cycle life and range. Headquartered in London, we work in partnership with global OEMs in the automotive, robotics and consumer electronics industries to deliver advanced battery system performance and experiences. We are developers of the worlds first health adaptive battery charging software.

What you will be doing
You will join our battery product team and will be developing advanced battery management solutions for automotive and consumer electronics. You will be developing an intelligent and scalable data pipeline that handles incoming data streams and serves data to internal RD functions and to external clients . You will be collaborating with the engineering team to identify and define data processing framework and automation opportunities to promote efficiency. You will be building data tools to support timely delivery of commercial projects and product development in our battery team.

Your responsibilities
 Developing and implementing our data processing and analytics pipeline Development of and responsibility for data analysis tools, including scripting and automation, parameter extraction, exploration and data visualisation Providing data pipeline support to the engineering and commercial teams Engaging in the battery management software product development and delivery, to identify and implement improvements in the data pipeline 
Requirements
 Bachelors degree or higher in highly relevant area 2 years in relevant industry or research experience Proficient with data analysis and visualisation in MATLAB or Python Experience with SQL is a plus but not essential A team player with a proactive and open-minded attitude Kind and conscientious 
Benefits
 A small, talented and cohesive team delivering exciting products  The ability to have a big say in how the company is run and where we are going Share options scheme 34 days holidays per annum including public holidays Ethical pension scheme Flexible working hours Central London location 

"
https://startup.jobs/data-engineer-robinhoodapp-3956924,Engineer,Data Engineer,Robinhood ,"New York, United States",,"


Join a leading fintech company thats democratizing finance for all.
Robinhood was founded on a simple idea that our financial markets should be accessible to all. With customers at the heart of our decisions, Robinhood is lowering barriers and providing greater access to financial information. Together, we are building products and services that help create a financial system everyone can participate in.
As we continue to build... 
Were seeking curious thinkers looking to co-author the next chapters of our story. Joining now means helping shape our vision, structures and systems; playing a key-role as we launch into our ambitious future.

About the team 
The preferred location for this position is in or around Robinhoods offices in Menlo Park, CA or New York, NY with in-office work capabilities, as may be required by management.
Robinhood is a metrics driven company and data is foundational to all key decisions from growth strategy to product optimization to our day-to-day operations. We are looking for a Senior Data Engineer to build and maintain foundational datasets that will allow us to reliably and efficiently power decision making at Robinhood. These datasets include application events, database snapshots, and the derived datasets that describe and track Robinhoods key metrics across all products. Youll partner closely with engineers, data scientists and business teams to power analytics, experimentation, and machine learning use cases. We are a fast-paced team in a fast growing company and this is a unique opportunity to help lay the foundation for reliable, impactful, data-driven decisions across the company for years to come.
What youll do day-to-day

Help define and build key datasets across all Robinhood product areas. Lead the evolution of these datasets as use cases grow.
Build scalable data pipelines using Python, Spark and Airflow to move data from different applications into our data lake.
Partner with upstream engineering teams to enhance data generation patterns.
Partner with data consumers across Robinhood to understand consumption patterns and design intuitive data models.
Ideate and contribute to shared data engineering tooling and standards.
Define and promote data engineering best practices across the company.

About you

CS background or any other relevant fields of study.
Strong product mindset
4 years of experience and a Bachelors degree or 3 years and a Masters degree 
Experience building high-quality data solutions
Strong analytical and problem solving skills.
Expertise building data pipelines using open source frameworks Hadoop, Spark, etc
Expertise in one or more programming languages ideally Python.
Strong SQL Presto, Spark SQL, etc skills.
Familiarity with data visualization tools Looker, Tableau, etc.
Great communication skills and ability to democratize data through actionable insights and solutions.

Bonus points

Passion for working and learning in a fast-growing company.

The expected salary range for this role is based on the location where the work will be performed and is aligned to one of 3 compensation zones. This role is also eligible to participate in a Robinhood bonus plan and Robinhoods equity plan.US Zone 1 157000 - 185000US Zone 2 139000 - 163000US Zone 3 122000 - 144000

Base pay for the successful applicant will depend on a variety of job-related factors, which may include education, training, experience, location, business needs, or market demands. You can view comp zones for our US office locations in the table below. For other locations not listed, compensation can be discussed with your recruiter during the interview process.
Office locations by comp zoneUS Zone 1 Menlo Park, NYC, Seattle, Washington DC US Zone 2 Denver, Westlake Dallas, Chicago US Zone 3 Lake Mary

Were looking for more growth-minded and collaborative people to be a part of our journey in democratizing finance for all. If youre ready to give 100 in helping us achieve our missionwed love to have you apply even if you feel unsure about whether you meet every single requirement in this posting. At Robinhood, were looking for people invigorated by our mission, values, and drive to change the world, not just those who simply check off all the boxes.
Robinhood promotes diversity and provides equal opportunity for all applicants and employees. We are dedicated to building a company that represents a variety of backgrounds, perspectives, and skills. We believe that the more inclusive we are, the better our work and work environment will be for everyone. Additionally, Robinhood provides reasonable accommodations for candidates on request and respects applicants privacy rights. To review Robinhoods Privacy Policy please visit Robinhood - US Applicant Privacy Policy. If you are an applicant located in the UK or EEA, please visit the Robinhood UKEEA Applicant Privacy Policy.
Click here to learn more about Robinhoods Benefits.


"
https://startup.jobs/data-engineer-okx-3951311,Engineer,Data Engineer,OKX ,,,"

About OKX
OKX is a leading crypto trading app, and a Web3 ecosystem. Trusted by more than 20 million global customers in over 180 international markets, OKX is known for being the fastest and most reliable crypto trading app of choice for investors and professional traders globally.
Since 2017, OKX has served a global community of people who share a common interest in participating in a new financial system that is designed to be a level playing field for everyone. We strive to educate people on the potential of crypto markets and how to invest Beyond the OKX trading app, our Web3 wallet, known as MetaX, is our latest offering for people looking to explore the world of NFTs and the metaverse while trading GameFi and DeFi tokens.
 
About the team
OKX data team is responsible for the whole data scope of OKG, from techincal selection, architecture design, data ingestion, data storage, ETL, data visualization to business intelligence and data science. We are data engineers, data analysts and data scientists. The team has end-to-end ownership of most of the data at OKx throughout the whole data lifecycle including data ingestion, data ETL, data warehouse and data services.  As a data engineer of the team, you will work with the team to leverage data technologies to empower evidence-based decision-making and improve the quality of the companys products and services.
 
Responsibilities

Design and build resilient and efficient data pipelines for both batch and real-time streaming data
Architect and design data infrastructure on cloud using industry standard tools
Execute projects with an Agile mindset
Build software frameworks to solve data problems at scale
Collaborate with product managers, software engineers, data analysts and data scientists to build scalable and data-driven platforms and tools
Ensure data integrity and scalability through enforcement of data standards. Improve data validation and monitoring processes to proactively prevent issues and quickly identify issues. Drive resolution on the issues.
Define, understand, and test externalinternal opportunities to improve our products and services.

 
Requirements

Bachelors Degree in Computer Science or have equivalent professional experience
Solid Experience with data processing tools such as Spark, Flink
Solid Experience implementing batch and streaming data pipelines
Solid experiences in PythonGoScalaJava.
In-depth knowledge of both SQL and NoSQL databases, including performance tuning and troubleshooting
Familiar with DevOps tools such as Git, Docker, k8s
Experience with the cloud e.g. AWS, Ali Cloud, GCP, Azure
Be proficient in SQL, familiar with advanced SQL features such as window functions, aggregate functions and creating scalar functionsuser-defined functions.
Proven successful and trackable experience in full end-to-end data solutions involving data ingestion, data persistence, data extraction and data analysis.
Self-driven, innovative, collaborative, with good communication and presentation skills
Fluent in English, both written and spoken.

Preferred Qualifications

Experience in FinTech, eCommerce, SaaS, AdTech, or Digital Wallet business industries.
Experience in working with teams across offices and timezones is a plus.
Experience in big data tools such as AmplitudeTableauQlikView, Ali Cloud DataWorks, MaxCompute, Hadoop, Hive, Spark and HBase is a big plus.


"
https://startup.jobs/data-engineer-stackadapt-3929148,Engineer,Data Engineer,StackAdapt ,,Full-Time,"

StackAdapt is a self-serve advertising platform that specializes in multi-channel solutions including native, display, video, connected TV, audio, in-game, and digital out-of-home ads. We empower hundreds of digitally-focused companies to deliver outcomes and exceptional campaign performance everyday. StackAdapt was founded with a vision to be more than an advertising platform, its a hub of innovation, imagination and creativity.

Were looking to add Data Engineers to our data team! This team works on solving complex problems for StackAdapts digital advertising platform. Youll be working directly with our data scientists, data engineers, Engineering team, and CTO on building pipelines and ad optimization models. With databases that process millions of requests per second, theres no shortage of data and problems to tackle.

Want to learn more about our Data Science Team httpsalldus.comieblogpodcastsaiinaction-ned-dimitrov-stackadapt

Learn more about our team culture here httpswww.stackadapt.comcareersdata-science

Watch our talk at Amazon Tech Talks httpswww.youtube.comwatch?vlRqu-a4gPuU


StackAdapt is a Remote First company, and we are open to candidates located anywhere in the US for this position.
What youll be doing 

Design modular and scalable real time data pipelines to handle huge datasets
Understand and implement custom ML algorithms in a low latency environment
Work on microservice architectures that run training, inference, and monitoring on thousands of ML models concurrently

What youll bring to the table

Have the ability to take an ambiguously defined task, and break it down into actionable steps
Have deep understanding of algorithm and software design, concurrency, and data structures
Experience in implementing probabilistic or machine learning algorithms
Interest in designing scalable distributed systems
A high GPA from a well-respected Computer Science program
Enjoy working in a friendly, collaborative environment with others

StackAdapters enjoy

Competitive salary  equity
401K matching
3 weeks vacation  3 personal care days  1 Culture  Belief day  birthdays off
Access to a comprehensive mental health care platform
Full benefits from day one of employment
Work from home reimbursements
Optional global WeWork membership for those who want a change from their home office
Robust training and onboarding program
Coverage and support of personal development initiatives conferences, courses, etc
Access to StackAdapt programmatic courses and certifications to support continuous learning
Mentorship opportunities with industry leaders
An awesome parental leave policy
A friendly, welcoming, and supportive culture
Our social and team events!


LI-KR1


StackAdapt is a diverse and inclusive team of collaborative, hardworking individuals trying to make a dent in the universe. No matter who you are, where you are from, who you love, follow in faith, disability or superpower status, ethnicity, or the gender you identify with if youre comfortable, let us know your pronouns, you are welcome at StackAdapt. If you have any requests or requirements to support you throughout any part of the interview process, please let our Talent team know.

About StackAdapt

Weve been recognized for our diverse and supportive workplace, high performing campaigns, award-winning customer service, and innovation. Weve been awarded


2023 Best Workplaces for Women by Great Place to Work
Top 20 on Ad Ages Best Places to Work 2023
Campaigns Best Places to Work 2023 for the UK

2023 Best Workplaces in Canada by Great Place to Work
1 DSP on G2 and leader in a number of categories including Cross-Channel Advertising.
G2 Top Software and Top Marketing and Advertising Product for 2023

LI-Remote

"
https://startup.jobs/data-engineer-launchpad-technologies-3915117,Engineer,Data Engineer,Launchpad Technologies ,,,"


Recognized as one of Canadas fastest-growing companies, Launchpad provides next-generation integration platform capabilities for connecting and managing enterprise automation and data integration. Headquartered in Vancouver, Canada, our operations span both North and South Americas, with a second headquarter located in Santiago, Chile.
In 2021, we were recognized as the winner of the BC Tech Associations Technology Impact Awards Company of The Year Growth. We recognize the global truth that today, every company is a tech company. To succeed, every company everywhere needs their data and workflows to be integrated and automated. That understanding gave us the confidence to compete with our sectors leading providers, win top-level contracts, and build a truly world-class list of clients that includes Walmart, GM, TIME Magazine, Salesforce, Tableau, Splunk, Bolt.com, Freedom House, and more.
Our vision is to bring people and technology together to work seamlessly to achieve extraordinary business outcomes and create more efficient societies. If this sounds like something you want to be part of, wed love to hear from you.

Main Responsibilities

Migration of existing Salesforce.com data pipelines from Matillion to our new Python based data platform
Migration of the existing Gainsight data pipeline from our current data platform to our new data platform
Migration of additional data pipelines as needed

Required Experience

3 years of experience in Python
3 years of experience in Linux
2 years of experience working with Data ETL  ELT
2 years of experience interfacing with third party APIs
1 year of experience working with Analytical Databases Redshift, Snowflake
2 years of experience working with Cloud technologies
1 year of experience working with GIT

Bonus Experience

Matillion
AWS
Docker
DBT
Terraform
Airflow
Hashicorp Vault
Data Modeling

Soft skills we consider important

Enthusiastic about tech and the data domain
Strong problem solving skills
Collaborative
Takes initiative without being reckless considers how their actions impacts other peoplesystems

Why work for Launchpad?

100 remote
People first culture
Excellent compensation in US Dollars
Hardware setup for working from home
Work with global teams and prominent brands based in North America, Europe, and Asia
Training allowances
Personal time off PTO for vacations, study leave, personal time, etc.
...and more!


At Launchpad, we genuinely care about our people as individuals. If you are looking for a team that values growth, drive, and passion for your craft, if youre seeking a place to achieve your goals and dreams with fairness and integrity, then you are the future of Launchpad. Launchpad is committed to fostering a diverse and representative workforce and an inclusive work environment where all employees are respected and treated equally.
Are you ready to elevate your career at Launchpad? We want to hear your story! Contact us today.
 


"
https://startup.jobs/data-engineer-yassir-3905118,Engineer,Data Engineer,Yassir ,"Algiers [El Djazaïr], Algeria",Full-Time,"

About Yassir

Yassir is the leading super App for on demand, ride-hailing, last-mile delivery, payment services and more, set to change the way daily services are provided. It currently operates in 45 cities across multiple countries. It has raised 150 million in Series B funding, five times what it raised in its previous priced round last November with world class investors such as BOND and Y Combinator, which is the precursor of the likes of Airbnb, Stripe, Dropbox, Doordash, among others. 

Were not just about serving people - were about creating a marketplace to bring people what they need while infusing social values.


ABOUT THE ROLE

We are looking for a Data Engineer at Yassir, with a thorough understanding of cloud technologies. You will help deploy Big Data solutions, build and manage data and model pipelines, ETL processes, and deploy machine learning models in a production, secure and scalable manner. The senior data engineer should be a data specialist with strong experience in the deployment and automation of data warehousing, data and model pipelines, data cleansing, monitoring data processing systems, and dealing with large and various sets of structured and unstructured data. You possess excellent communication skills along with the perseverance to solve problems that may not have obvious solutions. You may need to ""Think out of the box"" or ""hack around"" a problem to solve it. We like that!




TASKS

Design and build data and model pipelines in Google Cloud Platform, AWS or similar.
Manage and automate ETL, ML model pipelines and cloud deployment implementations.
Design and implement Data Warehousing solutions with their corresponding Data Governance processes.
Design, implement, test CICD pipelines for the various data products to be productised.
Design for data security and compliance.
Manage, provision and orchestrate necessary cloud solution infrastructure.
Be in charge of the MLOps lifecycle research, design, experimentation, development, deployment, monitoring, and maintenance.
Liaise with stakeholders to analyze business problems, clarify requirements and define the scope of the resolution needed.

REQUIREMENTS 

5 years experience using SQL and BigQuery or a SQL-based tool.
Demonstrable deep knowledge and experience in cloud migration, cloud architecture, and data engineering.
Production-level Python, SQL  NoSQL, CloudOps GCP, AWS or Azure ecosystems, Git, CICD Github Actions or CircleCI, MLflow.
2 years of hands-on experience with cloud orchestration tooling, infrastructure-as-a-code Terraform, CloudFormation or similar.
2 years of hands-on experience building scalable ETL pipelines Apache Beam, Airflow, Cloud Composer, Cloud Functions, PubSub.
Broad knowledge of one of the major cloud vendors.
Strong problem-solving or analytics experience.
Willingness to both teach others and learn new techniques.
Advanced English communication skills.

About your experience 

A set of certifications in GCP Cloud Architect, Data Engineer, Cloud Engineer, ML Engineer or similar in another cloud vendor.
Hands-on experience in MLOps, ML model deployment, governance, and workflow optimization.
Hands-on experience in one or various LightGBMXGBoostSklearn, NumpySciPy, KerasPyTorchTabnet, RustC, TensorFlowCaffeMXNet, CUDARay, Node.js, RESTGraphQL, Neo4J, GrafanaDatadog, KafkaSparkPresto.
Knowledge ofexposure to Google Analytics.
Knowledge ofexposure to Adobe Marketing Cloud or Adobe Analytics.

BENEFITS

Great compensation and bonuses including stock options. 
Ground floor opportunity with the team; shape the strategic direction of the company.
Sharp, motivated co-workers in a fun office environment.
Full social coverage.

PERKS

Ground floor opportunity with the team; shape the strategic direction of the company.
The rare opportunity to change the world around you such that everyone around you is using the product you built. Were not just another app, were infusing social values and reinventing how services are provided.
Sharp, motivated co-workers in a fun office environment.




As a company, we are passionate about diversity and inclusion, 40 of our team are women leaders in the tech sector. Research shows that women do not apply for jobs if they do not meet all of the requirements. We would like to hear from you if you feel you would be a good fit for us!

Do you want to become part of our first-class team? Then you absolutely have to send us your application. 

PS And if you want to stand out in your application, just let us know in your cover letter why we should have in our team. 

Diversity  Inclusion  Engagement 
We celebrate diversity and are committed to creating an inclusive environment for all employees, as we believe diverse teams are more successful in the long term. We do not discriminate on the basis of race, color, religion, marital status, age, national origin, ancestry, physical or mental disability, medical condition, pregnancy, genetic information, gender, sexual orientation, gender identity or expression, and we encourage all people equally to apply for jobs with us.

"
https://startup.jobs/data-engineer-alphagrep-securities-3897938,Engineer,Data Engineer,AlphaGrep Securities ,,,"

About the Company
AlphaGrep is a quantitative trading and investment firm founded in 2010. We are one of the largest firms by trading volume on Indian exchanges and have significant market share on several large global exchanges as well. We use a disciplined and systematic quantitative approach to identify factors that consistently generate alpha. These factors are then coupled with our proprietary ultra-low latency trading systems and robust risk management to develop trading strategies across asset classes equities, commodities, currencies, fixed income that trade on global exchanges..We are seeking bright and resourceful individuals for our Data team which is based out of our Mumbai office.Roles  Responsibilities

Build infrastructure tools and applications to support trading teams across the firm.
Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Assemble large, complex data sets that meet functional  non-functional business requirements.
Coordinate with global teams to understand their requirements and work alongside them.
Establishing programming patterns, documenting components and provide infrastructure for analysis and execution
Set up practices on data reporting and continuous monitoring
Write a highly efficient and optimized code that is easily scalable.
Adherence to coding and quality standards.

Required Skills

Strong working knowledge in Python.
Strong working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases.
Experience performing root cause analysis on internal and external processes to answer specific business questions and identify opportunities for improvement.

Good to have

Experience with web crawling and scraping, text parsing
Experience working in Linux Environment
Experience with Stock Market Data

Why You Should Join Us 


Great People. Were curious engineers, mathematicians, statisticians and like to have fun while achieving our goals

Transparent Structure. Our employees know that we value their ideas and contributions

Relaxed Environment. We have a flat organizational structure with frequent activities for all employees such as yearly offsites, happy hours, corporate sports teams, etc.

Health  Wellness Programs. We believe that a balanced employee is more productive. A stocked kitchen, gym membership and generous vacation package are just some of the perks that we offer our employees


"
https://startup.jobs/data-engineer-lever-implementation-training--3892075,Engineer,Data Engineer,Lever Implementation Training Environment ,"San Diego, United States",Full-Time,"

This is an example

PLEASE READ these jobs are testing jobs of Levers testing environment - please do not apply for this job.


Lever was founded ten years ago to tackle the most strategic challenge that companies face how to recruit and hire top talent. Were building the next generation of hiring software that companies like Netflix, Yelp, Cirque du Soleil, Shopify, and Spotify rely on to grow their teams. Weve rethought the talent acquisition paradigm and are the innovation leaders in our space and looking for the right people to join us as we scale. Were extraordinarily proud of the company weve built so far not to mention humbled to be recognized as the 1 place to work in San Francisco, as well as a top workplace in the entire United States. Our people are Levers biggest competitive advantage and well continue investing in our Leveroos and people-first culture. 


start to build out my first paragraph here about the role


Quialificaitons

1
2
3

Responsibilties

1
2
3

Requirements

1
2
3


additional pargraph about the role or anything that you want to say

THE LEVER STORY
Lever builds modern recruiting software for teams to source, interview, and hire top talent. Our team strives to set a new bar for enterprise software with modern, well-designed, real-time apps. We participated in Y Combinator in summer 2012, and since then have reached our Series D, raising more than 120 million. As the applicant tracking system of choice for Netflix, Atlassian, KPMG, and McGraw-Hill Education, and thousands more leading companies, Lever means you hire the best by hiring together.
 
Lever is an equal opportunity employer. We are committed to providing reasonable accommodations and will work with you to meet your needs. If you are a person with a disability and require assistance during the application process, please dont hesitate to reach out! We celebrate our inclusive work environment and welcome members of all backgrounds and perspectives. Learn more about our team culture and commitment to diversity and inclusion. 
 
California residents applying for positions at Lever can see our privacy policy here. 

"
https://startup.jobs/data-engineer-etl-aisera-3890511,Engineer,Data Engineer (ETL),Aisera ,"Bengaluru, India",,"

What We Do AI Service Management AISM
Aisera offers the worlds first AI-driven service experience solution that automates operations and support for IT, Sales and customer service, making businesses and customers successful by offering consumer-like self-service resolutions to users. Aisera fast tracks the digital transformation journey with user and service behavioral intelligence that drives end-to-end automation of tasks, actions, and business processes. Aisera is a top-tier, VC-funded startup headquartered in Palo Alto, Calif. and a strategic partner with AWS, Microsoft Azure, Google Cloud, ServiceNow and Salesforce.
Aisera has received numerous recognitions, including the following Forbes AI50; CNBC Upstart 100 Top Startup; Gartner Cool Vendor; Red Herring Top 100 Global Innovator; CIO Innovation Startup Award; CIO Review Top ITSM Solution; Aragon Research Hot Vendor; TiE50 Startup Award; and Silicon Review 50 Most Admired Companies.
 
The Team
We are a small but passionate team of 100 based in the Bay Area. Aiseras seasoned founding team has led companies through several prior successful startups and acquisitions. We give our employees a lot of responsibility and ownership of their work, and we hire people from a very wide range of backgrounds and experience. Our team members operate with a high degree of empathy for our customers and each other.
What Youll Do

As a Data Engineer, you will be part of the core platform engineering team and work with other team members, engineering management and product management to plan, develop and deliver on time various features and functionality for the platform.
Work with teammates, leadership and product management to design and deliver the data platform and connector features as per roadmap.
Deliver testable, modular, highly scalable and reusable code, test cases and documentation on time.
Working in an agile development environment.

What Youll Need

Bachelors Degree in Computer Science or Computer Engineering or Electrical Engineering
At least 3 years professional experience in similar positions in software development
Strong Java skill
Strong database skills
Good experience in microservices
Good experience in Elasticsearch 
Good experience in using one or more public cloud environments AWS, Azure, Google Cloud, etc.
Be very keen on quality in any aspect
Be responsible over projects quality we deliver
Very good verbal and written communication skills English


"
https://startup.jobs/data-engineer-1-klub-3876192,Engineer,Data Engineer 1,Klub ,"Bengaluru, India",Full-Time,"


Klub was founded to make capital accessible for founders we know how taxing raising funds can get. As Indias largest Revenue Based Financing platform, we offer non-dilutive capital up to 30 crores for consumer brands across 35 sectors FB, Fashion, Health  Wellness, Electronics, Cloud Kitchens  more We have facilitated over 1000 investments in leading brands of India including BluSmart, Third Wave Coffee Roasters, SMOOR, Tagz Foods, Hero Electric, Bewakoof  more! 

Heres what makes us the most scalable growth capital partner for new-age businesses


Founder-friendly No pitch decks, no personal guarantees, no equity dilution, no board seats. Speed Our turnaround time is very fast, which helps us in meeting any urgent capital requirements. Companies can apply online on our app in less than 10 mins and get a term sheet within 48 hours! 

Product-led By relying exclusively on data and metrics, we take bias out of decision-making. Fun fact30 of our portfolio has women foundersCXOsFlexible Companies pay as a percentage of revenues. They can choose to raise as little as  5 lakhs to as much as 30 crs, based on their metrics and capital requirements. 

Our journey

 We raised 2 mn at an idea stage in December 2019 from Sequoia Capitals Surge program, and marquee investors like Naveen Tewari Founder, InMobi Group, and Kunal Shah Founder, CRED. 
 Raised 20 mn in one of the largest seed rounds by an Indian Fintech company. This includes marquee investors like GMO Japan, Alter Global USA, and others.
 Became one of the first RBF players to launch a 30 mn fundFacilitated 1000 investment rounds in over 400 leading consumer brands from bootstrapped companies to unicorns!

About the role

You will be building data pipelines that transform raw, unstructured data into formats data scientists can use for analysis. You will be responsible for creating and maintaining the analytics infrastructure that enables almost every other data function. This includes architectures such as databases, servers, and large-scale processing systems.


Responsibilities

Responsible for setting up a  scalable Data warehouse
Building data pipeline mechanisms  to integrate the data from various sources for all of Klubs data.  
Setup data as a service to expose the needed data as part of APIs. 
Have a good understanding on how the finance data works.
Standardize and optimize design thinking across the technology team.
Collaborate with stakeholders across engineering teams to come up with short and long-term architecture decisions.
Build robust data models that will help to support various reporting requirements for the business , ops and the leadership team. 
Participate in peer reviews , provide codedesign comments.
Own the problem and deliver to success.

Requirements- 

Overall 1 years of industry experience
Prior experience on Backend and Data Engineering systems
Deep understanding on python tech stack with the libraries like Flask, scipy, numpy, pytest frameworks.
Good understanding of Apache Airflow or similar orchestration tools. 
Good knowledge on data warehouse technologies like Apache Hive or similar. Good knowledge on Apache PySpark or similar. 
Good knowledge on how to build analytics services on the data for different reporting and BI needs. 
Good knowledge on data pipelineETL tools Hevo data or similar. Good knowledge on Trino  graphQL or similar query engine technologies. 
Deep understanding of concepts on Dimensional Data Models. Familiarity with RDBMS MySQL PostgreSQL , NoSQL MongoDBDynamoDB databases  cachingredis or similar.
Should be proficient in writing SQL queries. 
Good knowledge on kafka. Be able to write clean, maintainable code.

Nice to have
Built a Data Warehouse from the scratch and set up a scalable data infrastructure.
Prior experience in fintech would be a plus.
Prior experience on data modelling.




Our team
We are a 100 member team based out of Bangalore Koramangala. Our founders are Anurakt Jain Corporate Intrapreneur at InMobi, VC at DFJ and Vertex; IIT-D, Wharton, CFA  Ishita Verma i-bankinginvesting veteran at Kotak, Unitus Capital, Snyder UAE; LSR, IIM-B.Find more about our leadership team here. 

Here are some of the reasons why our team enjoys working at Klub

 Solving challenging problem statements
 High-growth stage startup with fast execution
 Diverse cultureStructured thinking  approach
 Supportive team
 Know more about life at Klub from our team.

What you should expect at Klub?


 Career acceleration Our best performers had 3 promotionscharter expansions in 2yrs

 Upskilling Dedicated learning and development department with budgets and weekly learning hours

 International exposure Klubs going global and you will have a front-row seat. We have already piloted in the middle east.

 Salary Market-benchmarked compensation

 ESOPs Get skin in the game, and grow with the company!

 Generous paid time off For days you wake up with sniffles and days when you need to unwind

 Healthcare benefits Insurance coverage for self, spouse, and kids

 Fun Our team likes football matches, board games, frequent team lunches, and celebrating milestones!

 Diversity We are an equal opportunity employer We do not discriminate on the basis of race, creed, color, religion, gender, sexual orientation, gender identityexpression, age, or similar conditions. 40 of our team is women and were committed to increasing this number.

What are we looking for in a candidate? 

Read the blog that defines our culture. TLDR; Fire in the belly, intellectual curiosity, a drive to deliver, exceptional accountability, and humility.

About the application process
Apply through our careers page

3 rounds of video or in-person interviews to assess role fit, general aptitude, and cultural fitment
Craftsmanship assignments or case studies for select roles

"
https://startup.jobs/data-engineer-ozow-3846216,Engineer,Data Engineer,Ozow ,"Cape Town, South Africa",,"

What to expect 
If you are looking for an opportunity to work with data in a different and connected way, you might be the Data Engineer we are looking for! If you are an innovative Data Engineer, that likes connecting the dots in Data together and building high-performance Data Platforms that service multiple Data and Business Services, joining our team might just be the challenge you are looking for.   
You will be responsible for expanding and optimising our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.   
They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our companys data architecture to support our next generation of products and data insights and solutions.  
 
Requirements 


3 years of experience in developing data solutions including analysis, design, coding, testing, deploying, and supporting applications.  


A degree in Computer Science, Applied Mathematics, Physics, Statistics or area of study related to data sciences, data engineering and data mining or relevant experience.  


Are proficient in data applicationsoftware architecture Definition, Business Process Modelling, etc.. 


Experience working SQL  Python knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases.  


Experience building and optimising big data data pipelines, architectures, and data sets.  


Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.  


Previous experience in operations of a big data ecosystem   


Experience with operational aspects of platforms such as monitoring and alerts management, availability, capacity management and service management  


The ability to build processes supporting data transformation, data structures, metadata, dependency, and workload management.  


A successful history of manipulating, processing, and extracting value from large, disconnected datasets.  


Working knowledge of message queuing, stream processing, and highly scalable big data data stores.  


Strong project management and organizational skills.  


Experience supporting and working with cross-functional teams and mentoringleading teams  


Experience working with Fintech data advantageous. 


 

In office perks 


Free healthy breakfast, lunches and snacks Cape Town 

Free healthy lunches and snacks Johannesburg 
Free laundry service Cape Town and still to launch in Johannesburg 
Monthly socials Cape Town and Johannesburg 
On-site Barista Cape Town 
Workplace Concierge Cape Town
Ozow Library Cape Town  
Ozow Playroom Cape Town 

 
Perks for South African based employees


Medical aid subsidy permanent staff


Group Risk Insurance permanent staff


Paid annual leave  


Employee Health and Wellness Assistance 


Gym  Fitness sessions 


Ozow Cookbook 


Learning and Development opportunities 


Mentorship programme 


Community initiatives 

Quarterly team retreats and annual company retreats

Employee gifting 


Access to cutting edge technology 

Hybrid

 
Sounds good? Apply now! 
 
Keen to know more? 
Interested in joining our rocket ship?  
To find out more about life at Ozow, head over to our Careers Page here!


"
https://startup.jobs/data-engineer-8435-iongroup-3832590,Engineer,Data Engineer - 8435,ION Group ,"Noida, India",Full-Time,"

The Role

The Automation team is looking for is looking for a data engineer to help expand our internal data analytics and machine learning infrastructure at ION group. You would be responsible for developing and supporting integration with systems to ingest new datasets, ETLELT pipelines, data models for BI dashboards and machine learning automation. The ideal candidate is a self-starter who can pick up new skills easily we will train, with great attention to detail and a love of data and automation.
Key Responsibilities

Our objective is to promote a strong data culture within ION Group and to provide staff the resources and tools they need to use data to its full effect. 
We provide new standardized and discoverable datasets, Business Intelligence BI tools for self-serve dashboards to uncover deeper actionable insights, as well as a state-of-the-art toolkit for machine learning automation. 
We have been delighted to see company culture change since the team started this journey, and we are excited to continue this growth with your help

Required Skills, Qualifications and Experience

Python
SQL
Data modelling and transformation
 API development. 
Any of the following would be a bonus Azure, Databricks, Power BI, DAX, machine learning classification, NLP.
We are looking for someone excited about engineering, data science and BI, able to work with multiple teams and with great attention to detail in delivering new products and services.


About us

Were a diverse group of visionary innovators who provide trading and workflow automation software, high-value analytics, and strategic consulting to corporations, central banks, financial institutions, and governments. Founded in 1999, weve achieved tremendous growth by bringing together some of the best and most successful financial technology companies in the world. 
Over 2,000 of the worlds leading corporations, including 50 of the Fortune 500 and 30 of the worlds central banks, trust ION solutions to manage their cash, in-house banking, commodity supply chain, trading and risk.
Over 800 of the worlds leading banks and broker-dealers use our electronic trading platforms to operate the worlds financial market infrastructure.
With 10,000 employees and offices in more than 40 cities around the globe, ION is a rapidly expanding and dynamic group.
At ION, we offer careers that provide many opportunities To invent. To design. To collaborate. To build. To transform businesses and empower people around the world to do more, faster and better than before. Imagine what you can do and experience. This is where you can do your best work.



ION is committed to maintaining a supportive and inclusive environment for people with diverse backgrounds and experiences. We respect the varied identities, abilities, cultures, and traditions of the individuals who comprise our organization and recognize the value that different backgrounds and points of view bring to our business.
ION adheres to an equal employment opportunity policy that prohibits discriminatory practices or harassment against applicants or employees based on any legally impermissible factor.
 

"
https://startup.jobs/data-engineer-mexico-city-rise-interactive-3778751,Engineer,Data Engineer (Mexico City),Rise Interactive ,"Mexico City, Mexico",,"

Rise Interactive httpwww.riseinteractive.com, a rapidly growing full service interactive advertising agency, is looking to hire a Software Engineer, focused on Data Management, into the Innovation team.  We are seeking a detail-oriented, highly motivated individual contributor that is excited about joining a new Innovation department and contributing professionally to an entrepreneurial company with an expanding Fortune 500 client base.
 
JOB SUMMARY
Were looking for a Data Engineer to help us build out our marketing and analytics platforms.  Our platform processes billions of records today  and were just getting started.  This role is not just a research or analytical role, it is an individual contributor role that is expected to contribute to production solutions for the entire data pipeline. Our goal is to add a data engineer who is strong at building systems from scratch by communicating closely with business stakeholders and passionate about delivering the best possible product and customer experience.  
 
FUNCTIONS AND RESPONSIBILITIES
We operate a cross-functional team that specializes when needed but aims to have everyone able to contribute. Duties of the Data Engineer include

Implement and optimize data processing pipelines for megabytes to terabytes of data
Onboard and integrate client data into our analytics platform
Design and build our data warehouse as well as real-time data reporting systems


Promote and nurture good team practices such as unit testing, code reviews, buildtest automation, etc.
Proactively mentor and guide developers to improve their quality and simplicity in design and code
Design, build and use tools to understand our product platform behavior and performance
Design and conduct experiments to test concepts, technologies, and algorithms
Implement analytics tools to maximize the value of collected data
Implement data tests in data quality frameworks to ensure data is clean and accurate
Adhere to security policies and guidelines to ensure our data is protected and safe
Embrace and assist in evolving our Agile Scrum team processes and developer role responsibilities

 
EDUCATION, TRAINING, AND EXPERIENCE
Education
Bachelors degree in computer science or equivalent field
Training
NA
Experience
1-3 years of on-the-job experience

PREFERRED SKILLS
QUALIFICATIONS

Smart, high aptitude to learn new things and sense of urgency to get things done
Extremely strong ETL programming skills using tools like Python, Spark, PySpark, Hadoop, MapReduce, Kafka
Experience building data access layers via cubes, data marts, APIs, or visualization tools like Tableau or D3 
Practical experience with Big data and NoSQL technologies desired
Comfortable working with several large, complex SQL databases and SQL queries
Experience working in cloud-only infrastructure, especially Amazon Web Services
1 years of experience working with large amounts of real data
A strong passion for empirical research and answering hard questions
Team player  demonstrated experience on a few teams that have shipped a product
Professional developer  experienced with source control Git and bug tracking
Practical-minded  chooses stabilityreliabilitymaintainability over shiny new objects
Passionate about technology  ideally you build things outside of work for fun 
Scripting skills  must be totally comfortable at the Linux command line
Nice to have Expertise in applied statistics or machine learning
Experience delivering data products in the marketing, ad tech space is also a plus
Excellent English verbal and written communication skills


If you were here this month, you would have

Modeled web impression data into usable information
Created scripts to manage our cloud environment
Sped up an ETL process to make our users happy
Tried to get the top pinball score


"
https://startup.jobs/data-engineer-ii-rackspace-3773013,Engineer,Data Engineer II,Rackspace ,,Full-Time,"

DESCRIPTION 

         Collaborate with customer to gather requirements and to understand their business processes.


         Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional  non-functional business requirements.


         Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.


         Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Spark, SQL and Azure big data technologies.


         Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.


         Use tools to deploy and monitor the performance of the systems in production.


         Demonstrate knowledge and real-world experience on Big data technologies. A successful history of manipulating, processing and extracting value from large datasets.


         Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases.


         Strong analytic skills related to working with unstructured datasets.


         Experience building and optimizing big data data pipelines, architectures and data sets.


REQUIREMENT

         We are looking for a candidate with 2 years of experience in a Data Engineer role. They should also have experience using the following softwaretools


         Experience with big data tools Hadoop, Spark, Kafka, etc.


         Experience with relational SQL databases such as Oracle, SQL Server, MySQL or Postgres.


         Experience with data pipeline and workflow management tools Azkaban, Luigi, Airflow, etc.


         Experience with stream-processing systems Storm, Spark-Streaming, etc.


         Experience with object-orientedobject function scripting languages Python, Java, C, Scala, etc.


         For senior data engineers also should have demonstrable experience in Project management and organizational skills.


         Experience supporting and working with cross-functional teams in a dynamic environment.


         Good Academic track record preferred.


         Some experience on visualization tools such as PowerBITableau or other BI tools will be beneficial.


QUALIFICATIONS

         Degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field


         3-5 years of relevant experience.




About Rackspace Technology
We are the multicloud solutions experts. We combine our expertise with the worlds leading technologies  across applications, data and security  to deliver end-to-end solutions. We have a proven record of advising customers based on their business challenges, designing solutions that scale, building and managing those solutions, and optimizing returns into the future. Named a best place to work, year after year according to Fortune, Forbes and Glassdoor, we attract and develop world-class talent. Join us on our mission to embrace technology, empower customers and deliver the future.
 
 
More on Rackspace Technology
Though were all different, Rackers thrive through our connection to a central goal to be a valued member of a winning team on an inspiring mission. We bring our whole selves to work every day. And we embrace the notion that unique perspectives fuel innovation and enable us to best serve our customers and communities around the globe. We welcome you to apply today and want you to know that we are committed to offering equal employment opportunity without regard to age, color, disability, gender reassignment or identity or expression, genetic information, marital or civil partner status, pregnancy or maternity status, military or veteran status, nationality, ethnic or national origin, race, religion or belief, sexual orientation, or any legally protected characteristic. If you have a disability or special need that requires accommodation, please let us know.

"
https://startup.jobs/data-engineer-embrace-3750396,Engineer,Data Engineer,Embrace ,,,"


Our world is mobile  the phones in our pockets, our 10 IoT devices at home, the cars we drive, the way we conduct our work, and the point-of-sale devices from which we buy our coffee and donuts. Embrace is on a mission to make all of our edge experiences exceptional by helping revolutionary companies evolve and excel in this mobile-world that did not exist even 7 years ago. Customers, like Goat, Hilton, Masterclass, Home Depot, and Cameo, love Embraces mobile observability and data platform because it makes extremely complicated and voluminous data actionable.
About the Role
Were looking for a confident data engineer who will help build the data infrastructure vision by creating systems and datasets that the entire company will use. You will design, build, scale, and evolve our data engineering platforms, services and tooling. We are seeking someone who wants to back impact and lay a solid foundation as we are quickly expanding.
What Youll Do

Select, evaluate, and implement the newest and evolving technologies to collect high volume, infinitely cardinal data sets
Own one of the most modern data infrastructures to create highly-efficient data pipelines that connect to the entire data infrastructure from Snowflake and Big Query to streaming technologies
Design and implement data management services for data trust, data compliance, data access and scalable and configurable metadata management.
Optimize data pipelines to scale to processing of 100B messages per day in real time
Develop methods to ensure the integrity of very complex and evolving data sets

Basic Qualifications

2 years experience with Python or Golang OR excited to learn Python or Golang plus experience with Java, Rust, CC
Demonstrate a sense of logic, decision making and problem resolution skills
Experience with processing data in large volumes and managing with data stores

Gathering is integral to our culture for collaboration and connection. We want to ensure that all of our new hires have the ability to travel and meet other team members. We estimate the travel for any given role to be a couple of days every month or two.
Preferred Qualifications

Experience working with time-series data 
Experience profiling and optimizing systems in production
Experience creating data pipelines with Kafka, Cassandra, and Clickhouse
An opinion on Star Wars vs Star Trek

Culture Values


Perspective - seeking to understand others perspectives

Investing - investing in discovering value, unprompted

Honesty - delivering brutal honesty kindly

Simplest - finding the simplest solutions by focusing on outcomes

Ownership - empowering yourself and others through solutions, not answers

Dark Humor - finding levity together, even when tackling hard problems

Why join Embrace?
Embrace is growing and recently raised a Series B backed by YCombinator, NEA, Greycroft, and the founders of Testflight, Parse, and PagerDuty. The repeat founding team also started Scopely, a 3B unicorn mobile startup. Enterprises, revolutionaries, and vendors rely on the Embrace Data Platform to capture 100 of user-behavioral and technical time-based session data rather than relying on sampling. The engineering, data science, UX, and product teams at NYTimes, Hyatt, OkCupid, and Owlet are driving their businesses with Embrace.
For this role, for a mid-level engineer, the base is 115K - 165K. For someone earlier in their career, our range is a base of 80K - 120K. Note that these are guideline base salary ranges and if you have questions, we would love to discuss! In addition to base pay, we offer equity in the form of options, a variety of benefits, and the opportunity to grow in an exciting and collaborative environment.

 

"
https://startup.jobs/data-engineer-merchant-intelligence-remote-constructor-3749057,Engineer,Data Engineer : Merchant Intelligence (Remote),Constructor ,"San Francisco, United States",Full-Time,"

About us
Constructor.io powers product search and discovery for some of the largest retailers in the world. We serve billions of requests every week, and youve probably seen our results somewhere and used our product without knowing it. We differentiate ourselves by focusing on metrics over features, and reinventing search and discovery from the ground up as a machine learning challenge with the specific goal of improving metrics like revenue. Were approximately doubling year over year despite the market slow down and have customers in every eCommerce vertical. Were a passionate team of technologists who love solving problems and want to make our customers and coworkers lives better. We value empathy, openness, curiosity, continuous improvement, and are excited by metrics that matter. We believe that empowering everyone in a company to do what they think is best can lead to great things.

Merchant Intelligence Team
An important part of our product is the Customer dashboard that helps merchandizers to analyze and impact user behavior. We provide a number of tools for them to influence which products and product attributes should and will receive more attention to create value for their business. The goal of the Merchant Intelligence team is to
 Help merchandizers achieve their e-commerce goals, increasing satisfaction with their sites and retention of customers. Provide insights to merchants they cant get anywhere else. Become a critical point of merchant team planning, decision making, and evaluation so we become a sticky part of their organization. 
Challenges you will tackle
 Deliver new reports and tools for merchandizers and analysts from e-commerce companies. Improve the existing dashboard experience by building analytics that provide insights to improve KPIs. Perform data exploration and research user behavior. Implement end-to-end data pipelines to support realtime analytics for important business metrics. Take part in product research and development, iterate with prototypes and customer product interviews. 
Requirements
 You are proficient in BI tools data analysis, building dashboards for engineers and non-technical folks. You are an excellent communicator with the ability to translate business asks into a technical language and vice versa. You are excited to leverage massive amounts of data to drive product innovation  deliver business value. Youre familiar with math statistics AB-tests You are proficient at SQL any variant, well-versed in exploratory data analysis with Python pandas  numpy, data visualization libraries. Big plus is practical familiarity with the big data stack Spark, PrestoAthena, Hive. You are adept at fast prototyping and providing analytical support for initiatives in the e-commerce space by identifying  focusing on relevant features  metrics. You are willing to develop and maintain effective communication tools to report business performance and inform decision-making at a cross-functional level. Stack python, numpy, pandas, SQL, pyspark, flask, docker, git 
Benefits
 Unlimited vacation time -we strongly encourage all of our employees take at least 3 weeks per year A competitive compensation package including stock options Company sponsored US health coverage 100 paid for employee Fully remote team - choose where you live Work from home stipend! We want you to have the resources you need to set up your home office Apple laptops provided for new employees Training and development budget for every employee, refreshed each year Parental leave for qualified employees Work with smart people who will help you grow and make a meaningful impact 

Diversity, Equity, and Inclusion at Constructor
At Constructor.io we are committed to cultivating a work environment that is diverse, equitable, and inclusive. As an equal opportunity employer, we welcome individuals of all backgrounds and provide equal opportunities to all applicants regardless of their education, diversity of opinion, race, color, religion, gender, gender expression, sexual orientation, national origin, genetics, disability, age, veteran status or affiliation in any other protected group.

"
https://startup.jobs/data-engineer-simply-business-3744849,Engineer,Data Engineer,Simply Business ,"London, United Kingdom",Full-Time,"

Heres what youll be doing

You will be working as part of an international team, driving our class-leading data platform forwards as well as developing greenfield data products to sell to the business. Youll get access to
 a fantastic training and conferences budget 
 Friday afternoon learning time
 a team who will support you to succeed

Reporting into James our Data Engineering Manager, whos hands on and interested in pretty much everything technology, youll be joining an amazing team of five data engineers. 
As one of our Data Engineers, youll

work as part of a larger centre of excellence of data specialists. Youll help to look after our data, business intelligence and machine learning platforms
be able to develop your own scripts and triage any potential bugs 
contribute ideas and help to prioritise technical work
operate in an Agile environment utilising scrum methodology
be part of an open, friendly, and distributed team. We love being proactive and simplifying very technical concepts to the business

Were looking for someone who is

able to demonstrate experience coding in JVM languages Java, Kotlin, or Scala, as well as Ruby or Python. You dont have to be great at coding in them all
hungry to learn and pick up new skills
a savvy communicator, with the ability to take technical concepts and simplify them to a non-technical audience
inclusive, great to work with, and loves helping the team succeed


We know its tough, but please try to avoid the confidence gap. You dont have to match all the bullet points above to be considered for this role.

Ready to join us and level up your data engineering skills? Apply today.


About Simply Business
We insure small businesses and enable big dreams  not just for our customers, but for our people and communities too. With over 800,000 active insurance policies, we protect builders, bakers, landlords, and more than 1,000 other trades.

Were as much a technology company as one that sells insurance. We build, we fail, we learn, we improve. Were a B Corp too, which recognises our strong track record of having a positive impact on people, society and the environment. 

Simply Business TV advert 2022 You name it. We insure it.

What are the benefits?
We support every team member to balance work and life effectively.
  remote working - you can balance working remotely and connecting with colleagues in the office
  mental health and wellbeing resources - access to counselling and technology to support your mental health
  flexible parental leave - we pay six months full pay to the primary caregiver, and four weeks full pay if youre the secondary caregiver
   paid sabbatical - two weeks off when youve been here five years and four weeks off when youve been here 10, 15 and 20 years
  a dedicated learning platform - the platform lets you balance both work and life goals including management and leadership programme
  life event leave - an extra day of leave every two years for whatever you want moving house, welcoming a new pet, or your birthday

We also make sure youre compensated fairly.
  competitive salary - based upon your experience and the market were in from day one
  annual bonus - the potential to earn a bonus based on business performance
   pension - we match what you put into your pension up to five percent
  health cash plan - we reimburse your everyday medical expenses
   holiday entitlement - 25 days leave, plus bank holidays you can also use your flexible benefits to get up to five days more!
   life assurance - four times your basic salary
   flexible benefits scheme - an allocated allowance to use each year on things like private medical insurance, dental insurance, travel insurance, up to five days extra holiday, and gym membership.

Ready to join us and drive our success as a high-performing team? Apply today.

"
https://startup.jobs/data-engineer-online-job-oowlish-technology-3732549,Engineer,Data Engineer - Online Job,Oowlish Technology ,,,"

Be part of our team!


Oowlish is one of the fastest software development companies in LatAm. We work with large corporations and startups both in the USA and Europe, helping these companies accelerate innovation by having access to the best talent in the industry.

We take the best elements of virtual teams and combine them with a support structure that encourages innovation, social interaction, and fun. We see no borders, move at a fast pace, and are never afraid to break the mold.

We are considered A Great Place to Work thanks to our highly talented and engaged developers that are eager to grow by learning from each other, which creates a culture of collaboration and inspiration.



We are looking for an experienced Data Engineer to join our team and work on international projects. We are looking for people that want to grow, are challenge-driven, follow good practices, and are passionate about technology.
Must Haves

Fluent in English
Bachelors degree in Computer Science.
4 years of experience as a Data Engineer
Experience using MongoDB and GraphDB.
Experience in building data lakes
High-level experience in methodologies and processes for managing large-scale databases.
Ability to work with stakeholders to assess potential risks.
Ability to analyze existing tools and databases and provide software solution recommendations.
Ability to translate business requirements into non-technical, lay terms.
Understanding of addressing and metadata standards.


Benefits  Perks

- Home office 
- Flexible Hours;
- Competitive compensation based on experience; 
- Career plans to allow for extensive growth in the company;
- International Projects;
- Oowlish English Program Improving and Certifying
- Spanish Classes 
- Oowlish fitness with Total Pass;
- Oowlish Academy Grants for Education;
- Build your Home Office Grants for Computer  Equipment;
- Connecting You Internet allowance;
- Anniversary bonus;
- Wedding gift;
- Pet adoption incentive;
- New baby Owl bonus;
- Back to School bonus;
- Netflix Subscription;
- Oowlish paradise  4 daysyear at a Resort with your family  pets;
- Vacation Bonus;
- Games and Competitions;
- Enjoy your national Holidays;
- Oowlet Store

You can also apply here

Website httpswww.oowlish.comwork-with-us

LinkedIn httpswww.linkedin.comcompanyoowlishjobs

Instagram httpswww.instagram.comoowlishtechnology



amazon job home linkedin career online employment remote work  international clients in united statesclients in USA vacant hiring opportunities hire application apply virtual wfh tech best english online offer vacancydesarrollador desarrolladora front end programador programadora programmer full stack software engineer desarrollo system developer application code program desenvolvedor engineer engenheiro sql sde web backend empleo trabajo ofertas emprego vagas

"
https://startup.jobs/data-engineer-azure-tiger-analytics-3697491,Engineer,Data Engineer (Azure),Tiger Analytics ,"Charlotte, United States",Full-Time,"

Tiger Analytics is a fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning and AI. We are the trusted analytics partner for multiple Fortune 500 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent as we continue to build the best global analytics consulting team in the world.
The Data Engineer will be responsible for architecting, designing, and implementing advanced analytics capabilities. The right candidate will have broad skills in database design, be comfortable dealing with large and complex data sets, have experience building self-service dashboards, be comfortable using visualization tools, and be able to apply your skills to generate insights that help solve business challenges. We are looking for someone who can bring their vision to the table and implement positive change in taking the companys data analytics to the next level.
Requirements
 8 years of overall industry experience specifically in data engineering 4 years of experience building and deploying large-scale data processing pipelines in a production environment. Strong experience in building ETL data pipelines and analysis using Python, SQL, and PySpark. Creating and optimizing complex data processing and data transformation pipelines using python. Python hands-on expertise is a must-have.   Azure experience is good to have. Design, develop, test, and deploy data integration solutions seamlessly to connect with the Enterprise Data Platform Deep understanding of data warehousing concepts, database designs and big data platforms and associated tools. Working with a wide range of data sources and intermediate expertise in SQL and PLSQLoptional Ability to work with a global team, playing a key role in communicating problem context to the remote teams, stake holders and product owners. A desire to work in a collaborative, intellectually curious environment Strong communication and organizational skills 
Benefits
This position offers an excellent opportunity for significant career development in a fast-growing and challenging entrepreneurial environment with a high degree of individual responsibility.

"
https://startup.jobs/data-engineer-mlops-job-id-2216-conversenowai-3682293,Engineer,Data Engineer / MLOps (Job ID: 2216),ConverseNow.ai ,"Bengaluru, India",,"

Who We Are

ConverseNow is the leading voice AI platform for restaurants. Our virtual ordering assistants automate drive-thru and phone orders for a growing list of the worlds leading restaurant brands. With cutting-edge technology and a passion for service, we sit at the intersection of tech, B2B SaaS and hospitality. Named to the Forbes AI 50 List, Built Ins Startups to Watch List and recognized by TechCrunch, VentureBeat, Business Insider and Nations Restaurant News among others, were proud to have created a truly best-in-class technology thats leading the AI and restaurant worlds forward.

Backed by blue chip investors such as CRAFT Ventures Tesla, Uber, Airbnb, Postmates, Slack and Enlightened Hospitality Investments led by Shake Shack Founder Danny Meyer, weve raised 35M in funding with our sights set on continued exponential growth. 

We owe our growth to the remarkable culture weve built at ConverseNow. Driven by team-selected core values of Ambition, Passion, Inclusiveness, Guest-Centricity and Low-Ego our personal favorite, we boast a fun, collaborative work environment where team members are constantly learning from and supporting one another, voices are heard, and everyone has a seat at the table. Were based in Austin, TX, with a remote team distributed across the globe. 
 

ConverseNow is looking for a Data EngineerMLOps to help in building the ultimate voice-based conversational AI. Were looking for coders and creators with a refined code sense people whove built cool things, but who can also maintain and scale those things, and who feel proud seeing their code in production.

What Youll Do

Be the technical product owner for the data pipeline, evaluationbenchmarking and monitoring tools working closely with Data Scientists to support their needs
Design, build and maintain the core data infrastructure
Design and develop Infrastructure for data pipelines, data versioning, model servingversioning and monitoring.
Design and develop Benchmarking and Evaluation Tools
Support internal teams with all Data Requirements, including analytics teams, product teams as well as the data science teams
Build, operationalize and help take POCs to production
Support all DE operations critical to Production as required

What Youll Bring

The ideal candidates would have professional experience building data and model pipelines with additional skills for production grade MLOps for cutting edge data science teams, be proficient with Python andor Golang and excited to work in a fast-moving start-up environment
Strong experience with Data Engineering - With an additional knack for building production grade pipelines for ML teams
Strong software engineering, data management skills
Strong proficiency in Python or Scala. Golang is a plus
3 years related experience strongly preferred
Experience building scalable services
Good design and problem solving skills
Experience working with cloud infrastructure GCP, AWS, Azure
Experience working with dockercontainers, andor Kubernetes
Strong with SQL andor NoSQL databases
Experience designing and creating APIs. Flask or FastAPI knowledge is a plus

What We Provide

Flexible PTO 
An incredible team with tremendous passion for the companys mission, sharing a Lets do this attitude while maintaining a strong worklife balance


We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.

"
https://startup.jobs/data-engineer-sr-python-digital-on-us-3671917,"Engineer,Python",Data Engineer Sr. (Python),Digital On Us ,"Monterrey, Mexico",,"

DATA ENGINEER SR. Python
 
We are looking to hire a data analyst to join our data team. Who will take responsibility for managing master data set, developing reports, and troubleshooting data issues. To do well in this role is necessary a very fine eye for detail, experience as a data analyst, and a deep understanding of the popular data analysis tools and databases.
 
Responsibilities
 
 You will be using Data wrangling techniques converting one ""raw"" form into another including data visualization, data aggregation, training a statistical model etc.
 Work with various relational and non-relational data sources with the target being Azure based SQL Data Warehouse  Cosmos DB repositories
 Clean, unify and organize messy and complex data sets for easy access and analysis
 Create different levels of abstractions of data depending on analytics needs
 Hands on data preparation activities using the Azure technology stack especially Azure Databricks is strongly preferred
 Implement discovery solutions for high speed data ingestion
 Work closely with the Data Science team to perform complex analytics and data preparation tasks
 Work with the Sr. Data Engineers on the team to develop APIs
 Sourcing data from multiple applications, profiling, cleansing and conforming to create master data sets for analytics use
 Experience with Complex Data Parsing Big Data Parser and Natural Language Processing NLP Transforms on Azure a plus
 Design solutions for managing highly complex business rules within the Azure ecosystem
 
Minimum Requirements
 
 5-7 years of solid experience in Big Data technologies a must.
 Mid to advanced level knowledge of Python and Pyspark is an absolute must.
 Knowledge of Azure, Hadooop 2.0 ecosystems, HDFS, MapReduce, Hive, Pig, sqoop, Mahout, Spark etc. a must.
 Significant programming experience with above technologies as well as Java, R and Python on Linux a must.
 Experience with Web Scraping frameworks Scrapy or Beautiful Soup or similar
 Extensive experience working with Data APIs Working with RESTful endpoints andor SOAP
 Excellent working knowledge of relational databases, MySQL, Oracle etc.
 Experience with Complex Data Parsing Big Data Parser a must. Should have worked on XML, JSON and other custom Complex Data Parsing formats
 Knowledge of High-Speed Data Ingestion, Real-Time Data Collection and Streaming is a plus
 Bachelors in computer science or related educational background
 
 
We Offer a competitive benefits package for you

Permanent contract for an indefinite period since the first day
Competitive salary
100 Payroll scheme  all legal benefits
Hybrid WFH scheme assist only 12 days per month to the office!
Grocery Vouchers 10
Savings Fund 13
SGMM and SGMm
22 days of Xmas Bonus
50 Vacacional prime
15 Vacations days
5 Personal days
Life insurance
Annual Performance Bonus
Payroll are the 10th and the 25th
Pension Plan
Course and Certifications among other benefits
And more

If you apply for this opportunity we will get you resume and its contain personal data whose treatment has been authorized by its owner for Digital OnUs, S. de RL de CV the ""Company. If you are not the owner of this information or have no relation whatsoever with the subjects treated in it, you are requested in the most attentive way not to make copies of it and  or its attached files and delete it immediately, under the risk of being considered as responsible for the unauthorized treatment of personal data in accordance with the Federal Law on Protection of Personal Data Held by Private Parties, its Regulations, and other applicable regulations. If you are the owner of personal data in possession of the Company and wish to obtain further information regarding the processing of your personal data or the exercise of your ARCO rights, please consult our integral privacy notice on the website httpswww.digitalonus.comprivacy-policy

"
https://startup.jobs/data-engineer-python-digital-on-us-3663005,"Engineer,Python",Data Engineer (Python),Digital On Us ,"San Pedro Garza García, Mexico",,"

DATA ENGINEER Python
 
We are looking to hire a data analyst to join our data team. Who will take responsibility for managing master data set, developing reports, and troubleshooting data issues. To do well in this role is necessary a very fine eye for detail, experience as a data analyst, and a deep understanding of the popular data analysis tools and databases.
 
Responsibilities
 
 You will be using Data wrangling techniques converting one ""raw"" form into another including data visualization, data aggregation, training a statistical model etc.
 Work with various relational and non-relational data sources with the target being Azure based SQL Data Warehouse  Cosmos DB repositories
 Clean, unify and organize messy and complex data sets for easy access and analysis
 Create different levels of abstractions of data depending on analytics needs
 Hands on data preparation activities using the Azure technology stack especially Azure Databricks is strongly preferred
 Implement discovery solutions for high speed data ingestion
 Work closely with the Data Science team to perform complex analytics and data preparation tasks
 Work with the Sr. Data Engineers on the team to develop APIs
 Sourcing data from multiple applications, profiling, cleansing and conforming to create master data sets for analytics use
 Experience with Complex Data Parsing Big Data Parser and Natural Language Processing NLP Transforms on Azure a plus
 Design solutions for managing highly complex business rules within the Azure ecosystem
 
Minimum Requirements
 
 3-5 years of solid experience in Big Data technologies a must.
 Mid to advanced level knowledge of Python and Pyspark is an absolute must.
 Knowledge of Azure, Hadooop 2.0 ecosystems, HDFS, MapReduce, Hive, Pig, sqoop, Mahout, Spark etc. a must.
 Significant programming experience with above technologies as well as Java, R and Python on Linux a must.
 Experience with Web Scraping frameworks Scrapy or Beautiful Soup or similar
 Extensive experience working with Data APIs Working with RESTful endpoints andor SOAP
 Excellent working knowledge of relational databases, MySQL, Oracle etc.
 Experience with Complex Data Parsing Big Data Parser a must. Should have worked on XML, JSON and other custom Complex Data Parsing formats
 Knowledge of High-Speed Data Ingestion, Real-Time Data Collection and Streaming is a plus
 Bachelors in computer science or related educational background
 
We Offer a competitive benefits package for you
 

Permanent contract for an indefinite period since the first day
Competitive salary
100 Payroll scheme  all legal benefits
Hybrid WFH scheme assist only 12 days per month to the office!
Grocery Vouchers 10
Savings Fund 13
SGMM and SGMm
22 days of Xmas Bonus
50 Vacacional prime
15 Vacations days
5 Personal days
Life insurance
Annual Performance Bonus
Payroll are the 10th and the 25th
Pension Plan
Course and Certifications among other benefits
And more

If you apply for this opportunity we will get you resume and its contain personal data whose treatment has been authorized by its owner for Digital OnUs, S. de RL de CV the ""Company. If you are not the owner of this information or have no relation whatsoever with the subjects treated in it, you are requested in the most attentive way not to make copies of it and  or its attached files and delete it immediately, under the risk of being considered as responsible for the unauthorized treatment of personal data in accordance with the Federal Law on Protection of Personal Data Held by Private Parties, its Regulations, and other applicable regulations. If you are the owner of personal data in possession of the Company and wish to obtain further information regarding the processing of your personal data or the exercise of your ARCO rights, please consult our integral privacy notice on the website httpswww.digitalonus.comprivacy-policy

"
https://startup.jobs/data-engineer-satori-analytics-2-3655781,Engineer,Data Engineer,- - Satori Analytics ,"Athens, Greece",,"

Satori is an Analytics Agency made with one simple vision to give clarity in decision making through data analytics.
Whether its a cloud-based big data ecosystem for a global fintech or a machine learning model predicting churn for a leading airline group, Satori develops cutting edge analytics solutions, providing real value to its clients. Services cover the whole data lifecycle from ingestion and warehousing to ML and AI applications.
Satori is a scale-up, well on track to become the leading data and analytics company in South-Eastern Europe and already a 70 specialized tech team, consisting of Data Engineers, Data Scientists, Software Engineers, QAs, and CRM Specialists, delivering innovative data, tech, and customer experience solutions. The majority of our clients are household names and global brands in FMCG, retail, ecommerce, financial services, and travel, with a footprint in Greece but also across Europe and beyond.

We are currently looking for an exceptional Data Engineer to join our team.

Your day to day

You will be working in a vibrant, fast-paced environment, so you can never expect to have just another typical day at work. Sometimes you will feel out of your comfort zone, but you will probably never feel bored! 

As part of the team, you will work with Solution Architects, Developers, Data Engineers, Product Owners, Business Analysts, Project Managers, and QAs to ensure the validity of work. You will also liaise with clients and stakeholders as part of a project team. You will be using the latest tools, techniques and methodologies.

On any particular day you will be doing several of the following

Participate in agile meetings such as daily stand ups, sprint planning and retrospectives.
Liaise with clients  BAs  Product Owners to understand detailed requirements.
Participate in design sessions with a Solution Architect, Developers, and other Data Engineers.
Create, enhance and maintain current business logic, normally through the use of SQL stored procedures, for both OLTP and OLAP systems.
Create, enhance, and maintain ETL pipelines using technologies such as Azure Data Factory Databricks.
Perform technical analysis for migrating on-premise data to a hybrid or fully cloud based data infrastructure, that is a state of the art, modern, and scalable data estate.
Migrate on-premise databases to the cloud. This includes all aspects of an on premise system for example SSIS packages, Analysis services, Reporting services, Roles and Permissions, extended stored procedures etc.
Ensure that all automated tasks are running smoothly.
Document current data processes.

We guarantee one thing, working with the latest cloud tech stack and investing in your continuous growth and development!

"
https://startup.jobs/data-engineer-homepage-86-3651993,Engineer,Data Engineer,Homepage ,,,"

About us
  At Laurens Coster, we truly care not only about our clients and their success, but also about our team and their satisfaction. We are a small, but successful agency thats been around for over 10 years on the market. This allows us to really know the people were working with every day. If theres a problem, we support and help each other, just like that.
  We know that people hate spending hours in unproductive meetings so we simply dont waste any time in them if our input is not needed. Were also not fans of corporate talk and endless procedures either. Do you want to leave earlier or start later? Maybe you have Pilates at 11AM? No problem, were open to talk about your weekly plans and tailor your working hours.
  We also value our teams independence and we want you to make good choices for yourself. You dont feel like working from home today? Come to the office. Fully remote? Sure, not a problem for us, just choose whats comfortable for you.
  As were all either IT or data or marketing nerds, we understand that continuous development is part of the job, and we encourage people to learn more and be better. Thats why each month during working hours you can spend 16 hours participating in the training you choose and well even finance it for you  just choose a course that will help you grow in your role.
  Were also open like really open! to new ideas. Do you see a possible improvement to an existing project? Implement it and make your life easier. Do you know software that can make our work more productive? Talk to us about it and well be happy to introduce it for the whole team. We just know that once youre happy with your job, itll also be a pleasure for our clients to work with us. 

Job description
Data Engineer is a position that requires both independence and the ability to work in a team. The main responsibilities in this position include designing and developing a data warehouse, building independently or collectively ETL processes. Here you will learn a professional approach to data management and you will learn about the data life cycle from the source to Business Intelligence systems.

Here are some things to expect in your day-to-day

Building and maintaining data pipelines and workflows using Apache Airflow and Apache Beam
Designing and implementing data models, database schemas, and ETL processes
Collaborating with cross-functional teams to understand business requirements and develop solutions
Ensuring data quality and reliability by monitoring and debugging data pipelines
Maintaining and improving our existing data infrastructure and processes
Staying up-to-date with industry trends and best practices in data engineering


"
https://startup.jobs/data-engineer-remote-mx-human_api-3614274,Engineer,Data Engineer (remote) MX,Human API ,,Full-Time,"

At Human API a LexisNexis Risk Solutions Company, we exist to radically accelerate the pace of health innovation for everyone. That is our vision. We unlock siloed health data from everywhere and put it directly into the hands of the consumer, right when they need it enabling a meaningful transaction that creates value in their lives. Some call us the ""PayPal"" of health data, but our dreams are bigger. 

To name a few examples, we help rapidly screen participants for clinical trials, enable life-changing software through accurate wearable data, and we accelerate the buying process for important insurance products.  Tomorrow we will inspire the innovation of impactful digital health products to maximize human longevity and potential. We are looking for talent who are equally inspired by these big ideas and have the grit and determination to make them come to life.

Were looking for people who are good at engineering systems to manipulate, process, and make sense of data. If youre an expert in some areas of data engineering, but not others, well train you up. If you have the knowledge and skills to architect and design systems, well let you do that and follow your lead. 

The core of this role is taking complex data and making it accessible for others.
What youll do

Youll build out our data platform capabilities across a wide selection of themes pipelines, graphs, ml models, annotations, automation, data observability, data lifecycle management, data governance, semantic layers using a set of core services currently AWS, Databricks, and Looker
Manage and scale data pipelines across the terabytes of arbitrarily complex data in our data lake, both structured and unstructured
Performance tuning and scaling internal and external real-time data services

About you

At least 5 years working as an individual technical contributor, in a software engineering role
3 years working with distributed data systems and data engineering
Streaming processing semantics and debugging streaming applications

Advanced proficiency in

Design and maintenance of pipelines built atop distributed data technologies DatabricksSpark, Snowflake, BigQueryRedshift, KafkaKinesis, etc. 
All aspects of managing distributed systems design, scalability, security, deployment, observability, resiliency
CICD principles even better applying devops practices to data
Workflow orchestration Airflow, Prefect, AWS Step Functions, etc.
SQL and Python, and willingness to learn other languages
Data modeling and query optimization

Qualities that will help you succeed

Keen eye for opportunities to better leverage time both your own and others
Desire to take complex problems and making them simple for others
A desire to help others grow
A commitment to continuous self improvement
An ability to operate both individually and collaboratively

Bonus 

Experience with analytics engineering working with dbt, Looker
Experience with machine learning engineering and NLP problems like entity recognitionresolution
Experience with privacy engineering 




LexisNexis Risk Solutions and Human API will deliver a next-generation consumer consent management solution that enables more seamless delivery of data. This approach empowers consumers with better access to their healthcare data and insurance resources that can improve care coordination and automate life insurance underwriting. Currently serve some of the largest financial and health companies in the world from Prudential, Allstate, John Hancock, Omada Health, Thrive Global, AAA, and many more. 

We have thousands of API integrations into enterprise health record systems, wearable devices, as well as health and wellness applications, that all benefit the end consumer and our enterprise customers.  Were looking for independent thinkers who care deeply about the problems were trying to solve. At Human API, we welcome people of all backgrounds.

LI-remote, Trabajo remoto


"
https://startup.jobs/data-engineer-rapp-3606681,Engineer,Data Engineer,RAPP ,,,"

RAPP Mauritius is looking for Data Engineer to join our award-winning team.
WHO WE ARE
We think too much marketing isnt us. Its mass markets, not audiences of one. Homogenous groups to be targeted, not individuals to be inspired. Segments to reach, not people with their own ideas and ambitions. Its one size fits all, and its dull.
When it comes to individuals, were fierce. We stand up for individuality. We speak up against bland, broad-brush generalizations. We fight for solutions that adapt to the individuals needs, beliefs, behaviors and aspirations. And we commit to doing this in every aspect of our work for clients and their consumers.
We actively foster an inclusive workplace where diversity and individual difference are valued and leveraged to achieve the agencys vision. And most importantly, we value every individuals wellbeing.
We are Fiercely Individual.
HOW WE DO IT
At RAPP, we are fiercely focused on the individual and how we can create value from every individuals experience with a brand. We do this across three capability areas customer-centric consulting, creativity that inspires action and customer experience management.
Our data analysts know who that person is. Our strategists understand what they want. And our award-winning technologists and creatives know how to get it to them.
RAPP is an integral part of Omnicoms Precision Marketing Group, comprising 4,000 creatives, technologists, strategists, and data and marketing scientists across 40 global markets.
YOUR ROLE 

Evaluate business needs, objectives and build algorithms and prototypes
Responsible for designing, building, and supporting the components of data warehouse, such as ETL processes, databases, reports and reporting environments
Conduct complex data analysis and report on results, analyze and organize raw data
Identify opportunities for data acquisition by developing analytical tools and programs
Collaborate with data scientists and architects on several projects
Consolidate and optimize available data warehouse infrastructure
Responsible for designing, building, and supporting the components of data warehouse, such as ETL processes, databases, reports and reporting environments
Assembles performance statistics, analyzes them, and makes recommendations for improvements
Establishes system documentation and ensures it is continually sustained
Conceive analytics and business intelligence platform architecture for clients, including internal and third-party clients
Collaborate with business and technology stakeholders in ensuring data warehouse architecture development and utilization
Carry out monitoring, tuning, and database performance analysis
Design, automate, build, and launch scalable, efficient and reliable data pipelines into production
Perform root cause analysis and resolve production and data issues

YOUR RESPONSIBILITIES

Experience and expertise with data models, data mining, and segmentation techniques
Experience with SQL database design
SQL Server Integration Service SSIS
Experience with programming languages e.g. Java and Python
Experience in combining raw data from different sources and conducting complex data analysis
Experience in building algorithms and prototypes and perform complex data analysis
Experience in design and implement ETL procedures for intake of data from both internal and external sources, as well as ensure data is verified and quality is checked
Experience in design and implement ETL processes and data architecture to ensure proper functioning of analytics, as well as clients or third-partys reporting environments and dashboard
Carry out monitoring, tuning, and database performance analysis
Minimum 4 years experience performing data warehouse architecture development and management
Minimum 4 years experience with technologies such as SQL Server, SQL Server Integration Service SSIS, and stored procedures
Minimum 4 years experience developing code, testing for quality assurance, administering RDBMS, and monitoring databases
High proficiency in dimensional modeling techniques and their applications
Minimum 3 years experience in custom ETL design, implementation and maintenance
Advanced SQL skills is a must

REQUIRED SKILLS

Bachelor or masters degree in Computer Science or related technical field
Data engineering certification preferable
Ideally Intermediate English Proficiency B2 reading, writing, and conversation.

 
NOTE  This job description is not intended to be all-inclusive. Employee may perform other related duties as negotiated to meet the ongoing needs of the organization.

"
https://startup.jobs/data-engineer-smile-identity-3603417,Engineer,Data Engineer,Smile Identity ,,,"


Smile Identity builds trust.
Smile Identity is Africas leading identity verification, and digital Know Your Customer KYC provider. We help companies scale rapidly across Africa by confirming the true identity of their users in real time, using any smartphone or computer. Our technology is powered by proprietary Machine Learning algorithms designed specifically for African faces, devices and network connections. 
Our team is a diverse group of hardworking, truth-seeking, and fun-loving Smilers spanning 5 offices, 10 countries and 8 time zones. Our products are already making waves across many industries, from Banking to Fintech and Telecoms. We recently announced a 20M Series B raise and are backed by leading global investors, including Norsken22, Costanoa, CRE, Future Africa, Susa Ventures, Commerce Ventures, Courtside Ventures, Two Culture Capital, Latitude, Valuestream Ventures, Intercept Ventures and Vinod Khosla who are supporting us every step of the way. 
Do you like working alongside a team of intelligent individuals? Do you want to have fun while making a real difference? Here at Smile Identity, youll get the freedom and autonomy you need to do your best work, the flexibility to be creative, and the opportunity to grow and put your unique stamp on our mission.
What are you waiting for? Come with us on this amazing journey!

The Role
We are looking for a data engineer who loves bringing order to chaos. The individual in this role will maintain our data warehouse and associated data infrastructure and provide the entire organization with the data they need to be successful. This role is open to candidates across the globe. You will be working with colleagues ranging from the US West Coast to Eastern Africa, with that in mind candidates working in timezones between US Eastern and GMT are preferred. 
What You Will Do

Work with our entire organization based in the US, London, Berlin, Lagos, Nairobi, and Cape Town to centralize our data and maintain our data warehouseslakes. 
You will select the right tools and services to bring our data together and provide a solid foundation for all our product and business analytics. Your north star? Empowering the entire organization with data to make the best possible decisions.
Design, build and launch extremely efficient and reliable data pipelines to move data.
Architect, build and launch new data models that provide intuitive analytics.
Manage the delivery of high impact dashboards, tools and data visualizations
Build data expertise and own data quality, including defining and managing SLAs for data sets.
Partner with leadership, engineers, commercial, and data scientists to understand data needs.
Influence short- and long-term strategy with cross-functional teams to drive impact.
Educate your partners Use your data and analytics experience to discover opportunities, identifying and addressing gaps in existing logging and processes.

Requirements

4 years experience with data infrastructure, ETL design, data warehousing, schema design and dimensional data modeling
2 years of experience in SQL, Python, or similar languages
Experience with designing and implementing real-time pipelines
Experience with code management tooling such as Git, Github
Experience with data migrations in production settings
You have a deep understanding of modern data tooling and infrastructure
You are comfortable working independently with periodic guidance from engineering  business teams
You are a strong believer in scale and automation
You are entrepreneurial  you take initiative, solve problems and love to troubleshoot. 
You are a great collaborator and can communicate effectively. You enjoy teaching and learning from your colleagues
You are not ideological about programming languages or tools. You have opinions but are open to discussion and tradeoffs
You are a pragmatist
You are a seeker of truth

Preferred Qualifications

Experience querying big data using Spark, Presto, Hive, Impala, etc.
Experience with data quality and validation
Experience with SQL performance tuning and E2E process optimization
Experience creating reports and dashboards with modern business intelligence tools Tableau, Metabase preferred
Experience working with Postgres, Hevo, and cloud or on-prem Big DataMPP analytics platform i.e. Snowflake, AWS Redshift, Google BigQuery, Azure Data Warehouse, Netezza, Teradata, or similar.
Interest or experience in working in the African Fintech Ecosystem
Experience in a high-growth team andor startup experience
Ability to communicate and prioritise effectively with a distributed team around the world

Compensation

Salary commensurate with experience
Stock options 
Healthcare
Opportunities for travel Post-Covid19

Autonomy and a chance to work at a mission-driven company with purpose
What success looks like
Successes in your first 3 months include

Take the time and learn the ins and outs of our data warehouse and dashboards. Investigate how data is being logged in our systems and what existing data pipelines exist across our two data warehouses Postgres and Redshift. 
Evaluate existing dashboards, data quality, and pipelines and identify gaps.
Get introduced to the Product and Engineering team and their bi-weekly sprint processes. At this point you are mostly observing the dynamics and taking on tasks by the team, while building a partnership and exploring support opportunities.

In your first 6 months

Based on your initial exposure to our data stack, you have already built several improvements based on the gaps youve identified. You are able to manage the flow of data across the stack. You have extended our system capabilities as needed and have improved efficiency and simplicity of shared tools and libraries.
You are deeply embedded in how we set up data logging and are able to manage and successfully execute on data requirements from teams across the company e.g. Engineering, Product, CVML, Data Science, Marketing, Commercial.
You proactively develop technical methodologies or tools which can solve important classes of problems. You can evangelize these methodologies and tools to other data scientists and engineers to scale multiple people.

In your first 12 months

You are the company expert in our data, infrastructure, and technical architecture, and are actively involved in product and business operations to either improve existing data tools or suggest new methodologies to accelerate team execution, including influencing data best practices.
You drive scalable solutions across teams.
You are able to solve challenging technical problems faced by multiple teams and provide significant technical advice to newer or less-technical analysts.


"
https://startup.jobs/data-engineer-allbirds-2-3593152,Engineer,Data Engineer,Allbirds ,"Córdoba, Argentina",,"

Cordoba, Argentina 
Who is Allbirds?
At Allbirds, we believe in using business as a force for good. Were a global footwear and apparel brand with roots in New Zealand and headquarters in San Francisco. Since 2016, weve been on a mission to prove that comfort, design, and sustainability arent mutually exclusive. Our commitment to creating better things in a better way is fueled by a belief that the fashion industry needs to focus less on flash and more on thoughtfulness. Were a certified B Corporation, meaning we meet rigorous standards of social and environmental performance, accountability, and transparency. Were dedicated to making the most sustainable products we can using premium natural materials - designed for lifes everyday adventures. In fact, TIME 100 named us for being one of the most innovative companies. Already calling our first shoe, the Wool Runners, the most comfortable in the world, and there is much more to come.
We take our craft seriously, but not ourselves. As part of the Allbirds family, you can look forward to team lunches, product tests, fun company events, inspiring guest speakers, comfortable shoes, sheep puns, and lots of creative ideas.
What does the job entail?

Responsible for owning entire data infrastructure, from development to support
Help make key decisions on the technologies and tools we buy or build ex. Redshift vs BigQuery vs Snowflake vs Hadoop, etc.
Engineer robust architectures to ensure near zero downtime for company-critical ETLs
Write code for all parts of our data infrastructure. This is a hands-on role
Work closely with Marketing, Supply Chain, Analytics and others to define data requirements
Design and implement data models using Cloud data warehouse best practices
Implement rigorous code reviews and testing guidelines to ensure that we have a world-class codebase that stands the test of time
Create and improve our CICD pipeline to enable our high developer productivity

What kind of person are we looking for?

You are obsessed with building high quality, performant, and resilient systems
You are versed in data warehousing architecture and data modeling best practices, including performance tuning techniques
You have experience with AWS or equivalent cloud technologies or have the aptitude to quickly learn them
You are curious and are motivated to explore new technologies on your own
You have strong opinions and can communicate them well. However, you also listen all sides of a debate and adapt to new perspectives that emerge from it
You start by fully understanding the problem first and then work your way into the right solution
Youre curious about new technologies and business problems. You will be jumping in deep into a new territory for the business, and you have a strong hunger to learn along with us
Youre customer-focused You start by fully understanding the problem first and then work your way into the right solution

More practical skills needed

BS in Computer Science or equivalent
3 years of industry experience
Exceptional analytical, conceptual, and problem-solving abilities
2 years of experience working with cloud databases, such as Snowflake, Redshift or similar
Experience with programming languages such as Python and its usage for data processing, making API calls etc.
Advanced SQL skills 
Experience with AWS or equivalent cloud technologies or have the aptitude to quickly learn them
3 years of experience creating and maintaining ETLs, building data models, designing and maintaining data warehouse
Experience creating reportsdashboards using data visualization tools like Looker and Tableau is a plus
Nice to have Experience with data governance, master data management
Nice to have Experience with DBT  Fivetran Rudderstack 
Nice to have experience with DAG orchestration

What do we offer?

Competitive salary  equity
50 Discount on Allbirds Products
Employee Stock Purchase Program

Diversity
Allbirds is an equal opportunity employer and makes employment decisions on the basis of merit. Allbirds policy prohibits unlawful discrimination based on race, color, religious creed, sex, gender identity, marital or veteran status, age, national origin, ancestry, citizenship, physical or mental disability, medical condition, genetic information or characteristics or those of a family member, sexual orientation, pregnancy, or any other consideration made unlawful by federal, state, or local laws. It also prohibits discrimination based on a perception that anyone has any of those characteristics, or is associated with a person who has or is perceived as having any of those characteristics. All such discrimination is unlawful.

WHAT PERSONAL INFORMATION WE COLLECT 
Professional, employment-related, or schooling information. Current or past job history, performance evaluations, and educational background, including grades and transcripts.

 

How We Use Your Information 
For professional, internal analysis, or employment-related purposes, including job applications.   


"
https://startup.jobs/data-engineer-miles-3590631,Engineer,Data Engineer,Miles ,"Redwood City, United States",Full-Time,"

Miles is a universal rewards app empowering anyone to earn miles automatically for all forms of travel and commute. You can then redeem your miles from amazing brands such as HP, Garmin, Pandora, Chewy, Home Chef, Buffalo Wild Wings, Wayfair, Sams Club and many more.
Similar to a frequent flyer program, but for all forms of transportation, Miles delivers value for every mile traveled, across every mode of travel, anywhere in the world. Whether by car as a driver, passenger, or rideshare, plane, train, subway, bus, boat, bicycle, or on foot, the Miles app effortlessly awards users travel - regardless of where their journey takes them. Miles can be saved or redeemed at any time - with the value increasing every month as more merchants accept them as a form of payment.
Miles is a Silicon Valley-based startup with backing from prominent VCs Porsche, Scrum, Panasonic, and Urban.us. Join the Miles family and be part of this revolutionary program!

Miles Engineering
You will be working with a great team from diverse backgrounds in a collaborative and supportive environment. We solve a wide variety of interesting technical challenges and continually build up our platform to power the next generation of scale and features. We partner closely with Product, Design, and UX teams to build and ship the most impactful

About you
You want to join an early startup on a fast growth trajectory. You are self-motivated, take end-to-end ownership, communicate effectively, and are a fast learner.

Responsibilities
 Work with product managers and data analysts to build data products that power our business. Build and maintain data analytics reports, ETL pipelines, data lakes, and streaming solutions. Analyze new data sources and work with stakeholders to understand the impact of integrating new data into existing pipelines and models Develop  maintain critical data pipelines, using tools such as Apache Airflow, to ensure highly accurate and reliable business reporting Perform ad-hoc analysis and provide insights to stakeholders within the organization such as Marketing, Growth  Sales teams to drive prioritization. Create data visualizations and analytic reports using various tools such as Tableau, Databox, etc Optimize database queries to improve and maintain performance on very large data sets. Work closely with the business and data science team for the deliverables. 
Requirements
 Masters or bachelors degree in CSMLAI or relevant computationalengineering discipline. Experience building stream-processing systems, using solutions such as Kinesis, Kafka, Storm or Spark-Streaming. Expertise with coding with Pyspark, Pandas, Scipy, and visualizations with Tableau  Databox. Schedulingorchestrating pipelines using Databricks and Airflow. Expertise with data querying SQL, Redshift, Spark and analyzing billions of rows of data points. Familiarity with SQL, especially within cloud-based data warehouses like Snowflake, Google BigQuery, and Amazon Redshift Strong understanding of relational DB such as MySQLPostgreSQL, document-based storage systems such as MongoDB or CouchDB, key-value stores Memcached, Redis, Column-oriented stores HBase, Cassandra, graph-oriented stores; their distinct advantages, disadvantages, and trade-offs. Experience building machine learning, statistical and analytical models, and tuning parameters. Experience designing experiments and extracting insights. Knowledge of various ETL techniques and frameworks  
Bonus points
 You have worked extensively with location and trip data at scale. Specialization in spatiotemporal clustering in the presence of noise. Prior startup experience. 
Benefits
 Competitive salary based on experience Opportunity to create impact in a high-growth startup environment 401K program with company matching to help you invest in your future Employee healthcare benefits include medical, dental, vision insurance Paid time off  sick days Work-from-home Fridays Wellness Day - One additional holiday each month for employee wellness Employee Referral bonus Daily office meals and a fully stocked kitchen Monthly team-building activities and happy hours Stock in an early-stage, fast-growing startup company  
Our Commitment to Inclusivity and Diversity
Miles is committed to creating an inclusive and diverse environment where people of every background can thrive and feel welcome. We consider applicants without regard to race, color, creed, religion, national origin, genetic information, gender identity or expression, sexual orientation, pregnancy, age, marital, veteran, or physical or mental disability status.

"
https://startup.jobs/data-engineer-sql-digital-on-us-3584556,Engineer,Data Engineer - SQL,Digital On Us ,"Monterrey, Mexico",,"


DATA ENGINEER - SQL
 
Data engineer requires strong technical background, hands-on experience in Data Modeling, XMLs and other Azure technologies along with data analysis skills and communication abilities. Reporting to the COG IPL and Distribution, the candidate will participate and execute multiple project teams for the delivery of data solutions, including new development, maintenance and enhancements as well as assist with the daily operations.
 
Candidate must have followed skill set to successfully fulfill their responsibilities
 
With hands-on experience in writing advanced SQL scripts, ensure data is loaded from multiple data sources to target systems
Able to quickly understand data model from physical and logical models
Hands-on experience in analyzing large volumes of data using different tools like like Excel, SQL, etc.
Basic knowledge on engineering data using SSIS
Proactively liaise with architect, business  technology partners to resolve data anomalies appropriately.
Communicate complex data issues to business partners in a concise and impactful way
Good understanding of best practices in SQL and ETL process including managing data privacy
Advanced expertise in SQL for parallel data warehouse in order to implement complex business logic
Ability to critically think and identify root cause of a data issue
Knowledge in maintaining script files using Git HUB
 
Technologies
 

Azure SQL
Azure Synaptic Analytics
Oracle
Azure Data Factory
SSIS

 
We Offer a competitive benefits package for you

Permanent contract for an indefinite period since the first day
Competitive salary
100 Payroll scheme  all legal benefits
Hybrid WFH scheme assist only 12 days per month to the office!
Grocery Vouchers 10
Savings Fund 13
SGMM and SGMm
22 days of Xmas Bonus
50 Vacacional prime
15 Vacations days
5 Personal days
Life insurance
Annual Performance Bonus
Payroll are the 10th and the 25th
Pension Plan
Course and Certifications among other benefits
And more


If you apply for this opportunity we will get you resume and its contain personal data whose treatment has been authorized by its owner for Digital OnUs, S. de RL de CV the ""Company. If you are not the owner of this information or have no relation whatsoever with the subjects treated in it, you are requested in the most attentive way not to make copies of it and  or its attached files and delete it immediately, under the risk of being considered as responsible for the unauthorized treatment of personal data in accordance with the Federal Law on Protection of Personal Data Held by Private Parties, its Regulations, and other applicable regulations. If you are the owner of personal data in possession of the Company and wish to obtain further information regarding the processing of your personal data or the exercise of your ARCO rights, please consult our integral privacy notice on the website httpswww.digitalonus.comprivacy-policy

"
https://startup.jobs/data-engineer-carbonchain-3584374,Engineer,Data Engineer,CarbonChain ,"London, United Kingdom",,"


Do you want to work on the most pressing problem of our generation? 
Were building the infrastructure for the net zero transition, and were looking for brilliant builders who want to help define a low carbon future.
Decarbonizing the economy requires a granular, real-time view of where emissions come from and how they might be reduced. We build software to automate the carbon footprinting of supply chains. Banks, traders, and manufacturers use our product to tame the complexity of international supply networks, identify the most carbon-intensive parts, and find greener alternatives. Having developed technology which is significant in advance of competitive solutions, we are now investing heavily in market adoption.You can find out more about interviewing at CarbonChain at httpswww.carbonchain.comcareersinterview-process.

To join Carbon Chain, youll be a keen technologist who loves to learn from others. Our company is made up of 10 passionate people with expertise ranging from oil refining to deep learning. Between us weve run Amazons European supply chain, built JustEats corporate meal delivery platform, and monitored industrial emissions with satellites for Al Gore. Weve got MBAs and PhDs but we know that theres a lot we dont know, and were hoping you can help fill that gap.
What will you be responsible for at Carbon Chain?
As the world wakes up to the reality of climate change and the need to decarbonize, theres a pressing need to understand the carbon intensity of every activity in the economy. Your job as Data Engineer is to work with our Data Scientists to organize, automate, and deploy the data pipelines we need to provide that understanding.
Were a small team of versatile technologists and we dont believe in a siloed approach. Our data engineers sit side by side with software engineers and designers, making sure that we have the data we need to provide the experience our customers want. Youll be deeply embedded in the product team, with your work being deployed to clients every week. Youll work closely with our domain experts, and have the chance to present to clients if thats something that excites you.
You can expect to have

Ownership of your projects
An independent path to production
The ability to make real changes with tangible business value

Our data science stack is predominantly Python. We deploy our work in a variety of ways depending upon the challenge, from Lambdas to Docker containers. Our ETL is run in Dagster, which is a friendlier and more modern version of Airflow. Youd be joining an experienced team but youd be the first data engineer, so youd have lots of scope to define best practices and choose your tools.
Were interested in talking to people with DevOps and classical software engineering experience, as well as those coming from Data Science who have a passion for scaling ETL systems.
Our only must-haves are possessing a hunger to solve business challenges using technology, the ability to build close relationships with your team, and the right to work in the UK.
Which tools, technologies, and processes will you work with?

Data processing with the standard scientific stack Pandas, Numpy, Scipy and beyond
Automation with Dagster and Github Actions
Deployment via GCP
Containerised applications are the key to our technology vision allowing us to replicate production environments locally and scale services at will.
Object-oriented code forms the bulk of our codebase.
PostgreSQL and DynamoDB managed databases form the persistence layer - youll learn to navigate document and relational databases and appreciate the values in both worlds.
Infrastructure automation is owned by the whole team, helping to spread the DevOps mentality across the whole technology department and beyond.

You dont need to be a pro at all of these skills to apply for the role, but wed love to hear about any relevant knowledge and experiences that you have in these areas.
What we require from applicants

Right to work in the UK and willingness to come to London office 2 days a week
1 years of commercial Data Engeering, Data Science, or Software Engineering experience
A passion for environmental issues
A demonstrated interest in building products and collaborating tightly with scientists and engineers
The grit and energy to work in an early stage startup

What were offering

Competitive salary  generous equity package
Flexible working hours - we encourage regular breaks and being AFK away from keyboard to support your wellbeing
Flexible working location we like to meet in the office couple of times every week
2000 annual development allowance for you to spend on developing your current skills and learning new things
Tech equipment of your choice
Team lunch on Wednesdays, and frequent pub trips
Pakt coffee and snacks of your choice in the office
26 days holiday  bank holidays 

Were striving to build a diverse team and we would love to hear from applicants from backgrounds less frequently represented in technology, be that in terms of gender, race, or professional background.

If you think your skills and experience match what were looking for and youd like to join a Carbon Tech industry unicorn, please get in touch!
 


"
https://startup.jobs/data-engineer-breakwater-technology-3554538,Engineer,Data Engineer,Breakwater Technology ,"Tallinn, Estonia",Full-Time,"

Breakwater Technology is a technology solutions company passionate about Customer tailored product development. From requirements definition and specification, software coding and development, to application support and maintenance; we can understand and assist with the entire product lifecycle. With a drive fueled by curiosity, engagement, and an agile mindset, we strive to deliver the highest possible business output of every task and project.
 
We are looking for a Data Engineer to join our team. If you passionate about building and managing data pipelines and you are always keeping in mind that the data flow is efficient and reliable, the following might be just for you!
As a Data Engineer BWT you will

Build and manage efficient and reliable batch and real-time data pipelines from disparate data sources Kafka, and 3rd party tools
Design, develop and launch data ingestion and storage systems with high availability and reliability that can scale dont worry here we like scalable and high available cloud solutions
Drive the advancement of data infrastructure by developing and implementing underlying logic and structure for how data is set up, cleaned, and stored
Architect, launch and manage automated extraction and transformation processes
Build scalable data aggregation layer from streams and batches of data for data visualization
Collaborate with development teams on design, architecture, and expansion of infrastructure
Work as an SME Operational Data Stores, Data Warehouse, and Data Marts development; guide the development design activities with input and data dependencies


To be successful in this position you need to have

Hands-on, demonstrable experience in data pipelines  architecture  modelling  governance  quality fields
Knowledge of programming languages like Scala, Java and Python
Experience in building architectures based on streaming data technologies for low-latency data processing Apache SparkFlink, Apache Kafka, Hadoop ecosystem
Experience in data pipeline orchestration Apache Airflow
Confidence in using Git, CICD, and containerization
Experience with Data Quality Tools, Monitoring and Alerting
Familiarity with core Kubernetes concepts
Experience with working with data coming from various sources RDMS, APIs, files in various formats JSON, Avro, Parquet, Delta
Experience participating in an Agile software development team, e.g. SCRUM
Experience working in the iGaming industry with gaming platforms is considered a strong advantage
Knowledge of software development lifecyclesmethodologies i.e. agile as strong presentation and collaboration skills and can communicate all aspects of the job requirements including the creation of formal documentation
Great teamwork skills and a proactive attitude 
Willingness to learn new technologies
Our working environment is fully English speaking, ability to communicate in English is required

It would be beneficial you have

Experience working in gambling or gaming industry
Experience with SQLNoSQL databases and MPPcolumnar data warehouse solutions AWS Redshift, Google BigQuery, Microsoft Azure Cosmos DB, Databricks, Snowflake, etc
Experience with any cloud platform MS Azure, Google Cloud, AWS
Experience with BI tools and data visualization PowerBI, Tableau, Spot fire, Qlikview or similar tool
Experience with Kubernetes
Experience with ETL tools like Kafka Connect, Nifi, Logstash

Why us

At Breakwater Technology you are not recruited to work for us, you are hired to work with us. This means we are fully invested in your professional growth and development and ensuring your work is engaging, fulfilling and fun
We have a vibrant multicultural working environment with representatives from over 20 different nationalities and we love the diversity this brings
Our working language is English and we work hard to create an inclusive environment, at BWT you never lunch alone unless you really want to
We are a social bunch, whatever your sport or whatever your game, you will be sure to find someone to join you. We even compete as a team in different sporting events and we like to win
Our benefits are industry leading and we value our employees offering competitive salaries and incentives
Our modern office has a stunning view of lemiste lake and as if that was not enough, we have breakfast together once a week and you can always find fresh fruit and snacks in our kitchen


Candidate experience is one of the core values in our recruitment process. To make it as transparent and smooth as possible, we have prepared this video to give you an idea what are the stages if you qualify for the role.

"
https://startup.jobs/data-engineer-dark-wolf-solutions-3554347,Engineer,Data Engineer,Dark Wolf Solutions ,"McLean, United States",,"

 

Dark Wolf Solutions is looking for a Data Engineer to support the design, development, deployment, and maintenance of a sophisticated big data ecosystem critical to answering key intelligence questions. Support a wide variety of data processing, data-flow, data management, data modeling, and data optimization efforts critical to our client. Identify needs associated with database design, optimization, and implementation to store big data datasets. Orchestrate complex data flow patterns and data enrichment analytics from a diverse and constantly growing range of data sets. Build and test solutions to address mission requirements for real-time data ingest and analysis.
 
Responsibilities

Leverage distributed compute technologies such as Spark, Hadoop, or similar.
Leverage data flow management and orchestration tools such as NiFi, Airflow, or similar.
Leverage coding languages such as Python, Java, Spark, or similar.
Implement database technologies such as SQL, Mongo, or similar.
Evaluate, prototype, and deploy big data database technologies such as Accumulo, HBASE, Cassandra, Elastic, or similar.
Utilize containerization technologies such as Docker, Podman, Kubernetes, or similar.
Leverage mathematics, computer science, and data science expertise to support analytic design, development, and implementations to support critical mission requirements.
Integrate with distributed file systems such as Hadoop File System, Gluster, Ceph, or similar.
Leverage bucket storage technologies such as S3 or similar.
Evaluate, demonstrate, and deploy hotcold storage design patterns for cost optimization.
Manage and test new models for data processing and data flow patterns. Work with key stakeholders to identify and remediate issues related to broken data flows.
Maintain awareness of emerging technologies and advancements in database design and management, data science, and machine learning.

 
Required Qualifications

5 years of relevant experience
Experience with Spark, Hadoop, or similar technologies
Coding language experience with Python, Java, Spark or similar
Experience implementing database technologies such as SQL, Mongo, etc
Experience with big data technologies such as Accumulo, HBASE, Cassandra, Elastic, or similar
Bachelors Degree

US Citizenship and an active TSSCI with Polygraph security clearance required


Desired Qualifications

Masters Degree
Experience utilizing containerization technologies such as Docker, Podman, Kubernetes, or similar

 

This position is located in McLean, VA.
 
 

BenefitsGenerous PTO policy401k with employer matchA range of medical, dental, and vision insurance options.Health Savings Account HSAFlex Spending Account FSA for medical and childcare expensesSupplemental Life InsuranceShort-term and long-term disabilityIncentive Compensation
 

We are proud to be an EEOAA employer MinoritiesWomenVeteransDisabled and other protected categories.In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification form upon hire. 

"
https://startup.jobs/data-engineer-python-scala-knorex-3540796,"Engineer,Python,Scala","Data Engineer (Python, Scala)",KNOREX ,"Pune, India",Full-Time,"

About Knorex
Founded in 2009, Knorex httpsknorex.com is a technology company that provides programmatic advertising products and solutions to marketers to connect in real-time to their desired audience worldwide. Through its flagship universal marketing platform, Knorex XPO unifies the connectivity to each of the major marketing channels including social media, search, connected TVsOTTs, video, audio, displaynative, and email, in one place, while simplifying execution and optimization across these channels through AI automation. Knorex operates across the US and APAC.
Why Knorex?
We are constantly on the lookout to recruit the best and the brightest - from engineering to sales to account management to operations and HR.
Knorex offers you many different opportunities to scale your ambition and creativity far and beyond. We embrace a dynamic and pragmatic way of doing things, setting ourselves up for long term achievement yet relentlessly focused on delivering the short term goals. If you love the joy of building stuffs and seeing them grow, growing yourself and others in the process, and challenging yourself to do stuffs that you once thought impossible, we invite you to explore a career with us.
What Knorex offers
 Competitive remuneration package including quarterly bonus pay out.  Comprehensive benefits scheme such as W3F Work, Wellbeing, Welfare Fund for courses, materials, personal health and wellbeing. Work from home benefit. Quarterly recognition program and long service reward. Career growth and personal development scheme. You will have the advance to influence and drive the changes, at Knorex we welcome good ideas and accept any possibility to make our company great. We offer great learning opportunities, you will have exposure in cutting edge performance marketing and advertising technology. Opportunity to work cross-country and with variety of projects of a different nature. 
Position DescriptionYou will work closely with our cross-country teams located regionally to learn about the business and technical analytics requirements and translate them into production system. Owing to the large data and real-time stream of data, coming up with efficient and pragmatic solutions and algorithms to the challenging problems will be come imperative. You will work with other team members to ensure the timely delivery of the systems and solutions and critically assess and monitor the efficiency andor effectiveness of the developed solution.

Key Responsibilities
 Develop clever algorithms and pragmatic solutions to our data analytics problems. Develop metrics to measure the outcomeimpact of your introduced solutions. Develop and maintain API to support other teams in retrieving the metrics Work with other members to implement and integrate into our existing systems. Document and improve the solutions over time. Evaluate and identify new technologies for implementation. Communicate with our business and technical teams to understand the analytics requirements. Respond and follow up to incorporate feedback and draw new insights. Prioritize tasks to meet multiple deadlines. 

Requirements
 Minimum 3-6 years of experience.  Must be proficient in Python  Scala.   Experience in Spark is a good plus Proficiency in multiple languages listed above is a plus. Good knowledge of algorithms and data structures Experience with ad serving, ad tracking and optimization is a plus Strong in analytics and problem solving technique Willingness to learn and able to pick up new technology or new concepts fast; Able to work independently as well as in collaborative mode with minimum supervision; Work productively even under pressure; Possess good work ethic, attitude with good follow-through; Excellent communication in written and spoken English. 

Benefits
 Ample opportunities to grow. You get to propose your own ideas and see it through Work with passionate, talented and driven colleagues who get things done!  Opportunity to work cross-country and with variety of projects of different nature  Challenging and exciting problems that await you to solve Comprehensive Health Insurance Coverage Personal Development Fund for courses and materials 

"
https://startup.jobs/data-engineer-senior-data-engineer-c3ai-3501898,"Engineer,Senior",Data Engineer/Senior Data Engineer,C3.ai ,"Guadalajara, Mexico",,"

C3.ai, Inc. NYSEAI is a leading provider of Enterprise AI software for accelerating digital transformation. The proven C3 AI Platform provides comprehensive services to build enterprise-scale AI applications more efficiently and cost-effectively than alternative approaches. The core of the C3 AI offering is an open, data-driven AI architecture that dramatically simplifies data science and application development. Learn more at www.c3.ai
C3 AI has an opening for a Data EngineerSenior Data Engineer. In this post-sales, customer-facing position, you will have the opportunity to build and maintain data models, integrations, and pipelines for Enterprise AI Applications using the C3 AI Suite. The C3 AI product suite is entirely data-driven, so a great candidate will have a passion for acquiring, analyzing, and transforming data to generate insight. This role is very hands-on and requires a structured mindset and solid implementation skills. 
Qualified candidates will have a solid knowledge of integration architecture and best-practice distributed data processing concepts. 
Responsibilities 


Engage directly with customers to participate in the design and development of data integration and architecture solution according to functional and performance requirements 


Develop complex data integrations and pipelines on high volume, high-frequency datasets to enable AI Application workflows feature engineering, training, inference, alerting 


Participate in the development of documentation, technical procedures, and user support guides 


Perform debugging, troubleshooting, modifications, and unit testing of integration solutions 


Support, monitor, and execute production application jobs and processes 


Qualifications 


2 years of experience 4 for Senior Data Engineer with systemdata integration, development, or implementation of enterprise andor cloud software 


Engineering degree in Computer Science, Engineering , or related field 


Extensive hands-on experience with data integrationEAI technologies File, API, Queues, Streams, ETL Tools, and building custom data pipelines 


Demonstrated proficiency with Python, JavaScript , andor Java 


Familiarity with version controlSCM is a must experience with git is a plus 


Experience with relational and NoSQL databases any vendor 


Solid understanding of cloud computing concepts 


Strong organizational and troubleshooting skills with attention to detail 


Strong analytical ability, judgment, and problem-solving techniques 


Interpersonal and communication skills with the ability to work effectively in a cross-functional team 


Preferred Qualificaitons


Expertise in Postgres and Cassandra 


Experience with Hadoop, Spark, and Cloud Databases Snowflake, BigQuery, Redshift 


Experience implementing and supporting DataOps in a highly available, mission-critical environment 


Working knowledge of Machine Learning algorithms 


Experience with SaaS and business intelligence or advanced analytics implementations 


C3 AI provides a competitive compensation package and excellent benefits.
C3 AI is proud to be an Equal Opportunity and Affirmative Action Employer. We do not discriminate on the basis of any legally protected characteristics, including disabled and veteran status. 

"
https://startup.jobs/data-engineer-animoca-brands-limited-3381411,Engineer,Data Engineer,Animoca Brands Limited ,,Full-Time,"


Animoca Brands, a Deloitte Tech Fast winner and ranked in the Financial Times list of High Growth Companies Asia-Pacific 2021, is a leader in digital entertainment, blockchain, and gamification that is working to advance digital property rights. It develops and publishes a broad portfolio of products including the REVV token and SAND token; original games including The Sandbox, Crazy Kings, and Crazy Defense Heroes; and products utilizing popular intellectual properties including Disney, WWE, Snoop Dogg, The Walking Dead, Power Rangers, MotoGP, and Formula E. The company has multiple subsidiaries, including The Sandbox, Blowfish Studios, Quidd, GAMEE, nWay, Pixowl, Forj, Lympo, Grease Monkey Games, and Eden Games. Animoca Brands has a growing portfolio of more than 200 investments in NFT-related companies and decentralized projects that are contributing to building the open metaverse, including Axie Infinity, OpenSea, Dapper Labs NBA Top Shot, Yield Guild Games, Harmony, Alien Worlds, Star Atlas, and others. For more information visit www.animocabrands.com or follow on Twitter or Facebook.



Responsibilities 

- You will be responsible for creating and maintaining optimal data pipelines to streamline the digestion and organization of crypto-related data
- Transform, clean and maintain the data so other business units such as research, finance could easily access and analyze the data. Front-end skill helpful but not necessary.
- Design, build and maintain real-time data pipelines that process blockchain transactions from dozens of different blockchain networks.
- Develop data models that translate complex, esoteric blockchain data into standardized formats that are analytics-ready.
- Design automated systems that evaluate and parse the results of smart contract calls.
- Work alongside the Data Science team to curate and prototype new data-sets to tackle emerging problems.
- Assemble large, complex data sets that meet functional  non-functional business requirements.
- Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
- Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS big data technologies.
- Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
- Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
- Keep our data separated and secure across national boundaries through multiple data centers.
- Create data tools for analytics and data scientist team members that assist them in building and optimizing our product.
- Work with data and analytics experts to strive for greater functionality in our data systems.

Requirements  Skill Sets

- You possess a strong technical background that includes 2 years of experience working in a senior engineering position with data infrastructuredistributed systems.

Good understanding of blockchain and cryptocurrencies is preferred

- Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases.
- Experience building and optimizing big data data pipelines, architectures and data sets.
- Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
- Strong analytic skills related to working with unstructured datasets.
- Build processes supporting data transformation, data structures, metadata, dependency and workload management.
- A successful history of manipulating, processing and extracting value from large disconnected datasets.
- Working knowledge of message queuing, stream processing, and highly scalable big data data stores.
- Strong project management and organizational skills.
- Experience supporting and working with cross-functional teams in a dynamic environment.

Skillset we are looking for any subset of the below combinations

- Databases - SQL, NoSQL, GraphQL, etcBlockchain - Solidity, Rust etc
- Data handling - Spark, Apache, Airflow including Postgres and Cassandra. 
- Big data tools - Azkaban, Luigi, Airflow, Hadoop, Spark, Kafka, etcCloud Computing - AWS cloud services EC2, EMR, RDS, RedshiftScripting languages - Python, Java, C, Scala, etc.


Working visa sponsorship and relocation allowance will be provided for non-Hong Kong candidates. 

If you feel you might be who we are looking for, please send your cover letter and CV. We regret that due to the large volumes of applications we receive, we can only respond to candidates who fit the brief.
 
Information provided will be treated in strict confidence and will only be used for recruitment-related purposes. Only short-listed candidates will be contacted. Information of unsuccessful applicants will be destroyed after six months.

THE PERKS
 
- Dynamic and Multi-cultural work environment 15 nationalities and counting!
- Token and NFT incentive schemes provided by Animoca Brands
- Flat company structure your ideas get heard by the right people very quickly
- Casual work attire
- Opportunity to get involved in working with various subsidiary brands
- Benefits Medical  Life  Transportation Allowances

Personal Information Collection Statement
Personal data provided by the Job Applicant will be used strictly in accordance with the Employers Personal Data Policies, a copy of which will be provided immediately upon hisher request. All classified data will be treated confidentially within the Human Resources  Administration Department. Personal data of unsuccessful applicants may be retained for a period of up to 3 months.

"
https://startup.jobs/data-engineer-prizeout-3360681,Engineer,Data Engineer,Prizeout ,"New York, United States",Full-Time,"

Do you remember that last A-ha moment you had? When something that seemed so complex and murky all of sudden clicked into precision and clarity? 

Prizeout is delivering these A-ha moments daily to our clients, partners, and stakeholders. We want you to be part of that reality. 

We have built a team that values humility and innovation. We believe that embracing challenges makes us better. We hire people we trust, who will live these values and who are able to find a way to navigate through obstacles and embody an entrepreneurial spirit while maintaining a spirit of optimism. 

If thats you, please read on and apply today! 
 
ABOUT THIS ROLE
As a Data Engineer at Prizeout, you will build and maintain enterprise data pipelines to enable the organizations data capabilities. This data will be used across all organizations including our engineering, analytics, data science and product teams. The ideal candidate will be able to execute on projects, drive them across the finish line, and work collaboratively with others.
WHAT YOULL BE DOING

Build infrastructure and pipelines to aggregate data from both internal and external data sources to manage data sets
Develop data expertise and own data quality for allocated areas of ownership
Support technical solutions in full-stack development tools and technologies
Collaborate with Data Scientists and product teams to recognize and help adopt best practices in reporting and analysis data integrity, test design, analysis, validation, and documentation

WHAT WERE LOOKING FOR

3 years of development experience, ideally in a microservice architecture using languages such as Node.js and Python on the backend
Experience working with distributed systems in a cloud computing environment
Strong SQL skills and database expertise
Experience in ad tech is a plus
Ability to thrive in a dynamic, fast-paced, collaborative, and high-growth environment
Team player first and foremost with a get-it-done approach
Must be willing to work full-time in NYC; this is not a remote role
The expected salary range for this position is 115,000-175,000


WHAT IS PRIZEOUT?
Prizeout is an adtech company with a three-sided marketplace that allows consumers to receive earned or allocated funds via digital gift cards. Our commercial partners offer Prizeout as a withdrawal or disbursement method, while brands can digitally advertise and acquire new customers through Prizeout ad campaigns.

Prizeout is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. 

"
https://startup.jobs/data-engineer-zeplin-3323756,Engineer,Data Engineer,Zeplin ,"İstanbul, Turkey",,"

As the Zeplin crew, were building an app used by tens of thousands of product teams daily. While creating a common language for our fellow designers and developers, we care deeply about building a product that delights them.

Data Engineer


Quality data is crucial to Zeplins success. With over 4 million users and counting, we are rapidly growing and profitable, and continuing to prepare for our future success. This is an enormous amount of data, that will only continue to grow as we grow, and is currently barely being used. There are already patterns in the data that will give us deeper insight into our customers and our product, help us quickly validate our choices and show us new areas to invest in. 
 
We are seeking a Data Engineer to help us define and implement our vision for reliable data across the company. We are building our Data Engineering practice from scratch to enable the companys success by building a solid data foundation. This is an opportunity to influence Data Engineering for a robust, but high potential, company early in its lifecycle.
 
Requirements

Deep understanding of the design, creation, organization, and business use of large data sets across a variety of data platforms
Strong communication skills to be able to work with cross functional teams to understand data requirements
Build ETL pipelines from scratch to scale our data infrastructure
Setting up the foundations for data engineering, tooling, and governance in the company while growing a new discipline

 
Qualifications

4 years experience in the field
Advanced degree in engineering or computer science

Fluent in software engineering and at least one general purpose programming language eg. Java., Python, etc


Fluent in cloud data processing technologies - Hadoop, Spark, DataFlow, Airflow

Fluent in large scale distributed data processing

Experience designing data models and data warehouses and with non-relational data storage systems NoSQL and distributed database management systems


Advanced querying SQL skills

Excellent communication and analytical skills



About Zeplin
 
Zeplin is a connected space for product teams to share designs, generate specs, assets, and code snippets. Weve reached over 5 million users from thousands of product teams, including Airbnb, Dropbox, Pinterest, Microsoft, and many more. Were a Y Combinator startup backed by notable investors like Elad Gil, Mike Maples, and Kevin Hale. Were a small, distributed crew with offices in San Francisco, Istanbul, and London.
 
Zeplin started as one piece of the product development cycle. And now designers and engineers interact at every point, from idea to production. Our goal is to help with all the issues that product teams might face in this process.


"
https://startup.jobs/data-engineer-experfy-inc-3262758,Engineer,Data engineer,Experfy Inc ,,Full-Time,"

 Iterative. They are excited to prototype at all levels of fidelityand have the humility to walk away from ideas when they fail. Collaborative. They have the ability and enthusiasm to work with researchers, engineers, business consultants, and other designers who will challenge and support one another. Comfortable with ambiguity. They know projects and businesses move fast. That means the path forward isnt always well-defined. They are comfortable and collaborative through our process. Interdisciplinary. They deliver data products for digital solutions, deploy analytical models into production, fix existing data platforms, or coach and enable other teams in best practices depending on need. 
Youre Good At
 Working with a diverse set of clients across domains and industries Implement data orchestration pipelines, data sourcing, cleansing, and augmentation and quality control processes Deploying machine learning models in production Leading data architects in designing data architectures Design flexible and scalable data architectures tailor made for the client Mentoring data engineers to further their personal and professional growth Leading other engineering staff on projects Developing teams talent by providing direction and facilitating technical architectural discussions Contribute to the running of BCG Platinion  MAYA Designs consulting business by Assisting with business development through writing proposals, scoping projects; Contributing to our thought leadership through written publications and speaking at events and conferences Translating business needs into solutions Designing overall data solution, integration, and enterprise architecture 
Youll Bring
 6 years of experience working on large scale, full lifecycle data implementation projects BSBA in data engineering, software engineering, data science, computer science, applied mathematics, or equivalent experience 5 years of experience in a client facing role 2 years professional development experience with some of the AWSAzureGCP data stack S3, Redshift, AWS glue, EMR, Azure Data Warehouse, Azure Blob Store, Google Big Query Subject matter expert in at least one area related to data management An RDBMS technology; a Big Data technology; Enterprise Data Management, Governance, Strategy, etc. A deep knowledge of performant SQL and understanding of relational database technology Hands-on RDBMS experience data modeling, analysis, programming, stored procedures Expertise in developing ETLELT workflows with one or more of the following Python, Scala, Java Deployment of data pipelines in the Cloud in at least AWS, Azure, or GCP A deep understanding of relational and warehousing database technology, working with at least one of the major databases platforms Oracle, SQLServer, Teradata, MySQL, Postgres 
Additional consideration to candidates who possess some of the following criteria
 Experience working with Big Data technologies such as Spark, Hive, Impala, Druid, or Presto A solid foundation in data structures, algorithms, and OO Design with fundamentally strong programming skills Proven success working in and promoting a rapidly changing, collaborative, and iterative product development environment Strong interpersonal and analytical skills Intellectual curiosity and an ability to execute projects An understanding of big picture business requirements that drive architecture and design decisions DevOps and DataOps skills including infrastructure as code systems like CloudFormation or Terraform Data system performance tuning Implementation of predictive analytics and machine learning models MLlib, scikit-learn, etc Willingness to travel around the globe to work with clients and BCG teams. At times, this role involves significant travel to client sites. The amount of travel will depend on client needs and nature of projects 
What to include in your application
A link to your portfolio that demonstrates your affinity for data engineering and shows how you approach digital challenges

"
https://startup.jobs/data-engineer-balbix-3253773,Engineer,Data Engineer,Balbix ,"San Jose, United States",Full-Time,"

WHO WE ARE

Balbix is the worlds leading platform for cybersecurity posture automation company. The Balbix Security Cloud uses AI and automation to reinvent how the Worlds leading organizations reduce their cyber risk. With Balbix, security teams can accurately inventory their cloud and on-prem assests, conduct vulnerability management and quantify their cyber risk in monetary terms. 


Balbix counts many global 1000 companies among its rapidly growing customer base. We are backed by John Chambers the former CEO and Chairman of Cisco, top Silicon Valley VCs and global investors. We have been called magical, and have received raving reviews as well as customer testimonials, numerous industry awards, and recognition by Gartner as a Cool Vendor, and by Frost  Sullivan. 


ABOUT THIS ROLE
As a senior data engineer you will work on complex data pipelines dealing with petabytes of data. Balbix platform is used as one of the critical security tools by the CIOs, CISOs, and the sec-ops teams of small, medium and large sized enterprises including Fortune 10 companies around the world. You will solve problems related to massive cybersecurity and IT data sets. You will collaborate closely with our data scientists, threat researchers and network experts to solve real-world problems plaguing cybersecurity. This role requires excellent algorithm, programming and testing skills as well as experience in large-scale data engineering projects.
You Will

Design and implement the features and own the modules for ingesting, storing and manipulating large data sets for a variety of cybersecurity use-cases
Write code to provide backend support for data-driven UI widgets, web dashboards, workflows, search and API connectors
Design and implement web services, rest APIs, and microservices
Build production quality solutions that balance complexity and meet acceptance criteria of functional requirements
Work with multiple-interfacing teams, including ML, UI, backend and data engineering

You Are

Driven to experience and learn more about design, architecture, and take on progressive roles
Collaborative and comfortable working with across teams including data engineering, front end, product management, and DevOps
Responsible and like to take ownership of challenging problems
An effective communicator, including good documentation practices and articulating thought processes in a team setting
Comfortable with working in an agile environment
Curious about technology and the industry, and a constant learner

You Have

MSBS 2 years in Computer Science or a related field
Expert programming experience with Python, Java, or Scala
Good working knowledge of SQL databases such as Postgres and NoSQL databases such as MongoDB, Cassandra, Redis
Experience with search engine database such as ElasticSearch is preferred
Time-series databases such as InfluxDB, Druid, Prometheus
Strong computer science fundamentals data structures, algorithms, and distributed systems


Life  Balbix
At Balbix, we have built a culture that aligns to our values of ownership, customer focus, curiosity, tenacity, innovation, judgement, teamwork, communication, honesty and impact. In joining our team youll work with very motivated and knowledgeable people, build pioneering products and utilize cutting-edge technology. Our Balbix team members see rapid career growth opportunities stemming from our culture of alignment, bottom up innovation, our clarity of goals and unrelenting mission. Last but not least, developing the worlds most advanced platform to address what the most important and hardest technology problem facing mankind today is exceptionally rewarding!

Benefits  Perks 
Balbix offers comprehensive medical, dental, vision, life insurance and long-term disability coverage for you and your family. Our Flex Time Off policy encourages you to take time off when you need it because we know and value how hard you work. When it comes to our offices its location, location, location were right next door to Santana Row so you can enjoy your time in and out of the office! 

More information at httpswww.balbix.comcompanycareers



Please reach out if you want a seat on our rocket-ship and are passionate about changing the cybersecurity equation. 

At Balbix were proud to be an equal opportunity workplace dedicated to equality, fairness and human kindness.
APPLY FOR THIS JOB


"
https://startup.jobs/data-engineer-minderaswcraft-3252427,Engineer,Data Engineer,Mindera ,"Bengaluru, India",Full-Time,"

We are looking for an experienced Data Engineer to join our team. 
Here at Mindera, we are continuously developing a fantastic team and would love for you to join us.
As a Data Engineer, you will be responsible for building, maintaining, scaling, and integrating big data-based platforms. you will also be engaged with the Data Science team in setting up and automating the Data Science modelsalgorithms for production use.
This is a fantastic opportunity for someone who is passionate about Data and is excited to use different tools to provide data insight for further analysis and drive business decisions.
National and international expected travelling time varies according to projectclient and organisational needs 0-15 estimated
Requirements
Youre great at
 Python Cloud - AWS like Glue, S3, EMR, Athena and ECSFargate or Azure Data bricks, ADF SQL Airflow Data Modelling Pyspark  
It also would be cool if you have
 Exposure to DBT would be preferable Experience working with modern data platforms such as redshift or snowflake would be preferable Experience working with Airflow, Docker, Terraform and CICD would be preferable Experience working with docker, Scala, and Kafka would be an added advantage 
What you will be doing
 Implementsupport new data solutions in the data lakewarehouse built on the snowflake Develop and design data pipelines using python. Design and Implement Continuous IntegrationContinuous Deployments pipelines. Perform Data Modelling using downstream requirements. Develop transformation scripts using advanced SQL and DBT. Write test casesscenarios to ensure incident-free production release. Collaborate closely with data analysts and different domains such as Finance, risk, compliance, product and engineering to fulfil requirements. Debug production and development issues and provide support to colleagues where necessary. Perform data quality checks to ensure the quality of the data exposed to the end users. Build strong relationships with team, peers and stakeholders. Contributes to overall data platform implementation. 
Benefits
We offer
 Flexible working hours self-managed Competitive salary Annual bonus, subject to company performance Access to Udemy online training and opportunities to learn and grow within the role 
At Mindera we use technology to build products we are proud of, with people we love.
Software Engineering Applications, including Web and Mobile, are at the core of what we do at Mindera. 
We partner with our clients, to understand their products and deliver high-performance, resilient and scalable software systems that create an impact on their users and businesses across the world. 
You get to work with a bunch of great people, and the whole team owns the project together. 
Our culture reflects our lean and self-organisation attitude. 
We encourage our colleagues to take risks, make decisions, work in a collaborative way and talk to everyone to enhance communication. We are proud of our work and we love to learn all and everything while navigating through an Agile, Lean and collaborative environment.
Check out our Blog httpmindera.com and our Handbook httpbit.lyMinderaHandbook
Our offices are located Porto, Portugal  Aveiro, Portugal  Coimbra, Portugal  Leicester, UK  San Diego, USA  Chennai, India  Bengaluru, India

"
https://startup.jobs/data-engineer-othram-3222525,Engineer,Data Engineer,Othram ,"The Woodlands, United States",Full-Time,"

Othram is the worlds only laboratory purpose-built to combine genome sequencing with advanced human identification applications. The laboratory, based in The Woodlands, Texas, is also the only facility in the United States or Canada offering end-to-end, in-house processing from forensic evidence to investigative leads. Over the last three years, this technology has helped law enforcement crack hundreds of cases at the local, state, and federal level, many of which had been unsolved for decades. 

Job Description
The Data Engineer will be responsible for implementing, maintaining and improving automated systems that process genetic, genealogical, and business information for the company.
Responsibilities

Design and develop systems for processing genomic and business data
Automate bioinformatics tasks, and scale them using cloud and local resources
Understand business objectives, and propose novel solutions and strategies to effectively meet those requirements

Requirements

Bash scripting, and familiarity with a LinuxUnix environment
Familiar with at least two programming or scripting languages
Familiar with web technologies
Experience with bioinformatics is a huge plus
Experience with containerization and cloud resources especially AWS is a plus
Excellent verbal and written communication and presentation skills
Educational background computer science, statistics, applied mathematics, or a related field.


Othram is an equal-opportunity employer, and we encourage candidates from all backgrounds to apply. Our team is on a mission to help families, affected by crime or the loss of a loved one, get the answers they deserve. If you are passionate about helping others, and have the focus and dedication to build new things from scratch, join our team and help us make the world a safer and more just place.

"
https://startup.jobs/data-engineer-security-bank-3069467,Engineer,Data Engineer,Security Bank ,"Makati, Philippines",,"

About Security Bank
Were one of the Philippines leading universal banks. Over the years, we received various awards and accolades for being one of the most stable in the banking industry.
As a Data Engineer, you will be responsible for developing strategies, managing execution timelines, and ensuring that project designs and implementations align with the data architectural vision and data governance standards as defined by Enterprise Data Architect and Data Governance Head.
What were looking for 

Bachelors degree in ComputerTelecommunication, Industrial, Computer ScienceInformation Technology, MathematicsStatistics, Economics, FinanceAccountancyBanking
Knowledge of Data Warehouse and Data Marts concepts, Query optimization techniques, Python experience, and Data Analysis
Knowledge of Microsoft BI Suite i.e, SSIS, SSRS, SSAS, PySpark, Python, Apache SQL, and Power BI.
Familiarity with AWS platform services such as S3, EC2, Athena, EMR

Fresh Graduates are welcome to apply!
LI-QA1
LI-Hybrid

"
https://startup.jobs/data-engineer-database-engineering-experfy-inc-3061735,Engineer,"Data Engineer, Database Engineering",Experfy Inc ,"Riverside, United States",Full-Time,"

As a Data Engineer for our Data Platform Engineering team you will join skilled Scala Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure Apache Spark-based, distributed SQL processing
frameworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between datawarehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a databasesystem thats highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadership
Requirements
Responsibilities
 Writing Scala code with tools like Apache Spark  Apache Arrow  Apache Kafka to build a hosted, multi-cluster data warehouse for Web3 Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques. Scaling up from proof of concept to cluster scale and eventually hundreds of clusters with hundreds of terabytes each, in terms of both infrastructurearchitecture and problem structure Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and management Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational components Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective Understand data and analytics use cases across Web3  blockchains Skills  Qualifications  Bachelors degree in computer science or related technical field. Masters or PhD a plus. 6 years experience engineering software and data platforms  enterprise-scale data warehouses, preferably with knowledge of open source Apache stack especially Apache Spark, Apache Arrow, Kafka, and others 3 years experience with Scala and Apache Spark or Kafka A track record of recruiting and leading technical teams in a demanding talent market Rock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not required Nice to have Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not required Experience with rapid development cycles in a web-based environment Strong scripting and test automation knowledge Nice to have Passionate about Web3, blockchain, decentralization, and a base understanding of how dataanalytics plays into this 

"
https://startup.jobs/data-engineer-snowflake-expert-experfy-inc-3061725,Engineer,Data Engineer - Snowflake expert,Experfy Inc ,"Boston, United States",Contractor,"

Opportunity Description
We are looking for a Data Engineer to join our digital data team in the dataarchitecture operation and governance team to build and operationalize datapipelines necessary for the enterprise data and analytics and insights initiatives,following industry standard practices and tools. The bulk of the work would be inbuilding, managing, and optimizing data pipelines and then moving them effectively into production for key data and analytics consumers like businessdata analysts, data scientists or any persona that needs curated data for data and analytics use cases across the enterprise. In addition, guarantee compliance with data governance and data security requirements while creating, improving, andoperationalizing these integrated and reusable data pipelines.  The data engineer will be the key interface in operationalizing data and analytics on behalf of the business units and organizational outcomes.
Tech skills
 Knowledge of AWS. Knowledge of Azure or GCP is a plus Orchestration Airflow Project management  support JIRA projects  service desk, Confluence, Teams Expert in ELT and ETL Expert in Relational database technologies and concepts Snowflake is a must have Perform SQL queries Create database models Maintain and improve queries performance Working knowledge of Python and familiar with other scripting languages Good knowledge of cloud computing 
Soft Skills 
 Pragmatic and capable of solving complex issues  Ability to understand business needs Good communication Push innovative solutions Service-oriented, flexible  team player Self-motivated, take initiative Attention to detail  technical intuition  
 Experience
 At least 5 years experiences in a data team as Data Engineer Experience in a healthcare industry is a strong plus Snowflake certified 
Preferred Qualifications
BS or MS in Computer Science
Requirements
Responsibilities
 Must work with business team to understand requirements, and translatethem into technical needs  Gather and organize large and complex data assets, perform relevantanalysis  Ensure the quality of the data in coordination with Data Analysts and DataScientists peer validation Propose and implement relevant data models for each business cases Optimize data models and workflows Communicate results and findings in a structured way Partner with Product Owner and Data Analysts to prioritize the pipelineimplementation plan Partner with Data Analysts and Data scientists to design pipelines relevant for business requirements Leverage existing or create new standard pipelines within to bringvalue through business use cases Ensure best practices in data manipulation are enforced end-to-end Actively contribute to Data governance community 

"
https://startup.jobs/data-engineer-gcp-experfy-inc-3061721,Engineer,Data Engineer (GCP),Experfy Inc ,,Contractor,"

Required Qualifications
 Hands on-experience working with cloud technologies  GCP knowledge strongly preferred ETL development experience with strong SQL background Hands-on experience of building and operationalizing data processing systems Experience in NoSQL databases and close familiarity with technologieslanguages such as PythonR, Scala, Java, Hive, Spark, Kafka Experience with any traditional RDBMS e.g., Teradata, Oracle, DB2. Experience working with data platforms Data warehouse, Data Lake, ODS Experience working with tools to automate CICD pipelines e.g., Jenkins, GIT, Control-M 
Requirements
Preferred Qualifications
 GCP google cloud platform experience Python  Experience working on healthcare  clinical data Data analysis  Data mapping skills JSON and XML Familiarity with HL7  FHIR 

"
https://startup.jobs/data-engineer-experfy-inc-3061720,Engineer,Data Engineer,Experfy Inc ,"Barcelona, Spain",Contractor,"

Opportunity Description
Our client is looking for a Data Engineer to join our digital data team in the dataarchitecture operation and governance team to build and operationalize datapipelines necessary for the enterprise data and analytics and insights initiatives,following industry standard practices and tools. The bulk of the work would be inbuilding, managing, and optimizing data pipelines and then moving them effectively into production for key data and analytics consumers like businessdata analysts, data scientists or any persona that needs curated data for data and analytics use cases across the enterprise. In addition, guarantee compliance with data governance and data security requirements while creating, improving, andoperationalizing these integrated and reusable data pipelines.  The data engineer will be the key interface in operationalizing data and analytics on behalf of the business units and organizational outcomes.
Tech skills
 Knowledge of AWS. Knowledge of Azure or GCP is a plus Orchestration Airflow Project management  support JIRA projects  service desk, Confluence, Teams Expert in ELT and ETL such as Informatica IICS, Databricks, Delta, Glue, ... Expert in Relational database technologies and concepts Perform SQL queries Create database models Maintain and improve queries performance Snowflake is a plus Working knowledge of Python and familiar with other scripting languages Good knowledge of cloud computing  
 Soft Skills 
 Pragmatic and capable of solving complex issues  Ability to understand business needs Good communication Push innovative solutions Service-oriented, flexible  team player Self-motivated, take initiative Attention to detail  technical intuition 
Experience
 At least 5 years experiences in a data team as Data Engineer Experience in a healthcare industry is a strong plus 
Preferred Qualifications
BS or MS in Computer Science
Requirements
Responsibilities
 Must work with business team to understand requirements, and translate them into technical needs Gather and organize large and complex data assets, perform relevant analysis  Ensure the quality of the data in coordination with Data Analysts and Data Scientists peer validation Propose and implement relevant data models for each business cases Optimize data models and workflows Communicate results and findings in a structured way Partner with Product Owner and Data Analysts to prioritize the pipeline implementation plan Partner with Data Analysts and Data scientists to design pipelines relevant for business requirements Leverage existing or create new standard pipelines within to bring value through business use cases Ensure best practices in data manipulation are enforced end-to-end Actively contribute to Data governance community 

"
https://startup.jobs/data-engineer-azure-tiger-analytics-3050429,Engineer,Data Engineer Azure,Tiger Analytics ,,Full-Time,"

Tiger Analytics is a fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning and AI. We are the trusted analytics partner for several Fortune 100 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent in various US locations as we continue to build the best analytics global consulting team in the world.
Key Responsibilities
-Define process to analyze and map specific data hierarchies across functions, customers, and systems. 
-Collaborate with Data Management Leads to ensure the model and lexicon of the data assets meets users needs. 
-Define sources and calculations for metrics and KPIs. Lead metrics standardization in functions, reconciling data as necessary. 
-Align hierarchy with corresponding business rules, requirements, and data transformations. 
-Define and manage an exception process to manage hierarchies and cross functional metrics. 
-Develop and maintain data models. 
-Build technical specs and map documentation.
-Partner with IT to develop data architecture standards and guidelines for processing and persisting data on prim and cloud. 
-Partner with IT to ensure appropriate standardization, categorization, and hierarchy is followed. 
Requirements
-Undergraduate Degree in Computer Science, Engineering, Mathematics, Statistics, or similar field.  -Minimum 5 years of experience in data architecture. -Strong SQL abilities.  -Deep understanding of data structure and cross-functional hierarchies to address business needs.  -Ability to translate business requirements into technical requirements and reverse engineering.  -Ability to communicate data hierarchy management procedures to non-technical stakeholders. -Ability to gather requirements to create conceptual, logical, and physical models. -Ability to perform data profiling to create business mapping and technical design.  -Knowledge of Cloud tools and systems; Azure preferred. -Knowledge of data storage systems, data factory, and data lake.  -Knowledge of cleansing data and building data pipelines.  -Consumer Products industry experience a plus.
Benefits
Significant career development opportunities exist as the company grows. The position offers a unique opportunity to be part of a small, challenging, and entrepreneurial environment, with a high degree of individual responsibility.

LI-remote

"
https://startup.jobs/data-engineer-for-amaan-project-dkatalis-3007796,Engineer,Data Engineer (for AMAAN Project),DKatalis ,"Jakarta, Indonesia",,"

Job Description
 
We are seeking a hands-on data engineer for our business unit in Amaan to help us build out and manage our data infrastructure, which will need to operate reliably at scale using a high degree of automation in setup and maintenance. The role will involve both setting up and managing the data infrastructure, as well as building and optimizing key ETL pipelines on both batch and streaming data. The ability to work with the teams from product, engineering and BIanalytics is essential. Ownership needs to be taken of data model design and data quality. Automation and the use of data science to manage and improve data quality would be valued. The individual will also play active roles in ensuring data governance policies and tooling are implemented and adhered to
A high degree of empathy is required for the needs of the downstream consumers of the data artefacts produced by the data engineering team, i.e. the software engineers, data scientists, business intelligence analysts, etc and the individual needs to be able to produce transparent and easily navigable data pipelines. Value should be assigned to consistently producing high quality metadata to support discoverability and consistency of calculation and interpretation.
Developer who is interested in Data Engineering are welcome to apply.
 
Candidates should have a set of experience across the following systems and languages

Apache Kafka
Apache Flink
Apache Airflow
PostgreSQL
Docker
Python and Java


"
https://startup.jobs/data-engineer-community-tech-alliance-2987940,Engineer,Data Engineer,Community Tech Alliance ,,,"

As a data engineer, youll leverage the products we build, along with our data warehouse and ETL pipelines, to empower our progressive organization partners to answer critical questions with data. Youll work alongside our software engineers to develop pipelines in our ETL platform, build production data materializations in BigQuery, and design solutions to pressing analytics and targeting problems in the progressive space.

About us
Community Tech Alliance is a group of progressive technologists and strategists formed to provide data infrastructure building blocks to the progressive ecosystem at a low cost. CTA seeks to uplevel program impact by unlocking the potential of data, using software and data engineering, and removing the barriers to entry. We are a small team of engineers, data practitioners, product managers, and strategists looking to create infrastructure for progressive change.
Community Tech Alliance believes strongly that

Inclusive teams are strongest, and supportive work environments take investment, intentionality, and openness
Empathy is the cornerstone of building smart technology solutions
All team members should take ownership of the project and teams development
Iteration is key, and smart solutions require action not perfection
Nothing great has been built without making mistakes and learning from them


"
https://startup.jobs/data-engineer-tabeo-ltd-2974060,Engineer,Data Engineer,Tabeo Ltd. ,,Full-Time,"

Tabeo is building the Shopify for private healthcare. Our platform simplifies payment and marketing services for local clinical providers including dentists, audiologists and opticians. Customers benefit from a quicker, easier access to relevant payment options in one unified check out flow.
Today, over 8,000 dentists across the UK use Tabeo. 200 more join our platform every month. Tabeo is the dominant market leader for patient finance and seeks to be the same for cards. Sector specific features and super competitive pricing will help us upsell the service. In parallel, we want to capture more market share in new verticals hearing and vision care. 99 of payments rely on cards so you buildown the data highway to simplify marketing flows for our Customers.
In H2 2024, we will expand to the US and EU. This will create exciting new challengesopportunities. The business is profitable and we plan to grow revenues by over 100 per year in the next 5 years.
Tabeo is growing its team from 40 to 50 in 2023. We believe in a remote first approach to work and equip teams with all perks and tools so they can thrive. However, we all come together 3x per year for Team Weeks. Last year, we met in Dubai, Lisbon and London. Next, it is Budapest and Reykjavik.

About the role
Growth brings data and we have a lot that we need to get in order. We are looking for a data engineer to sit between the tech team and the data analysts to take production data and help show the business how key areas are performing. You will join a small data team which is wholly responsible for importing, analysing and presenting data to the business and our partners. We have built some data infrastructure but there is plenty to do with the forthcoming expansion of the company.
Responsibilities
 Overseeing and owning the regular data import processes with aim of 100  Translating new business requirements into structured data logic Writing scalable, robust, testable, efficient, and easily maintainable code Building and maintaining real-time alert systems Contributing to the data team roadmap and direction. Were a small company and your input will be important 
Requirements
 Minimum 3 years experience of owning and deploying data architecture  Experience coding in Python Experience maintaining a BigQuery database Experience maintaining workflows with Apache Airflow Excellent communication and coordination skills Excellent troubleshooting Ability to solve problems creatively and meticulously Result-oriented approach Ability to spot necessary requirements outside of those briefed  Nice to haves
 Knowledge of fintech or consumer lending Experience with GCP Experience with Go 
Benefits
 Competitive salary and annual bonus Employee share options 2,000 per annual CPD package to be used at your discretion Full kit for home office Health insurance and life assurance including dental Quarterly company trips 24 days holiday annually Extended parental leave Contributing pension scheme 
We are considering applicants for our London office or fully remote from anywhere GMT - 3.
If youre applying to work remotely from the UK you have to be eligible to work in the UK. For working outside of the UK remotely GMT -3 eligibility to work in the UK is not required.

"
https://startup.jobs/data-engineer-hong-kong-lynx-analytics-2874636,Engineer,Data Engineer (Hong Kong),Lynx Analytics ,,,"

COMPANY OVERVIEW
Founded in 2010, Lynx Analytics is a predictive analytics company run by world-class quantitative marketing scientists and industry-experienced data scientists. Our focus is to become a leading analytics solution provider in our chosen fields of expertise telecom, retail, life sciences, and financial services while advancing graph analytics technology.
Lynx is headquartered in Singapore with operations in Hong Kong, Germany, USA, Hungary, South Africa, Indonesia, and several other Southeast Asian countries. We work with some of the worlds largest companies and are constantly looking to expand our knowledge base and geographical footprint. Lynx Analytics technology is deployed with various Clients across Asia and has significant growth potential.
We have a diverse and inclusive global team comprising Professors, PhDs, MScs, and MBAs from Ivy Leagues, INSEAD and NUS with a broad spectrum of experience in start-ups and blue-chip companies Google, SAP, Vodafone, GE, Morgan Stanley, Barclays, HSBC to name but a few. It is the combination of our industry insight and experience, scalable proprietary technology, and highly qualified people that drives our compelling value proposition.
We are looking for ambitious, innovative, empathetic and relentless team players to explore the career opportunities that we offer as we continue to scale our operations.

We are looking for a Data Engineer to work on automating and productizing advanced big data transformation and analytics pipelines. You would be working with standard big data technologies Hadoop, Spark, etc,, as well as our proprietary big graph analysis framework.
KEY RESPONSIBILITIES
A Data Engineers responsibility is to implement and deploy data analysis pipelines at various clients of Lynx Analytics. This includes participating in the activities below

Understand deeply the business problem that we are trying to solve by our analytical solution
Through continuous consultations with employees of our client, discover the clients existing data sources that are relevant to the problem we try to solve. This includes discussions with client IT, data owners, future business users, etc.
Working together with the IT teams of the client, define the technical architecture for the analytical solution that we are to deploy for the client.
Implement the data ingestion subsystem this is the system responsible for moving all the necessary data sources to a single location where the actual analysis will happen.
Implement the data analysis pipelines. 
Integrate the results into business UIs developed by Lynx or pre-existing client software systems

REQUIREMENTS

Relevant tertiary qualification, preferably at Masters level or above, in Engineering or another relevant discipline with strong academic results
Strong programming skills
Experience in Power BI
Solid knowledge of Python and Java or better yet, Scala
Good understanding of the Linux OS including basic sysadmin and shell scripting abilities
SQL
Experience in project delivery in a B2B setting
Good problem-solving skills
Fluency in English and Cantonese
Willingness to travel

DESIRABLE

Experience in Big Data
A minimum of 3 years of experience in Data Science or Analytics
Industry experience in working for a big enterprise like our clients

WHAT WE OFFER

Opportunity to work on creating innovative, leading-edge data science pipelines using our state of the art, in-house built big graph tool
Work closely with the developers of the big graph tool you will be building upon
Be a member of a very strong team with mathematicians, ex-Googlers, Ivy League  professors, MBA alumni and telecommunications industry experts
Startup atmosphere
Competitive salary
Equity incentives for employees
Opportunity to travel Southeast Asia, US, and Europe
Flexible working hours, family-friendly workplace



"
https://startup.jobs/data-engineer-blackbirdai-2870292,Engineer,Data Engineer,Blackbird.AI ,"New York, United States",Full-Time,"

This is a fully remote opportunity at Blackbird.AI. You will not be required to relocate.

The Company
What has been the effect of disinformation on the world?Blackbird.AI creates leading-edge AI software to provide critical real-time insights to provide our clients with a deep understanding of ongoing disruptive narratives, their motives, and overall digital noise. We are united by our dedication to our mission. We believe that we have a responsibility to society and that our service is vitally needed by organizations and individuals to create an empowered and critical thinking society.
If this mission resonates with you, wed love to hear from you.
The Opportunity
Get ready to join a small but growing team of highly talented engineers and leaders, building exciting AI-driven services and technologies. As a Data Engineer for Blackbird.AI, you will own the pipeline optimization for a real-time streaming cloud-hosted analytics platform that spans data collection and analysis, and serves results to a user dashboard for interactive visual exploration. Our position requires a breadth of experience with database technologies, especially the engineering of horizontally scalable solutions for big data.
Responsibilities
 Writes ETL processes to support ingestion and normalization of a wide variety of social media, news, and web scrape formats Designs database systems and develops tools for query and analytic processing, including for streaming real-time applications Performs analysis and comparative empirical studies to evaluate performance tradeoffs with respect to scaling e.g., cost vs throughputlatency Develops, manages and owns the database architecture for a real-time streaming cloud hosted analytics platform, spanning data collection, analytics and user management Owns build automation, continuous integration, deployment and performance optimization in compliance with our security requirements 
Requirements
Must Have
 BS degree in Computer Science or equivalent Demonstrated product success with deployment in the cloud and SaaS model; proven capability to develop processing pipeline for platforms that are optimized for streaming analytics applications and that are cloud agnostic Kubernetes, dockerized solutions Expert level capable on PostgreSQL, Neo4j graph, ElasticSearch, MongoDB, Redis, Druid, with other NoSQL and graph DBs helpful Experienced with horizontal scaling of databases Experienced with Kafka and Airflow; expert in applying tools for runtime profiling to optimize throughput and latency and establish comparative performance benchmarks Capable in build automation, continuous integration and deployment CICD tools, e.g. Webpack, Buddy or using Jenkins  docker Expert level Python code development Experience working with distributed teams  
Helpful to Have
 Technical background in Artificial Intelligence AI and Machine Learning ML  Experience designing and implementing interactive query-driven man-machine intelligence systems Solid skills in Java 
Benefits
 Health Care Plan Medical, Dental  Vision Paid Time Off Vacation, Sick  Public Holidays Work From Home Stock Option Plan Exciting career development prospects, to grow into leadership roles  
Take note - due to the high volume of applicants, only shortlisted candidates will be notified. Thank you for taking the time to apply for the role at Blackbird.AI.

LI-Remote

"
https://startup.jobs/data-engineer-on-azure-agile-actors-2856890,Engineer,Data Engineer on Azure,Agile Actors ,"Chalandri, Greece",Full-Time,"

Who we are 
A coaching and learning ecosystem for talented and passionate tech professionals where you can find your next career goal in a diverse and multidisciplinary environment. At Agile Actors, you will experience continuous growth and development through coaching, learning and practice! An innovative self-paced personal development and rewarding model will support your advancement and along with the necessary tools, appropriate learning material, and real projects from organisations that are leaders of the industry both domestic and international, such as RedHat, Blueground, GFK, Austrian Post, etc, cultivate a continuous growth mindset! 
Be part of both the customers and the Agile Actors team, providing high-quality deliverables for the former and contributing to the cultivation of an inclusive and developmental culture in the latter! 
Who we are looking for
We are looking for passionate, multi-talented Data Engineers across levels  i.e. Juniors, Mids and Seniors. The successful candidates will be part of a highly motivated blended team that provides Data Management and Business Intelligence services, including Data Warehousing, Data Marts, Database Integration Applications, Streaming Data Ingestion, and Streaming Analytics on Microsoft Azure Cloud.
Requirements
 At least 1 year of related working experience for the junior level Good knowledge of OLTP, Data Warehouse relational and Multi-Dimensional databases design and implementation  Normal Forms, Star Schemas, ideally working experience on Azure Synapse or Azure SQL DW Experience with T-SQL programming Store procedures, functions, joins, analytics functions, CTEs  Understanding of Database optimization techniques indexes, partitioning  Understanding ETL process design and experience with an ETL tool used in the industry eg SSIS, Azure Data Factory, Databricks Some experience with Apache Spark Understanding of Azure Data Lake Storage  Bachelors degree in Computer Science or equivalent subject Understanding of fundamental computer science knowledge data structures, algorithms etc Strong problem-solving skills and analytical thinking along with a desire to keep learning, growing  teaching 
Benefits

Why join us? 
Join us if you enjoy being part of a people-centered culture that empowers you to work on the most important product; yourself! Continuously grow with the support of tech experts and maintain a matching skill set to market needs. 

 Personal Development Plan crafted with your dedicated coach  360 continuous feedback model; drive your developmental pace  Unlimited personal traininglearning budget to cover all your career developmental needs  Chapters internal communities-share knowledge, create training material, help others grow and shape our technological future  Vast roster of customersaccounts, providing more opportunities for growth and development within the AA ecosystem  Onboarding Buddy-Continuous support from day 1! A dedicated buddy to help you navigate through the first few weeks  Tailored remuneration package according to your level and expertise  Private Health Care insurance for physical wellbeing  Professional helpline for our professionals and their families for mental wellbeing  Flexible working conditions fully remote work according to assigned account  

By clicking ""Apply"" for this Job, you agree that you have read and accepted our Data Protection Statement relating to job applicants and that you provide your consent for the processing of your personal data for the purposes described therein

"
https://startup.jobs/data-engineer-assuranceateam-2835014,Engineer,Data Engineer,Assurance Careers ,"Toronto, Canada",Full-Time,"

About Assurance
Assurance IQ is a technology company headquartered in Seattle. We were acquired by Prudential NYSE PRU to further the joint mission of improving financial wellness across the world. 

Our team of world class software engineers, data scientists, and business professionals work every day to expand our product offerings and the reach of our platform. We simplify the complex world of insurance and financial services into straightforward, valuable solutions to improve peoples lives. We start by asking customers a few questions, so our system can learn about their needs; from there, our ground-breaking, proprietary platform takes over and analyzes the thousands of data points that make customers unique. This is how we create custom-tailored plans for each customer; plans built precisely for their needs and budget. Our platform serves as the intersection between customer and seller, technology, and the human touch. 

At Assurance, we are innovative, persevering, collaborative, calculated, and authentic, and were working together to improve the lives of millions!


About the Position 

As we build the future of consumer insurance in a modern age, data is at the core of everything that we do.  The role requires team members who are adept at building software tools to move and organize data with an approach that is rooted in improving the insights and efficiency of the business.  Our team uses a variety of data mining and analysis methods, a variety of data tools, builds and implements models, develops algorithms, and creates simulations.   Our Data Engineers design and build the backbone that makes this development possible with no support from engineering we own our stack end to end.  At Assurance, we hire experts in their field, and we give them the independence and trust to build based on their expertise. 
To be successful in this role, you must possess the following

Experience with Python and SQL
Experience in data modelling  
Business Acumen  you are always eager to understand how the business works, and more specifically, how your work impacts the business.  
Comfort with QAing your own data, to include menial tasks like listening to calls or scrubbing excel files to ensure everything is correct 
Comfort with learning new technologies to help the team explore new solutions to existing problems 
Excellent communication ability  you can explain your work in a way that anyone on the team can understand, and you can frame problems in a way that ensures the right question is being asked. 
Enthusiastic yet humble  you are excited about the work you do, but you are also humble enough to embrace feedback  you dont need to be the smartest person in the room. 
Bachelors degree in mathematics, statistics, data science or related field of study.

The following additional experience is desired 

You have a proven ability to drive business results by building the right infrastructure that enables data-based insights.
You are comfortable working with a wide range of stakeholders and functional teams.
The right candidate will have a passion for enabling the discovery of solutions hidden in large data sets and working with stakeholders to improve business outcomes.  
Were growing at a rapid pace, so its important that you embrace the opportunity to blaze your own trail. 
You thrive in a fast-paced environment where priorities can shift rapidly as we corner opportunity.  
You can work independently, with little oversight or guidance.   





Note Assurance is required by multiple state and city laws to include the salary range on position postings when hiring in those specific locals. The salary range for this position will be between 130,000-170,000 and may be eligible for additional bonus or commission plans  benefits. Eligibility to participate in the bonus or commission plans is subject to the rules governing those programs, whereby an award, if any, depends on various factors including, without limitation, individual andor organizational performance. In addition, employees are eligible for standard benefits package including paid time off, medical, dental and retirement.

"
https://startup.jobs/data-engineer-intern-vee-2798831,Engineer,Data Engineer Intern,Vee ,,Internship,"

Position Data Engineer Intern REMOTE
Reports to CTO
Location Remote
 
Are you passionate about architecting data solutions and applying your engineering experience to drive business decisions? Interested in how organizations use analytics and data to innovate, grow and transform? Driven to help establish, define, and evangelize how data architecture can help drive a strategy? 
 
Company Overview
Vee inspires people to align on Purpose so they can unlock their individual and collective human potential. We are an early stage startup poised for dramatic growth. We specialize in developing ways to drive greater strategic alignment to unlock enterprise innovation and growth. After spending considerable time exploring the need for better alignment models across diverse organizations, we have developed a framework and methodology based on insights from extensive research. We are building products to assess gaps and are offering our consulting services to help 21st century leaders develop and implement their organizational and people strategies, along with the capabilities required to operationalize their Purpose as they transform.
 
 
Position Overview
Join our fast-growing team! You will help drive business results through building a robust data engine to build business-critical, scalable, and robust data pipelines and intuitive data products that power data discovery and analysis. You will help build Vees data capability and data strategy.

Help architect the foundation of Vees data capability
Building a harmonized schema to feed downstream processing and natural language analysis
The data is contained in multiple source systems which needs to be filtered as appropriate to a model that captures interoperability conditions
Design the data flow with regard to structural soundness and robustness

 
General Responsibilities

Build highly scalable data pipelines to power experiments and key business metrics
Design a harmonized schema to feed downstream processing
Partner with Data Scientists and the People Science team to research and develop statistical learning models for data analysis
Devise innovative and forward-thinking solutions
Keep up-to-date with the latest technology trends
Communicate results and ideas to key stakeholders at Vee

 
Core Experience  Requirements

Software Engineering experience with proficiency in at least one high-level programming language preferably Python.
Strong SQL skills
SQL-centric ETL Extract, Transform, and Load but will consider someone from an Engineering background who is more familiar with JVM-centric ETL
Have excellent pattern recognition and predictive modeling skills
Knowledge of data visualization tools e.g. Tableau

 
Preferred Experience

Experience with AWS Data Lakes, S3, Redshift
PostgreSQL
ETL
Data Lineage knowledge
Familiar with or awareness of automated workflows like Airflow

 
Profile
Purpose is what drives you. Your passion for your work has led you here. You are ready to combine your experience with ours to help our client companies gain competitive advantage. You bring strong creative thinking skills in addition to superior communication skills, allowing you to develop creative solutions for any challenge that comes your way.
 
You

Love what you do, love to be busy, and love to produce by being organized and methodical
Work with a sense of urgency and have a strong drive for results
Have the resilience and agility to adapt quickly in a fast-paced environment
Work independently as well as collaboratively to stretch thinking into creative solutions
Have strong verbal and written communication skills
Are excited about and suited for a startup, where youll be wearing a few hats
Are smart and fun with an empathetic nature, which will add to our culture

 
About Vee
Our founders and team have deep expertise in branding, innovation, design, IO psychology, organization design, digital  data product development, enterprise software and application development complemented by experience with clients ranging from emerging Silicon Valley unicorns to established leaders in government and the public and private sectors including American Express, Bank of America, BMW, Box, HP, Lowes, Mastercard, Microsoft, Nissan, Pepsico, Procter  Gamble, SAP and The Clorox Company, among others. For more information, check out our website in transition at httpswww.letsvee.com.

"
https://startup.jobs/data-engineer-paypay-2660431,Engineer,Data Engineer,PayPay ,,,"


About PayPay
PayPay is a fintech company that has grown to over 58M users since its launch in 2018. Our team is hugely diverse with members from over 50 different countries. To build ""PayPay"", we allied with Paytm; the biggest payment service company in India. Based on their customer-first technology, we created and expanded the smartphone payment service in Japan.Our biggest competitor is ""cash"". We seek people who can accept this challenge positively, brush up the product at a tremendous speed, and promote PayPay with professionalism and passion.

Job Description

Design, build, and maintain distributed batch and real-time data pipelines and data models.
Facilitate real-life actionable use cases leveraging our data with a user- and product-oriented mindset.
Be curious and eager to work across a variety of engineering specialties i.e., Data Science, and Machine Learning to name a few.
Support teams without data engineers with building decentralized data solutions and product integrations, for example around DynamoDB.
Enforce privacy and security standards by design.
Conceptualize, design and implement improvements to ETL processes and data through independent communication with data-savvy stakeholders.

Extra requirements depending on the potential team if it has been identified
 
Qualifications

3  years experience building complex data pipelines and working with both technical and business stakeholders.
Experience in at least one primary language e.g., Java, Scala, Python and SQL any variant. 
Experience with technologies like BigQuery, Spark, AWS Redshift, Kafka, or Kinesis streaming.
Experience creating and maintaining ETL processes.
Experience designing, building, and operating a DataLake or Data Warehouse.
Experience with DBMS and SQL tuning.
Strong fundamentals in big data and machine learning.


Extra requirements depending on the potential team if it has been identified
 
Preferred Qualifications

Experience with RESTful APIs, PubSub Systems, or Database Clients.
Experience with analytics and defining metrics.
Experience with measuring data quality.
Experience productionalizing a machine learning workflow; MLOps
Experience in one or more machine learning frameworks, including but not limited to scikit-learn, Tensorflow, PyTorch and H2O.
Language ability in Japanese and English is a plus We have a professional translator but it is nice to have language skills.
Experience with AWS services.
Experience with microservices.
Knowledge of Data Security and Privacy.

Extra requirements depending on the potential team if it has been identified
 
PayPay 5 senses

Please refer PayPay 5 senses to learn what we value at work.


Working Conditions 
Employment Status

Full Time

Office Location

WFAWork From Anywhere at Anytime  
Remote Job 
You can live anywhere in Japan 

Work Hours

Super Flex Time No Core Time
In principle, 1000am-645pm actual working hours 7h45m  1h break

Holidays


Every SatSunNational holidays In JapanNew Years breakCompany-designated Special days


Paid leave

Annual leave up to 14 days in the first year, granted proportionally according to the month of employment. Can be used from the date of hire
Personal leave 5 days each year, granted proportionally according to the month of employmentPayPays own special paid leave system, which can be used to attend to illnesses, injuries, hospital visits, etc., of the employee, family members, pets, etc.

Salary

Annual salary paid in 12 installments monthly
Based on skills, experience, and abilities
Reviewed twice a year
Special Incentive once a year Based on company performance and individual contribution and evaluation
Late overtime allowance, Work from anywhere allowance JPY100,000

Benefits

Social Insurance health insurance, employee pension, employment insurance and compensation insurance
401K
Language Learning support
TranslationInterpretation support
VISA sponsor  Relocation support



Other Information

PayPay Inside-Out Corporate Blog JP
PayPay Inside-out Corporate Blog ENG
PayPay Product Blog JP
PayPay Product Blog ENG



"
https://startup.jobs/data-engineer-netomi-2613159,Engineer,Data Engineer,Netomi ,"Gurugram, India",Full-Time,"

At Netomi AI, we are on a mission to create artificial intelligence that builds customer love for the worlds largest global brands.

Some of the largest brands are already using Netomi AIs platform to solve mission-critical problems. This would allow you to work with top-tier clients at the senior level and build your network. 

Backed by the worlds leading investors such as Y-Combinator, Index Ventures, Jeffrey Katzenberg co-founder of DreamWorks and Greg Brockman co-founder  President of OpenAIChatGPT, you will become a part of an elite group of visionaries who are defining the future of AI for customer experience. We are building a dynamic, fast growing team that values innovation, creativity, and hard work. You will have the chance to significantly impact the companys success while developing your skills and career in AI.

Want to become a key part of the Generative AI revolution? We should talk. 

Job Description

Netomi is seeking a highly analytical and detail-oriented candidate to join the Analytics team in Gurugram. As part of the team, you will work with product, engineering, and customer success teams to drive complex data and trend analyses to propose ways to improve and, thereby contributing to improve the experience. You will also be responsible for benchmarking and measuring the performance of various product operations projects, building and publishing detailed scorecards and reports, identifying and driving new opportunities based on customer and business data.

We are looking for a Data Engineer with a passion for using data to discover and solve real-world problems. You will enjoy working with rich data sets, modern business intelligence technology, and the ability to see your insights drive the features for our customers. You will also have the opportunity to contribute to the development of policies, processes, and tools to address product quality challenges in collaboration with teams.
Responsibilities

You will partner with teammates to create complex data processing pipelines in order to solve our clients most ambitious challenges.
 you will collaborate with Data Scientists in order to design scalable implementations of their models
You will pair to write clean and iterative code based on TDDLeverage various continuous delivery practices to deploy, support and operate data pipelines
Advise and educate clients on how to use different distributed storage and computing technologies from the plethora of options available
Develop and operate modern data architecture approaches to meet key business objectives and provide end-to-end data solutions
Create data models and speak to the tradeoffs of different modeling approaches
Seamlessly incorporate data quality into your day-to-day work as well as into the delivery process

Requirements

3 Years of Experience
Expertise in SQL, PLSQL  General Software Engineering proficiency coding in PythonJava any one language
Experience in MySQL 8.0, and AWS Aurora required
Expert SQL query optimization
Have a good understanding of data modeling and experience with data engineering tools 
You are comfortable taking a data-driven approach and applying data security strategy to solve business problems
Youre genuinely excited about data infrastructure and operations with familiarity working in cloud environments
Working with data excites you you can build and operate data pipelines, and maintain data storage, all within distributed systems
Assure effective collaboration between Netomi and the clients teams, encouraging open communication and advocating for shared outcomes
Experience writing data quality units and functional tests. 
Strong Experience with any Relational Database preferably Aurora MySQL
Experience with AWS Lambda, Kinesis, RDS, EC2, Quicksight
Experience with Streaming Platforms such as Kafka, Kinesis, etc.


Netomi is an equal opportunity employer committed to diversity in the workplace. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, disability, veteran status, and other protected characteristics.

"
https://startup.jobs/data-engineer-wattpad-2510974,Engineer,Data Engineer,Wattpad ,"Toronto, Canada",Full-Time,"

Wattpad is a global multiplatform entertainment company whose vision is to entertain and connect the world through stories. Since 2006, weve been on a mission to use the power of community and technology to unleash the full potential of stories to the world. Every month 85 million people around the world spend over 23 billion minutes on Wattpad to share and discover stories they cant find anywhere else.  Our brand banner includes Wattpad, Wattpad WEBTOON Studios, Wattpad Books and Wattpad Brand Partnerships. Were proudly based in Toronto, but our reach is global. Come build the future of entertainment and storytelling, and write your next chapter with us!

We are looking for a Data Engineer passionate about solving hard technical problems, loves to create data models, and thrives in a product focused environment. If this is you, keep reading!  

This position is for Wattpads Data Engineering Team. The vision of this team is to build the technology that will create capabilities to understand stories and users, and accelerate our ability to innovate at Wattpad. We believe in a team culture that enables empowered engineers to fix problems in the way that they see fit.
What you will be doing

Coordinate with data scientists, data analysts, software developers and product managers to design and evolve our existing architecture, improving the reliability and performance of our data infrastructure.
Build, troubleshoot and scale high traffic distributed systems processing hundreds of thousands of messages per second in real time
Develop and maintain reliable ETL pipelines to help power analytical needs
Provide input into the design and development of our in-house AB testing framework to maximize effectiveness of experiments
Evangelize data-driven decision making across the organization through data expertise

 What were looking for

At least 2 years experience in writing high-quality code in any programming language bonus if its in Python or Go
Strong understanding of database fundamentals
Experience with batch processing and streaming systems like Hive, Pig and Kafka
Experience with Spark and Scala is a plus
A critical thinker and possess creative problem solving skills  
Quantitative skills, ability to think analytically, and strong attention to details
A desire to contribute to the broader development community



Wattpad is conducting all interviews in a distributed manner using applicable third party software where needed and using visual interface tools such as Google Hangouts and Zoom.

About Wattpad


Who are we? Entrepreneurs and Do-ers. Our vision is to entertain and connect the world through stories, and our mission is to use the power of community and technology to unleash the full potential of stories to the world.

What does that mean? We are visionaries, community builders, passionate problem solvers, storytellers, coffee snobs tea drinkers, too!, curious by nature, and culturally diverse.

What are we obsessed with? Our users. Solving complex problems and maximizing flow. Learning constantly. Building the next great storytelling product. Finding the greatest stories ever told. Dogs and cats, coffee, and good snacks.

How do we work? Autonomously, collaboratively, respectfully. Balancing with work, family, and play...and all while having a great time.

Wattpad is a remote friendly company and encourages remote candidates to apply as long as they are located and authorized to work in either the US or Canada excluding Quebec as a precondition of employment. We are not able to sponsor applicants for work permits. 

If you happen to live near the areas of either Toronto, Ontario or Halifax, Nova Scotia, you may also have the opportunity to work from our beautiful offices - 1 located in Downtown Toronto and the other in Halifax.

Culture and Diversity

Wattpad is an equal opportunity employer. We do not discriminate. Period. 

Wattpad welcomes and encourages applications from people with disabilities. Accommodations are available on request for candidates taking part in all aspects of the selection process. We have taken a leadership position on creating a culture and an organization that truly values diversity. We are committed to fostering a global team that reflects the diversity of the Wattpad community. At Wattpad, we believe cultural fit doesnt mean culturally identical, and diversity of thought helps us to challenge one another to think big and think differently. We consider employment applicants without regard to age, race, colour, national origin, citizenship, religion, creed, sex, sexual orientation, veteran status, marital status, disability status or any other protected status.  

If you have any special needs or accessibility requirements, please let us know. We will do our utmost to accommodate, in accordance with applicable local legislation.

Dont meet all the requirements? Studies show women and people of colour are less likely to apply to jobs if they do not meet all the qualifications. Therefore, in an effort to build a more diverse workplace, we encourage you to apply anyways. You might actually be the right person or you may be a good fit for a number of other openings we currently have.

"
